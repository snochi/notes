\documentclass[linearalgebra]{subfiles}

\begin{document}

    \chap{Bilinear Forms} 

    \section{Bilinear Forms}

    \begin{remark}
        For this and next section, unless otherwise specified, let $V$ denote a finite-dimensional vector space.
    \end{remark}

    \begin{recall}{Linear Functional}{on a Vector Space}
        Let $V$ be a vector space. We say a function $L:V\to\F$ is a \emph{linear functional} on $V$ if $L$ is linear.
    \end{recall}

    \begin{definition}{Bilinear Form}{on a Vector Space}
        Let $V$ be a vector space. We say a function $f:V\times V\to\F$ is a \emph{bilinear form} on $V$ if
        \begin{equation*}
            \begin{cases}
                f(cv+u,w) = cf(v,w)+f(u,w) \\
                f(v,cu+w) = cf(v,u)+f(v,w) \\
            \end{cases}
        \end{equation*}
        In other words, $f(v,u)$ is bilinear if $f$ is a linear functional of $v$ when $u$ is fixed and vice versa.
    \end{definition}

    \begin{example}
        Clearly the zero function $0:V\times V\to \F$ is a bilinear form on $V$.
    \end{example}

    \begin{remark}
        Let $f,g:V\times V\to\F$ be bilinear and let $c\in\F$. Then
        \begin{equation*}
            cf+g:V\times V\to\F
        \end{equation*}
        is also bilinear. In fact, if $f_1,f_2,\ldots,f_n: V\times V\to\F$ are bilinear, then
        \begin{equation*}
            \sum^{n}_{i=1} c_if_i: V\times V\to\F 
        \end{equation*}
        is also bilinear. Together with Example 9.1, it follows that the set of bilinear forms on $V$, denoted as $\lin\left( V,V,\F \right)$, is a subspace of the vector space $\F^{V\times V}$.
    \end{remark}

    \begin{example}
        Let $V$ be a vector space and let $L_1,L_2:V\to\F$ be linear functionals on $V$. Then
        \begin{equation*}
            f(v,u) = L_1(v)L_2(u)
        \end{equation*}
        is bilinear, since $f$ is a linear functional of $v$ when $u$ is fixed and vice versa.
    \end{example}

    \begin{example}
        Let $m,n\in\N$ and let $V=M_{m\times n}(\F)$. Let $A\in M_{m\times m}(\F)$. Define
        \begin{equation*}
            f_A(X,Y) = \tr\left( X^TAY \right),
        \end{equation*}
        then $f_A$ is a linear functional on $V$. For, if $X, Y, Z\in V$ and $c\in\F$, then
        \begin{equation*}
            f_A\left( cX+Y,Z \right) = \tr\left( \left( cX+Y \right) ^TAZ \right) = \tr\left( cX^TAZ \right) + \tr\left( cY^TAZ \right) = f_A\left( cX,Z \right) + f_A\left( cY, Z \right).
        \end{equation*}
        In particular, when $n=1$, the matrix $X^TAY$ is $1\times 1$, and the bilinear form is simply
        \begin{equation*}
            f_A(X,Y) = X^TAY = \sum^{}_{i,j} A_{ij}X_iY_j.
        \end{equation*}
        We shall presently show that every bilinear form on the space $M_{m\times 1}(\F)$ is of this type, for some $A\in M_{m\times m}(\F)$.
    \end{example}

    \begin{example}
        Let us find all bilinear forms on $\F^2$. Suppose $f:\F^2\times\F^2\to\F$ is bilinear. If we let
        \begin{equation*}
            v = \left( v_1,v_2 \right) , u = \left( u_1,u_2 \right) \in\F^2,
        \end{equation*}
        then
        \begin{align*}
            f(v,u) & = f\left( v_1e_1+v_2e_2, u \right) = v_1f\left( e_1,u \right) + v_2f\left( e_2,u \right) = v_1f\left( e_1,u_1e_1+u_2e_2 \right) + v_2f\left( e_2,u_1e_1+u_2e_2 \right) \\
                   & = v_1u_1f\left( e_1,e_1 \right) + v_1u_2f\left( e_1,e_2 \right) + v_2u_1f\left( e_2,e_1 \right) + v_2u_2f\left( e_2e_2 \right) ,
        \end{align*} 
        where each $e_i\in\F^2$ is the $i$th element of the standard ordered basis for $\F^2$. That is, $f$ is completely determined by $a_{ij}=f(e_i,e_j)$'s, such that
        \begin{equation*}
            f(v,u) = \sum^{}_{i,j} a_{ij}v_iu_j.
        \end{equation*}
        If $X$ and $Y$ are the coordinate matrices of $v$ and $u$, respectively, and if $A\in M_{2\times 2}(\F)$ with entries
        \begin{equation*}
            A_{ij} = a_{ij} = f\left( e_i,e_j \right) ,
        \end{equation*}
        then
        \begin{equation*}
            f(v,u) = X^TAY.
        \end{equation*}
        Thus we see that any biliear form on $\F^2$ is precisely of the form which we discussed in Example 9.4.
    \end{example}

    \begin{remark}
        We may generalize the results of Example 9.5 as follows. Let $\beta = \left\lbrace v1,v_2,\ldots,v_n \right\rbrace $ be an ordered basis for $V$ and let $f:V\times V\to\F$ be bilinear. If
        \begin{equation*}
            x = \sum^{n}_{i=1} x_iv_i, y = \sum^{n}_{i=1} y_iv_i\in V
        \end{equation*}
        for some $x_1,x_2,\ldots,x_n, \ldots, y_n\in\F$, then
        \begin{equation*}
            f(x,y) = f\left( \sum^{}_{i} x_iv_i,y \right) = \sum^{}_{i} x_if\left( v_i,y \right) = \sum^{}_{i} x_if\left( v_i, \sum^{}_{j} y_jv_j \right) = \sum^{}_{i,j} x_iy_jf\left( v_i,v_j \right) . 
        \end{equation*}
        That is, if we let $A_{ij} = f\left( v_i,v_j \right)$, then
        \begin{equation*}
            f\left( x,y \right) = \sum^{}_{i,j} A_{ij}x_iy_j = X^TAY
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrices of $x$ and $y$ in the ordered basis $\beta$ for $V$, respectively. Thus every bilinear form on $V$ is of the type
        \begin{equation*}
            f\left( x,y \right) = \left[ x \right] _\beta A\left[ y \right] _\beta
        \end{equation*}
        for some $A\in M_{n\times n}(\F)$ and an ordered basis $\beta$ for $V$. Conversely, if $A\in M_{n\times n}(\F)$ is given, then clearly the above equation defines a bilinear form $f:V\times V\to\F$ such that
        \begin{equation*}
            A_{ij} = f\left( v_i,v_j \right) .
        \end{equation*}
        This motivates the following definition.
    \end{remark}

    \begin{definition}{Matrix}{of a Bilinear Function with Respect to an Ordered Basis}
        Let $f:V\times V\to\F$ be bilinear and let $\beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace$ be an ordered basis for $V$. Then we define the \emph{matrix} of $f$ with respect to $\beta$ by
        \begin{equation*}
            \left( \left[ f \right] _\beta \right) _{ij} = f\left( v_i,v_j \right) .
        \end{equation*}
    \end{definition}
    
    \clearpage
    \begin{theorem}{$[]_\beta:\lin\left( V,V,\F \right)\to M_{n\times n}(\F)$ Is an Isomorphism}
        Let $\beta$ be an ordered basis for $V$. Then $[]_\beta:\lin\left( V,V,\F \right)\to M_{n\times n}(\F)$ is an isormorphism.
    \end{theorem}

    \begin{proof}
        The bijectivity of $[]_\beta$ is supplied by Remark 9.5. To verify the linearity, let $f,g\in\lin\left( V,V,\F \right)$ and $c\in\F$. Then
        \begin{equation*}
            \left( \left[ cf+g \right] _\beta \right) _{ij} = \left( cf+g \right) \left( v_i,v_j \right) = cf\left( v_i,v_j \right) + g\left( v_i,v_j \right) = \left( c\left[ f \right]_\beta  \right)_{ij} + \left( \left[ g \right]_\beta  \right) _{ij}.
        \end{equation*}
        But this exactly means
        \begin{equation*}
            \left[ cf+g \right] _\beta = c\left[ f \right] _\beta + \left[ g \right] _\beta,
        \end{equation*}
        as desired.
    \end{proof}

    \begin{recall}{Dual}{of an Ordered Basis}
        Let $\beta = \left\lbrace v_1,v_2,v_n \right\rbrace $ be an ordered basis for $V$. Then the \emph{dual} of $\beta$, denoted as $\beta^* = \left\lbrace L_1,L_2,\ldots,L_n \right\rbrace $ is such that
        \begin{equation*}
            L_i\left( v \right) = \left( \left[ v \right] _\beta \right) _i
        \end{equation*}
        for all $v\in V$ and $i\in\left\lbrace 1,2,\ldots,n \right\rbrace$.
    \end{recall}

    \begin{cor}{}
        Let $\beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace$ be a basis for $V$ and let $\beta^* = \left\lbrace L_1,L_2,\ldots,L_n \right\rbrace $ be the dual of $\beta$. Then
        \begin{equation*}
            \left\lbrace f_{ij} = L_iL_j: i,j\in\left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace 
        \end{equation*}
        is a basis for $\lin(V,V,\F)$. In particular,
        \begin{equation*}
            \dim\left( \lin\left( V,V,\F \right)  \right) = n^2.
        \end{equation*}
    \end{cor}	

    \begin{proof}
        For convenience, write
        \begin{equation*}
            \alpha = \left\lbrace f_{ij}=L_iL_j: i,j\in \left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace .
        \end{equation*}
        Notice that each $f_{ij}$ is defined by
        \begin{equation*}
            f_{ij}\left( x,y \right) = L_i(x)L_j(y)
        \end{equation*}
        is a bilinear form on $V$ by Example 9.3. That is, if
        \begin{equation*}
            x = \sum^{n}_{i=1} x_iv_i, y = \sum^{n}_{i=1} y_iv_i\in V,
        \end{equation*}
        then
        \begin{equation*}
            f_{ij} (x,y) = x_iy_i.
        \end{equation*}
        Now, let $f:V\times V\to\F$ be bilinear and let $A = \left[ f \right] _\beta$. Then
        \begin{equation*}
            f\left( x,y \right) = \sum^{}_{i,j} A_{ij}x_iy_j
        \end{equation*}
        which exactly means
        \begin{equation*}
            f = \sum^{}_{i,j} A_{ij}f_{ij}.
        \end{equation*}
        Thus $\alpha$ is a basis for $\lin\left( V,V,\F \right)$, as required.
    \end{proof}

    \begin{remark}
        In terms of matrix point of view, one may rephrase Corollary 9.1.1 as follows: the matrix of each $f_{ij}$ with respect to $\beta$ is such that
        \begin{equation*}
            \left( \left[ f_{ij} \right] _\beta \right) _{rs} =
            \begin{cases}
                1 & \text{ if } r = i \land s = j \\
                0 & \text{ otherwise} 
            \end{cases}.
        \end{equation*}
        In other words,
        \begin{equation*}
            \left\lbrace \left[ f_{ij} \right] _\beta : i,j\in \left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace 
        \end{equation*}
        is a set of $n\times n$ matrices whose entries are all zero except for the entry $\left( \left[ f_{ij} \right] _\beta \right) _{ij} = 1$, which clearly is a basis for $M_{n\times n}(\F)$. Thus it follows from Theorem 9.1 that $\alpha$ is a basis for $\lin\left( V,V,\F \right)$.
    \end{remark}

    \begin{remark}
        The concept of the matrix of a bilinear form in an ordered basis is similar to that of the matrix representation of a linear operator. Just as for linear operators, we shall be interested in what happens to the matrix representing a bilinear form, as we change from one ordered basis to another. So suppose
        \begin{equation*}
            \beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace, \gamma = \left\lbrace u_1,u_2,\ldots,u_n \right\rbrace \subseteq V 
        \end{equation*}
        are ordered basis for $V$ and let $f:V\times V\to\F$ be bilinear. How are the matrices $[f]_\beta$ and $[f]_\gamma$ related? First, write
        \begin{equation*}
            u_i = \sum^{n}_{j=1} a_{ij}v_j 
        \end{equation*}
        for each $j\in\left\lbrace 1,2,\ldots,n \right\rbrace $. For all $v\in V$, there exists $c_1,c_2, \ldots,c_n\in\F$ such that
        \begin{equation*}
            v = \sum^{n}_{i=1} c_iu_i.
        \end{equation*}
        That is,
        \begin{equation*}
            v = \sum^{n}_{i=1} c_iu_i = \sum^{n}_{i=1} c_i\sum^{n}_{j=1} a_{ij}u_j = \sum^{}_{i,j} c_ia_{ij}v_j,
        \end{equation*}
        which means
        \begin{align*}
            \left( \left[ v \right] _\beta \right) _j & = \sum^{n}_{i=1} c_ia_{ij} \\ 
            \left( \left[ v \right] _\gamma \right) _j & = c_j.
        \end{align*} 
        So if $P_{ij} = a_{ij}$, then
        \begin{equation*}
            \left[ v \right] _\beta = P\left[ v \right] _\gamma.
        \end{equation*}
        This matrix is unique, and for all $v,u\in V$,
        \begin{equation*}
            f\left( v,u \right) = \left[ v \right] ^T_\beta \left[ f \right] _\beta \left[ v \right] _\beta = \left( P\left[ u \right] _\gamma \right) ^T \left[ f \right] _\beta P\left[ v \right] _\gamma = \left[ u \right] ^T_\gamma \left( P^T\left[ f \right] _\beta P \right) \left[ v \right] _\gamma.
        \end{equation*}
        Thus by the definition and uniqueness of the matrix $\left[ f \right] _\gamma$, it follows that
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P.
        \end{equation*}
    \end{remark}

    \begin{example}
        Define $f:\R^2\times\R^2\to\R$ by
        \begin{equation*}
            f\left( x,y \right) =x_1y_1+x_1y_2+x_2y_1+x_2y_2
        \end{equation*}
        provided that $x=\left( x_1,x_2 \right), y=\left( y_1,y_2 \right) \in\R^2$. Then $x=x_1e_1+x_2e_2$ and $y=y_1e_1+y_2e_2$ where $\beta = \left\lbrace e_1,e_2 \right\rbrace$ is the standard ordered basis for $\R^2$. So
        \begin{equation*}
            f\left( e_1,e_1 \right) =f\left( e_1,e_2 \right) =f\left( e_2,e_1 \right) =f\left( e_2,e_2 \right) = 1
        \end{equation*}
        which means
        \begin{equation*}
            \left[ f \right] _\beta =
            \begin{bmatrix}
                1 & 1 \\
                1 & 1 \\
            \end{bmatrix}.
        \end{equation*}
        Now let $v_1=\left( 1,-1 \right), v_2=\left( 1,1 \right) \in\R^2$ and let $\gamma = \left\lbrace v_1,v_2 \right\rbrace$ be an ordered basis for $\R^2$. If $P\in M_{2\times 2}(\R)$ is the change of basis matrix, then
        \begin{align*}
            v_1 = \left( 1,-1 \right) = P_{11}e_1 + P_{21}e_2 & \implies P_{11}=1, P_{21} = -1 \\
            v_2 = \left( 1,1 \right) = P_{11}e_1 + P_{21}e_2 & \implies P_{12} = P_{22} = 1 .
        \end{align*} 
        Thus,
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P =
            \begin{bmatrix}
                1 & -1 \\ 1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\ 1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\ -1 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0 \\ 0 & 4
            \end{bmatrix}.
        \end{equation*}
        What this means is that, if $x = x_1v_1 + x_2v_2$ and $y = y_1v_1+y_2v_2$, then
        \begin{equation*}
            f\left( x,y \right) = 4x_2y_2.
        \end{equation*}
    \end{example}

    \begin{remark}
        One consequence of the change of basis equation
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P
        \end{equation*}
        is the following. If $A,B\in M_{n\times n}(\F)$ represents the same bilinear form on $V$, then $A$ and $B$ have the same rank. For, if
        \begin{equation*}
            B = P^TAP
        \end{equation*}
        for some invertible $P\in M_{n\times n}(\F)$, it is clear that $A$ and $B$ have the same rank. This makes it possible to define the rank of a bilinear form $f$ on $V$ to be the rank of $\left[ f \right] _\beta$ for some ordered basis $\beta$ on $V$. However, it is more desirable to give more intrinsic definition of the rank of a bilinear form. This can be done as follows. Suppose $f\left( v,u \right)$ is a bilinear form on $V$. If we fix $v\in V$, then $f(v,u)$ becomes a linear functional on $V$; let us denote this by $L_f(v)$. This provides us a linear transformation $L_f:V\to V^*$ by the mapping
        \begin{equation*}
            v\mapsto L_f(v).
        \end{equation*}
        On the other hand, if we fix $u\in V$, then we also get a linear functional $R_f(u):V\to \F$, and $R_f$ is a linear transformation. After we prove that
        \begin{equation*}
            \rank\left( L_f \right) = \rank\left( R_f \right),
        \end{equation*}
        we shall define the rank of a bilinear form on a finite-dimensional vector space to be the rank of the associated linear transformations.
    \end{remark}

    \begin{prop}{$\rank\left( L_f \right) = \rank\left( R_f \right)$}
        Let $f$ be a bilinear form on $V$ and define linear transformations
        \begin{equation*}
            L_f,R_f:V\to V^*
        \end{equation*}
        as Remark 9.9. Then $\rank\left( L_f \right) = \rank\left( R_f \right)$.
    \end{prop}

    \begin{proof}
        To prove $\rank\left( L_f \right) = \rank\left( R_f \right)$, it is sufficient to prove that $\nullity\left( L_f \right) = \nullity\left( R_f \right)$ by rank-nullity theorem. Let $\beta$ be an ordered basis for $V$ and let $A=\left[ f \right] _\beta$. Then
        \begin{equation*}
            f\left( x,y \right) = X^T\left[ f \right] _\beta Y,
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrix of $x$ and $y$, respectively. So if $L_f(x)=0$, then
        \begin{equation*}
            \left( X^T\left[ f \right] _\beta \right) Y = 0
        \end{equation*}
        for any $Y\in M_{n\times 1}(\F)$. Clearly this means
        \begin{equation*}
            X^T\left[ f \right] _\beta = 0,
        \end{equation*}
        or, equivalently,
        \begin{equation*}
            \left[ f \right] ^T\beta X = 0.
        \end{equation*}
        This means
        \begin{equation*}
            \nullity\left( L_f \right) = \dim \left\lbrace X\in M_{n\times 1}(\F): \left[ f \right] ^T_\beta X = 0  \right\rbrace  .
        \end{equation*}
        Similar argument shows that
        \begin{equation*}
            \nullity\left( R_f \right) = \dim\left\lbrace Y\in M_{n\times 1}(\F): \left[ f \right] _\beta Y = 0  \right\rbrace .
        \end{equation*}
        Simce $\left[ f \right] ^T_\beta$ and $\left[ f \right] _\beta$ have the same column rank, they have the same dimension of solution space of homogeneous equations. That is,
        \begin{equation*}
            \nullity\left( L_f \right) = \nullity\left( R_f \right),
        \end{equation*}
        as desired.
    \end{proof}

    \begin{definition}{Rank}{of a Bilinear Form}
        Let $f$ be a bilinear form on $V$ and let $L_f, R_f: V\to V^*$ be linear transformations provided by Remark 9.9. Then we define the rank of $f$ by
        \begin{equation*}
            \rank\left( f \right) = \rank\left( L_f \right) = \rank\left( R_f \right),
        \end{equation*}
        where the second equality holds by Proposition 9.2.
    \end{definition}

    \begin{cor}{$\rank\left( f \right) =\rank\left[ f \right] _\beta$}
        Let $f$ be a bilinear form on $V$ and let $\beta$ be an ordered basis for $V$. Then
        \begin{equation*}
            \rank\left( f \right) = \rank\left[ f \right] _\beta.
        \end{equation*}
    \end{cor}	

    \begin{proof}
        From the proof of Proposition 9.2,
        \begin{equation*}
            \nullity\left( f \right) = \dim\left\lbrace X\in M_{n\times 1}(\F) : \left[ f \right] _\beta X = 0 \right\rbrace .
        \end{equation*}
        But it is clear that
        \begin{equation*}
            \dim\left\lbrace X\in M_{n\times 1}(\F) : \left[ f \right] _\beta X = 0 \right\rbrace = \nullity\left[ f \right] _\beta.
        \end{equation*}
        Thus $\rank\left( f \right) = \rank\left[ f \right] _\beta$ by rank-nullity theorem, as desired.
    \end{proof}

    \clearpage
    \begin{cor}{}
        Let $f$ be a bilinear form on $V$. Then the following are equivalent.
        \begin{enumerate}
            \item $\rank(f) = \dim(V)$.
            \item $\forall v\in V\setminus \left\lbrace 0 \right\rbrace\exists u\in V \left[ f\left( v,u \right) \neq 0 \right]$.
            \item $\forall u\in V\setminus \left\lbrace 0 \right\rbrace\exists v\in V \left[ f\left( v,u \right) \neq 0 \right]$.
        \end{enumerate}
    \end{cor}	

    \begin{proof}
        Observe that (a), (b), and (c) are all equivalent to the statement
        \begin{equation*}
            \nullity\left( L_f \right) = \nullity\left( R_f \right) = 0. \eqedsym
        \end{equation*}
    \end{proof}

    \begin{definition}{Nondegenerate}{Bilinear Form}
        Let $f$ be a bilinear form on an arbitrary vector space $V$. We say $f$ is \emph{nondegenerate} if it satisfies (b) and (c) of Corollary 9.2.2.
    \end{definition}

    \begin{remark}
        As Corollary 9.2.2 implies, if $f$ is a bilinear form on a finite-dimensional vector space $V$, then $f$ is nondegenerate if it satisfies one of (a), (b), and (c). In particular, $f$ is nondegenerate if and only if $\left[ f \right] _\beta$ is invertible for any ordered basis $\beta$ for $V$.
    \end{remark}

    \begin{definition}{Dot Product}{}
        Let $V = \F^n$ and let $f$ be a bilinear form on $V$ defined by
        \begin{equation*}
            f\left( x,y \right) = \sum^{n}_{i=1} x_iy_i,
        \end{equation*}
        where $x_i$ and $y_i$ are the $i$th entries of $x$ and $y$, respectively. Then $f$ is nondegenerate on $V$. Moreover, if $\beta$ is the standard ordered basis for $\F^n$, then
        \begin{equation*}
            \left[ f \right] _\beta = I.
        \end{equation*}
        That is,
        \begin{equation*}
            f\left( x,y \right) = \left[ x \right] _\beta^T \left[ y \right] _\beta.
        \end{equation*}
        We call such $f$ the \emph{dot product}.
    \end{definition}

    \section{Symmetric Bilienar Form}
    
    \begin{remark}
        The main purpose of this section is to answer the following question: if $f$ is a bilinear form on $V$, when does an ordred basis $\beta$ for $V$ such that $[f]_\beta$ is diagonal exist? We shall prove that such $\beta$ exists if and only if $f\left( v,u \right) = f\left( u,v \right)$ for all $v,u\in V$.  The result is proved only when the field underlying $V$ has characteristic zero.
    \end{remark}

    \begin{definition}{Symmetric}{Bilinear Form}
        Let $f$ be a bilinear form. We say $f$ is \emph{symmetric} if
        \begin{equation*}
            f\left( v,u \right) = f\left( u,v \right) 
        \end{equation*}
        for all $v,u\in V$.
    \end{definition}

    \begin{recall}{Characteristic}{of a Field}
        Let $\F$ be a field. We say $n\in\N$ is the \emph{characteristic} of $\F$, denoted as $\char\left( \F \right)$, if $n$ is the minimum positive integer such that adding 1 $n$ times is zero,
        \begin{equation*}
            1 + 1 + \cdots + 1 = 0.
        \end{equation*}
        If no such $n$ exists, we say $\F$ has \emph{characteristic zero}.
    \end{recall}

    \begin{remark}
        If $V$ is finite-dimensional, then a bilinear form $f$ is symmetric if and only if $\left[ f \right] _\beta$ is symmetric for some ordered basis $\beta$ for $V$. To see this, first suppose that $f$ is symmetric. Then,
        \begin{equation*}
            f\left( x,y \right) = X^TAY
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrices of $x$ and $y$, respectively. Since $f$ is symmetric, $f(x,y) = f(y,x)$ for any $x,y\in V$, so
        \begin{equation*}
            X^TAY = Y^TAX.
        \end{equation*}
        But $X^TAY, Y^TAX\in M_{1\times 1}(\F)$, which means
        \begin{equation*}
            X^TAY = Y^TAX = \left( Y^TAX \right) ^T = X^TA^TY
        \end{equation*}
        for any $X,Y\in M_{n\times 1}(\F) $. Thus it is clear from the above equation that $A=A^T$. The converse statement is clear, since $X^TAY, Y^TAX\in M_{1\times 1}(\F)$, so
        \begin{equation*}
            X^TAY = \left( X^TAY \right) ^T = Y^TA^TX = Y^TAX.
        \end{equation*}
        One particular result is that, if $\left[ f \right] _\beta$ is diagonal for some ordred basis $\beta$ for $V$, then $f$ is symmetric, since any diagonal matrix is symmetric. Whenver a bilinear form is symmetric, we are allowed to make the following definition.
    \end{remark}

    \begin{definition}{Quadratic Form}{Associated with a Symmetric Bilinear Form}
        Let $f$ be a symmetric bilinear form on $V$. Then we define the \emph{quadratic form} associated with $f$ to be
        \begin{equation*}
            q(v) = f(v,v) : V\to \F
        \end{equation*}
        for all $v\in V$.
    \end{definition}

    \begin{remark}
        If $\F\subseteq\CC$ is a subfield, the symmetric bilinear form $f$ is completely determined by its associated quadratic form, that
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right) .
        \end{equation*}
        The estabilishment of the above equation is a routine computation, so we shall omit it.
    \end{remark}

    \begin{definition}{Polarization Identity}{}
        We call the equation
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right), 
        \end{equation*}
        where $f$ is a symmetric bilinear form on $V$ over a subfield of $\CC$ and $q$ is the quadratic form of $f$, the \emph{polarization identity}.
    \end{definition}

    \begin{example}
        Suppose the bilinear form $f$ over $\F^n$ is the dot product. Then the associated quadratic form is
        \begin{equation*}
            q\left( x_1,x_2,\ldots,x_n \right) = \sum^{n}_{i=1} x^2_i.
        \end{equation*}
        In other words, the geometric interpretation is that $q(x)$ is the square of the length of $v$. Moreover, notice that for any symmetric bilinear form $f_A(x,y) = XA^TY$, the associated quadratic form is
        \begin{equation*}
            q_A(x) = X^TAX = \sum^{}_{i,j} A_{ij}x_ix_j.
        \end{equation*}
    \end{example}

    \begin{remark}
        One important class of symmetric bilinear forms consists of inner products on a vector space over $\R$ or $\CC$.
    \end{remark}

    \begin{definition}{Inner Product}{}
        Let $V$ be a vector space over a subfield of $\CC$. An \emph{inner product} $f$ on $V$ is a symmetric bilinear form which is positive definite. That is,
        \begin{equation*}
            q(v) = f\left( v,v \right) > 0
        \end{equation*}
        for any nonzero $v\in V$.
    \end{definition}

    \begin{definition}{Orthogonal}{Vectors}
        Let $v,u\in V$. We say $v$ and $u$ are \emph{orthogonal} with respect to an inner product $f$ if
        \begin{equation*}
            f(v,u) = 0.
        \end{equation*}
    \end{definition}

    \continued{Remark}
    \noindent The motivation for the definition of orthogonal vectors is clear: notice that if $f$ is the dot product - which is a special form of inner products -, then
    \begin{equation*}
        f(v,u) = 0
    \end{equation*}
    whenenver $v$ and $u$ are orthogonal. The above definition is a generalization of this result. The quadratic form of an inner product $q(v) = f(v,v)$ takes only nonnegative values by positive definiteness, and is usually thought as the square of the length of $v$. More discussions of inner product will be done in the following chapter.

    \begin{theorem}{If $f$ Is a Symmetric Bilinear Form, Then $[f]_\beta$ Is Diagonal for Some Ordered Basis $\beta$}
        Let $V$ be a finite-dimensional vector space over $\F$ of characteristic zero and let $f$ be a symmetric bilinear form on $V$. Then there exists an ordered basis $\beta$ for $V$ such that $[T]_\beta$ is diagonal.
    \end{theorem}

    \begin{proof}
        We proceed inductively. Observe that if $f=0$, then the proof is trivial, so suppose that $f$ is nonzero. When $\dim\left( V \right) =1$, then $[f]_\beta$ is diagonal for any ordered basis $\beta$ for $V$. Now suppose that the result holds for any bilinear form $f$ on a $k$-dimensional vector space. Let $V$ be a vector space with $\dim(V)=k+1$. Then there exists $v_{k+1}\in V$ such that $f\left( v_{k+1},v_{k+1} \right) = q\left( v_{k+1} \right) \neq 0$, where $q$ is the quadratic form of $f$, since any symmetric bilinear form can be written as
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right) 
        \end{equation*}
        by the polarization identity, and it is clear from the above equation that $q(v_{k+1})\neq 0$ for some $v_{k+1}\in V$. Let $W = \spn\left( v_{k+1} \right)$, then $\dim\left( W \right) =1$. Moreover, let
        \begin{equation*}
            W_\perp = \left\lbrace w\in V: f\left( v_{k+1},w \right) = 0 \right\rbrace .
        \end{equation*}
        Now the claim is that $V = W\oplus W_\perp$. To verify this, first observe that $W_\perp$ is a subspace of $V$. For, $f(v_{k+1}, 0) = 0$ and if $w_1,w_2\in W_\perp$ and $c\in\F$, then
        \begin{equation*}
            f\left( v_{k+1}, cw_1+w_2 \right) = cf\left( v_{k+1}, w_1 \right) + f\left( v_{k+1}, w_2 \right) = 0
        \end{equation*}
        so $cw_1+w_2\in W_\perp$, which means $W_\perp$ is closed under addition and scalar multiplication. Therefore, $W_\perp$ is a subspace of $V$. It is clear that $W$ and $W_\perp$ are independent, since if $w\in W$, then $w=cv$ for some $c\in\F$. So if $w=cv\in W_\perp$, then
        \begin{equation*}
            f\left( v,cv \right) = cf\left( v,v \right) = 0
        \end{equation*}
        which means $c=0$ and $w=cv=0$. To see that every $v\in V$ can be written as
        \begin{equation*}
            v = w + w_\perp
        \end{equation*}
        for some $w\in W$ and $w_\perp\in W_\perp$, observe that if we define
        \begin{equation*}
            w_\perp = v - \frac{f\left( v,v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v,
        \end{equation*}
        then
        \begin{equation*}
            f\left( v_{k+1}, w_\perp \right) = f\left( v_{k+1}, v \right) - \frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }f\left( v_{k+1},v_{k+1} \right) 
        \end{equation*}
        and since $f$ is symmetric, $f\left( v, w_\perp \right) = 0$. That is, $w_\perp\in W_\perp$. In other words, every $v\in V$ can be written as
        \begin{equation*}
            v = \frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v_{k+1} + w_\perp
        \end{equation*}
        for some $\frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v_{k+1}\in W$ and $w_\perp\in W_\perp$. Thus $V=W\oplus W_\perp$, as claimed. By induction hypothesis, $W_\perp$ has an ordered basis $\beta_\perp = \left\lbrace v_1,v_2,\ldots,v_k \right\rbrace $ such that $\left[ f_\perp \right] _{\beta_\perp}$ is diagonal. But this exactly means
        \begin{equation*}
            \left( \left[ f_\perp \right] _{\beta_\perp} \right) _{ij} = f\left( v_i,v_j \right) = 0
        \end{equation*}
        whenenver $i\neq j$. Thus, if we define $\beta = \left\lbrace v_1,v_2,\ldots,v_k,v_{k+1} \right\rbrace$, then $\beta$ is the ordered basis for $V$ by direct sum decomposition $V = W\oplus W_\perp$, and we have diagonal $\left[ f \right] _\beta$ by construction.
    \end{proof}

    \begin{cor}{}
        Let $\F\subseteq\CC$ be a subfield and let $A\in M_{n\times n}(\F)$. Then there exists invertible $P\in M_{n\times n}(\F)$ such that $P^TAP$ is diagonal.
    \end{cor}	

    \begin{theorem}{Diagonalization of a Symmetric Bilinear Form on a Real Vector Space where Nonzero Entries Are $\pm 1$}
        Let $V$ be a finite-dimensional vector spcae over $\R$ and let $f$ be a symmetric bilinear form on $V$ with $\rank\left( f \right) = r\in\N$. Then there exists an ordered basis $\beta=\left\lbrace v_1,v_2,\ldots,v_n \right\rbrace $ for $V$ such that $\left[ f \right] _\beta$ is diagonal and
        \begin{equation*}
            f\left( v_i,v_i \right) =
            \begin{cases} 
                \pm 1 & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases}.
        \end{equation*}
        Furthermore, the number of $v_i\in\beta$ such that $f\left( v_i,v_i \right) = 1$ is independent of the choice of an ordered basis $\beta$ for $V$. 
    \end{theorem}

    \begin{proof}
        By Theorem 9.3, there exists an ordered basis $\alpha$ for $V$ such that $\left[ f \right] _\alpha$ is diagonal. Since
        \begin{equation*}
            \rank\left( f \right) = \rank\left[ f \right] _\alpha,
        \end{equation*}
        it follows that exactly $r$ entries on the main diagonal of $\left[ f \right] _\alpha$ are nonzero. That is, we may rearrange the elements of $\alpha$ to obtain an ordered basis $\gamma$ for $V$ such that $\left[ f \right] _\gamma$ is diagonal and
        \begin{equation*}
            \left( \left[ f \right] _\gamma \right)_{ii} =
            \begin{cases} 
                1 & \text{ if } i\in \left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases}.
        \end{equation*}
        Write $\gamma=\left\lbrace u_1,u_2,\ldots,u_n \right\rbrace$ for convenience. For all $i\in\left\lbrace 1,2,\ldots,n \right\rbrace$, define
        \begin{equation*}
            v_i =
            \begin{cases} 
                \frac{1}{\sqrt{\left|f\left( u_i,u_i \right)\right| }}u_i & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                u_i & \text{ otherwise }
            \end{cases}
        \end{equation*}
        and let $\beta=\left\lbrace v_1,v_2,\ldots,v_r \right\rbrace$. Then
        \begin{equation*}
            f\left( v_i,v_i \right) = \left( \left[ f \right] _\beta \right) _{ii} = 
            \begin{cases} 
                \pm 1 & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases}
        \end{equation*}
        as desired. To prove the uniqueness of the number of $v_i\in\beta$ such that
        \begin{equation*}
            f\left( v_i,v_i \right) = \left( \left[ f \right] _\beta \right) _{ii} = 1,
        \end{equation*}
        let $p\in\N$ be the number of such $v_i\in\beta$. Also, let
        \begin{equation*}
            V_+ = \spn\left\lbrace v_i\in\beta: f\left( v_i,v_i \right) =1 \right\rbrace , V_-=\spn\left\lbrace v_i\in\beta: f\left( v_i,v_i \right) = -1 \right\rbrace \subseteq V 
        \end{equation*}
        be subspaces. Now $p=\dim\left( V \right)$, so what we shold verify is the uniqueness of the dimension of $V_+$. But first, notice that $f$ is positive definite on $V_+$. That is,
        \begin{equation*}
            \forall v\in V_+\setminus \left\lbrace 0 \right\rbrace \left[ f\left( v,v \right) > 0 \right].
        \end{equation*}
        Similarly, $f$ is negative definite on $V_-$. Moreover, if we define
        \begin{equation*}
            V_\perp = \spn\left\lbrace v_i\in\beta: f\left( v_i,v_i \right) =0 \right\rbrace \subseteq V,
        \end{equation*}
        then $V_\perp$ is also the subspace of $V$, and clearly we have
        \begin{equation*}
            \forall v\in V_\perp \left[ f\left( v,v \right) = 0 \right].
        \end{equation*}
        Since the union of bases of $V_+, V_-, V_\perp$ is $\beta$, and it is clear from the above constructions that $V_+, V_-, V_\perp$ are independent, we have
        \begin{equation*}
            V = V_+\oplus V_-\oplus V_\perp.
        \end{equation*}
        Now the claim is that, if $W\subseteq V$ is any subspace on which $f$ is positive definite, then $W, V_-, V_\perp$ are independent. To verify this claim, suppose $w\in W, v_-\in V_-, v_\perp\in V_\perp$ satisfy
        \begin{equation*}
            w + v_- + v_\perp = 0.
        \end{equation*}
        Then,
        \begin{equation*}
            \begin{cases} 
                f\left( w, w+v_-,v_\perp \right) & = f\left( w,w \right) + f\left( w,v_- \right) + f\left( w,v_\perp \right) = 0 \\
                f\left( v_-, w+v_-,v_\perp \right) & = f\left( v_-,w \right) + f\left( v_-,v_- \right) + f\left( v_-,v_\perp \right) = 0 
            \end{cases}.
        \end{equation*}
        Since $v_\perp\in V_\perp$, $f\left( w,v_\perp \right) = f\left( v_-,v_\perp \right) = 0$,
        \begin{equation*}
            \begin{cases} 
                f\left( w,w \right) + f\left( w,v_- \right) & = 0 \\
                f\left( v_-,w \right) + f\left( v_-,v_- \right) & = 0 \\
            \end{cases},
        \end{equation*}
        and $f$ is symmetric, it follows that $f\left( w,w \right) = f\left( v_-,v_- \right)$. Moreover, since $f$ is positive definite on $W$ and negative definite on $V_-$, we obtain
        \begin{equation*}
            f\left( w,w \right) = f\left( v_-,v_- \right) = 0.
        \end{equation*}
        So it must be the case that
        \begin{equation*}
            w = v_- = v_\perp = 0,
        \end{equation*}
        verifying the claim. Then by the direct sum decomposition
        \begin{equation*}
            V = V_+\oplus v_-\oplus V_\perp,
        \end{equation*}
        we have $\dim\left( V_+ \right) \geq \dim\left( W \right)$. On the other hand, if $\zeta$ is another ordered basis which satisfy the property
        \begin{equation*}
            \left( \left[ f \right] _\zeta \right) _{ii} =
            \begin{cases} 
                \pm 1 & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases},
        \end{equation*}
        then we get another direct sum decomposition
        \begin{equation*}
            V = W_+\oplus W_-\oplus W_\perp,
        \end{equation*}
        where $f$ is positive definite on $W_+$, negative definite on $W_-$, and zero on $W_\perp$. So we have
        \begin{equation*}
            \dim\left( V_+ \right) \geq \dim\left( W_+ \right) .
        \end{equation*}
        But by symmetry,
        \begin{equation*}
            \dim\left( W_+ \right) \geq \dim\left( V_+ \right) ,
        \end{equation*}
        which means $\dim\left( W_+ \right) = \dim\left( V_+ \right)$. Thus
        \begin{equation*}
            p = \dim\left( W_+ \right) = \dim\left( V_+ \right) 
        \end{equation*}
        is unique, as desired.
    \end{proof}

    \begin{remark}
        Let $f$ be a symmetric bilinear form on a finite dimensional vector space $V$ over $\R$ and let $\beta$ and 
        \begin{equation*}
            V = V_+\oplus V_-\oplus V_\perp
        \end{equation*}
        be as described in Theorem 9.4. Now let $V_0\subseteq V$ be subspace such that
        \begin{equation*}
            \forall v_0\in V_0\forall v\in V \left[ f\left( v_0,v \right) = 0 \right] .
        \end{equation*}
        We claim that $V_\perp = V_0$. To verify this, first observe that Theorem 9.4 provides $V_\perp\subseteq V_0$ by construction. On the other hand, it is clear that
        which means
        \begin{equation*}
            \dim\left( V_\perp \right) = \dim\left( V \right) - \left( \dim\left( V_+ \right) + \dim\left( V_- \right)  \right) = \dim\left( V \right) -\rank\left( f \right) ,
        \end{equation*}
        so it must be the case that every $v_0\in V$ such that $f\left( v_0,v \right) = 0$ for all $v\in V$ are such that $v_0\in V_\perp$. Thus $V_0\subseteq V_\perp$, verifying our claim. Thus we also see that $V_\perp$ is unique. On the other hand, the subspaces $V_+$ and $V_-$ are not unique. However, their dimension is unique as Theorem 9.4 shows, allowing us to make the following definition.
    \end{remark}

    \begin{definition}{Signature}{of a Symmetric Bilinear Form on a Finite-Dimensional Real Vector Space}
        Let $f$ be a symmetric bilinear form on $V$, a finite-dimensional vector space over $\R$, and let $V_+, V_-, V_\perp\subseteq V$ be subspaces such that
        \begin{equation*}
            V = V_+\oplus V_-\oplus V_\perp
        \end{equation*}
        and as described in Theorem 9.4. Then we define the \emph{signature} of $f$ by
        \begin{equation*}
            \dim\left( V_+ \right) - \dim\left( V_- \right).
        \end{equation*}
    \end{definition}

    \begin{remark}
        Let $V$ is a finite-dimensional vector space over $\R$ and let $V_1,V_2,V_3\subseteq V$ be such that
        \begin{equation*}
            V = \bigoplus^{3}_{i=1} V_i.
        \end{equation*}
        Let $f_1,f_2$ be an inner product on $V_1,V_2$, respectively. Then, we may define a symmetric bilinear form $f$ on $V$ as follows. If $v,u\in V$, then write
        \begin{equation*}
            v = \sum^{3}_{i=1} v_i, u = \sum^{3}_{i=1} u_i\in V
        \end{equation*}
        for some $v_1,u_1\in V_1, v_2,u_2\in V_2, v_3,u_3\in V_3$. Then a direct sum decomposition
        \begin{equation*}
            V = V_+\oplus V_-\oplus V_\perp
        \end{equation*}
        for $f$, as described in Theorem 9.4, would be, $V_+ = V_1, V_- = V_2, V_\perp = V_3$. Of course, $V_+$ and $V_-$ need not be $V_1$ and $V_2$, respectively; $V_1$ and $V_2$ are some suitable candidates. In fact, Theorem 9.4 guarantees that every symmetric bilinear form on $V$ can be constructed in this way. Moreover, the part which states that
        \begin{equation*}
            \left( \left[ f \right] _\beta \right) _{ii} 
            \begin{cases} 
                \pm 1 & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases},
        \end{equation*}
        for some ordered basis $\beta$ for $V$, is that any inner product can be represented by the identity matrix in some ordered basis for $V$.
    \end{remark}

    \begin{remark}
        If $V$ is instead a finite-dimensional vector space over $\CC$, then we may further simplify the matrix of a bilinear form.
    \end{remark}

    \begin{cor}{}
        Let $V$ be a finite-dimensional vector space over $\CC$ and let $f$ be a symmetric bilinear form on $V$ with $\rank\left( f \right) = r\in\N$. Then there exists an ordered basis $\beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace$ for $V$ such that $\left[ f \right] _\beta$ is diagonal and
        \begin{equation*}
            f\left( v_i,v_i \right) = 
            \begin{cases} 
                1 & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                0 & \text{ otherwise }
            \end{cases}.
        \end{equation*}
    \end{cor}	
    
    \begin{proof}
        A suitable proof for Corollary 9.4.1 can be done in an analogously to the proof of Theorem 9.4 above. The only difference is that, since $\CC$ is an algebraically closed field, we are allowed to take a square root of $f\left( u_i,u_i \right)$, and that we define
        \begin{equation*}
            v_i = 
            \begin{cases} 
                \frac{1}{\sqrt{\left( u_i,u_i \right) }}u_i & \text{ if } i\in\left\lbrace 1,2,\ldots,r \right\rbrace \\ 
                u_i & \text{ otherwise }
            \end{cases}. \eqedsym
        \end{equation*}
    \end{proof}

    \continued{Remark}
    \noindent In fact, the proof of Corollary 9.4.1 is valid for any algebraically closed field, or, any field closed under the operation of taking the square root. 
    
    \section{Skew-Symmetric Bilinear Form}

    \begin{remark}
        Throughout this section, let $V$ be a vector space over a subfield $\F\subseteq\CC$.
    \end{remark}

    \begin{definition}{Skew-Symmetric}{Bilinear Form}
        Let $f$ be a bilinear form. We say $f$ is \emph{skew-symmetric} if
        \begin{equation*}
            f(v,u) = -f(u,v)
        \end{equation*}
        for all $v,u\in V$.
    \end{definition}
    
    \begin{remark}
        We claim that any bilinear form $f$ on $V$ can be written as a sum of a symmetric bilinear form $g$ on $V$ and a skew-symmetric bilinear form $h$ on $V$. To verify this, let
        \begin{align*}
            g & = \frac{1}{2} \left( f(v,u) + f(u,v) \right) \\
            h & = \frac{1}{2} \left( f(v,u) - f(u,v) \right) ,
        \end{align*} 
        then $g$ and $h$ are symmetric and skew-symmetric by definition and clearly $f=g+h$. Moreover, it turns out that $g$ and $h$ are unique. That is, if $S$ is the set of symmetric bilinear forms on $V$ and $K$ is the set of skew-symmetric bilinear forms on $V$, then
        \begin{equation*}
            \lin\left( V,V,\F \right) = S\oplus K.
        \end{equation*}
    \end{remark}

    \begin{remark}
        If $f$ is a skew-symmetric bilinear form on $V$, then for any ordered basis $\beta$ for $V$, $\left[ f \right] _\beta$ is skew-symmetric.
    \end{remark}

    \begin{remark}
        Let $f$ be a skew-symmetric bilinear form on $V$. Similear to linear operators and symmetric bilinear forms, we are interested in finding an ordered basis $\beta$ such that $\left[ f \right] _\beta$ is simple, if such $\beta$ exists. We proceed as follows. If $f$ is nonzero, then there exists $v, u\in V$ such that
        \begin{equation*}
            f(v,u) = 1.
        \end{equation*}
        To see this, one may first pick $v',u\in V$ such that $f\left( v',u \right)\neq 0$, since $f$ is nonzero. Then it is clear that
        \begin{equation*}
            v = \frac{v'}{f\left( v',u \right) } 
        \end{equation*}
        satisfies the above condition. Let
        \begin{equation*}
            x = av + bu\in \spn\left\lbrace v,u \right\rbrace \subseteq V
        \end{equation*}
        for some $a,b\in\F$. Then
        \begin{align*}
            f\left( x,v \right) & = f\left( av+bu, v \right) = bf(u,v) = -b \\
            f\left( x,u \right) & = f\left( av+bu, u \right) = af(u,u) = a,
        \end{align*} 
        and so
        \begin{equation*}
            x = av+bu = f\left( x,u \right) v - f\left( x,v \right) u.
        \end{equation*}
        The above equation shows that $a=f(x,u)$ and $b=f(x,v)$ are zero whenever $x=0$, so $v$ and $u$ are linearly independent and $\dim(W)=2$. Moreover, let
        \begin{equation*}
            W_\perp = \left\lbrace w_\perp\in V: f\left( w_\perp,v \right) = f\left( w_\perp, u \right) = 0 \right\rbrace \subseteq V.
        \end{equation*}
        We claim that
        \begin{equation*}
            V = W\oplus W_\perp.
        \end{equation*}
        To verify this, let $y\in V$ be arbitrary, and let
        \begin{align*}
            w & = f\left( y,u \right) v - f\left( y,v \right) u \\
            w_\perp &= y-w.
        \end{align*} 
        Then $w\in\spn\left( v,u \right) = W$ and $w_\perp\in W_\perp$, since
        \begin{equation*}
            f\left( w_\perp, v \right) = f\left( y-f\left( y,u \right) v + f\left( y,v \right) u, v\right) = f\left( y+f(y,v)u, v \right) = f\left( y,v \right) + f\left( y,v \right) f(u,v) = 0, 
        \end{equation*}
        and we can show that $f\left( w_\perp, u \right) =0$ in a similar way. Thus any $y\in V$ can be written by the form $y = w + w_\perp$ for some $w\in W$ and $w_\perp\in W_\perp$. Moreover, it is clear from the definition of $W$ and $W_\perp$ that $W\cap W_\perp = \left\lbrace 0 \right\rbrace $. That is,
        \begin{equation*}
            V = W\oplus W_\perp,
        \end{equation*}
        as claimed. Now, let $f_\perp$ be the restriction of $f$ on $W_\perp$. Then $f_\perp$ is a skew-symmetric bilinear form on $W_\perp$. That is, if $f_\perp$ is nonzero, then we may further decompose $V$ as
        \begin{equation*}
            V = W_1\oplus W_2\oplus W_{\perp_2}
        \end{equation*}
        by repeating the process above, where $W_1 = W$. In other words, if $V$ is finite-dimensional, we are going to get a decomposition
        \begin{equation*}
            V = \left( \bigoplus^{k}_{i=1} W_i \right) \oplus W_0
        \end{equation*}
        at the end, where each $W_1,W_2, \ldots, W_k\subseteq V$ is spanned by two vectors $v_i, u_i\in V$ such that
        \begin{enumerate}
            \item $f\left( v_i,u_i \right) = 1$,
            \item $f\left( v_i, v_j \right) = f\left( u_i,u_j \right) = f\left( v_i,u_j \right) = 0$ for any $j\in \left\lbrace 1,2,\ldots,k \right\rbrace$ with $j\neq i$, 
            \item for all $w_0\in W_0$, $w_0$ is \textit{orthogonal} to any $v\in V$, which means $f\left( v, w_0 \right) = 0$, and
            \item the restriction of $f$ into $W_0$ is zero.
        \end{enumerate}
        The following theorem summarizes the matrix analogue of this result.
    \end{remark}

    \begin{theorem}{}
        Let $f$ be a skew-symmetric bilinear form on $V$. Then $r=\rank(f)=2k$ for some $k\in\N\cup\left\lbrace 0 \right\rbrace$, and there exists an ordered basis $\beta$ for $V$ such that
        \begin{equation*}
            \left[ f \right] _\beta = \left( \bigoplus^{k}_{i=1} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \right) \oplus O,
        \end{equation*}
        where $O\in M_{n-r\times n-r}(\F)$ is the zero matrix. 
    \end{theorem}

    \begin{cor}{}
        If there exists nondegenerate skew-symmetric bilinear form $f$ on $V$, then $n=\dim(V)$ is even. Moreover, there exists an ordered basis $\beta$ for $V$ such that
        \begin{equation*}
            \left[ f \right] _\beta = \left( \bigoplus^{\frac{n}{2}}_{i=1} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \right).
        \end{equation*}
    \end{cor}	

    \begin{remark}
        Theorem 9.5 also provides the following standard matrix representation of a nondegenerate skew-symmetric bilinear form $f$ on $V$. That is, if $\beta = \left\lbrace v_1,u_1, v_2,u_2,\ldots,v_{\frac{n}{2}}, u_{\frac{n}{2}} \right\rbrace $ is an ordered basis for $V$ such that $\left[ f \right] _\beta$ is as described in Corollary 9.5.1, then $\alpha = \left\lbrace v_1,v_2,\ldots, v_{\frac{n}{2}}, u_1, \ldots, u_{\frac{n}{2}} \right\rbrace$ is such that
        \begin{equation*}
            \left[ f \right] _\alpha = \begin{bmatrix} 0 & J \\ -J & 0 \end{bmatrix},
        \end{equation*}
        where $J\in M_{\frac{n}{2}\times \frac{n}{2}}(\F)$ is
        \begin{equation*}
            J = 
            \begin{bmatrix}
                0 & \cdots & 0 & 1 \\
                0 & \cdots & 1 & 0 \\
                \vdots & \iddots & \vdots & \vdots \\
                1 & \cdots & 0 & 0 \\
            \end{bmatrix}.
        \end{equation*}
    \end{remark}

    \section{Groups Preserving Bilnear Forms}

    \begin{definition}{Preserve}{}
        Let $f$ be a bilinear form on a vector space $V$ and let $T:V\to V$ be linear. We say $T$ \emph{preserves} $f$ if
        \begin{equation*}
            f\left( Tv, Tu \right) = f\left( v,u \right) 
        \end{equation*}
        for all $v,u\in V$.
    \end{definition}

    \begin{remark}
        Notice that for any bilinear form $f$ and linear operator $T$ on $V$,
        \begin{equation*}
            g\left( v,u \right) = f\left( Tv, Tu \right) 
        \end{equation*}
        is a bilinear form. Thus $T$ preserves $f$ if and only if $g=f$.
    \end{remark}

    \begin{remark}
        Clearly $f\left( Iv, Iu \right) = f\left( v,u \right)$. Moreover, if $T,S:V\to V$ preserve $f$ then
        \begin{equation*}
            f\left( STv, STu \right) = f\left( Tv, Tu \right) = f\left( v, u \right) 
        \end{equation*}
        so $ST$ preserves $f$ as well. That is, if
        \begin{equation*}
            M_f = \left\lbrace T\in\lin\left( V \right) : T\text{ preserves }f \right\rbrace,
        \end{equation*}
        then $\left( M_f, \circ \right)$ is a monoid, where $\circ$ is the usual composition operation. In general, there is not much to talk about monoids of linear operators. However, if $f$ is nondegenerate, then we have the following.
    \end{remark}

    \begin{prop}{Set of Linear Operator Preserving Nondegenerate $f$ Is a Group}
        Let $f$ be a nondegenerate bilinear form on a vector space $V$, and let
        \begin{equation*}
            G_f = \left\lbrace T\in\lin\left( V \right) : T\text{ preserves }f \right\rbrace,
        \end{equation*}
        Then $G_f$ is a group under the usual composition operation of linear operators.
    \end{prop}

    \begin{proof}
        Remark 9.27 provides that $G_f$ is a monoid, so we only have to prove that every $T\in G_f$ has an inverse $T^{-1}$. Let $T\in G_f$ be and let $v\in\ker(T)$. Then for any $u\in V$,
        \begin{equation*}
            f\left( v,u \right) = f\left( Tv, Tu \right) = f\left( 0, Tu \right) = 0.
        \end{equation*}
        Since $f$ is nondegenerate, $v=0$. That is, $v=0$ whenever $Tv = 0$, which means that $T$ is invertible. Moreover, 
        \begin{equation*}
            f\left( T^{-1}v, T^{-1}u \right) = f\left( TT^{-1}v, TT^{-1}u \right) = f\left( v,u \right) 
        \end{equation*}
        for any $v,u\in V$, so $T^{-1}$ preserves $f$, or $T^{-1}\in G_f$, as desired.
    \end{proof}

    \begin{remark}
        When $V$ is finite-dimensional, an immediate consequence of Proposition 9.6 is that
        \begin{equation*}
            G = \left\lbrace \left[ T \right] _\beta\in M_{n\times n}(\F): T\text{ preserves }f \right\rbrace 
        \end{equation*}
        is a group under usualy matrix composition, where $n=\dim(V)$. However, there is an alternative way of describing matrices which preserve $f$.
    \end{remark}

    \begin{cor}{}
        Let $A\in M_{n\times n}(\F)$ be invertible. Then
        \begin{equation*}
            G = \left\lbrace M\in M_{n\times n}(\F): M^TAM = A \right\rbrace 
        \end{equation*}
        is a group under usual matrix composition.
    \end{cor}	

    \begin{proof}
        We have shown that $[]_\beta:\lin\left( V,V,\F \right) \to M_{n\times n}(\F)$ is an isomorphism, where $\dim(V)=n$ and $\beta$ is an ordered basis for $V$, and also that
        \begin{equation*}
            \rank(f) = \rank\left[ f \right] _\beta
        \end{equation*}
        for any bilinear form $f$ on $V$. That is, if $A\in M_{n\times n}(\F)$ is invertible and if we fix an ordered basis $\beta$ for $V$, then there exists unique bilinear form $f$ on $V$ such that $A = \left[ f \right] _\beta$. Moreover, the isomorphism $[]_\beta: \lin(V)\to M_{n\times n}(\F)$, there exists unique linear operator $T:V\to V$ such that $M=\left[ T \right] _\beta$. Observe that
        \begin{equation*}
            f\left( x,y \right) = X^TAY,
        \end{equation*}
        where $X=\left[ x \right] _\beta$ and $Y=\left[ y \right] _\beta$ by definition. But $A=M^TAM$, so
        \begin{align*}
            f\left( x,y \right) & = X^TAY = X^TM^TAMY = \left( MX \right) ^TA\left( MY \right) \\ 
                                & = \left( \left[ T \right] _\beta\left[ x \right] _\beta \right) ^T \left[ f \right] _\beta\left( \left[ T \right] _\beta\left[ y \right] _\beta \right) = \left[ Tx \right] _\beta^T\left[ f \right] _\beta\left[ Ty \right] _\beta = f\left( Tx,Ty \right) ,
        \end{align*} 
        which means $T$ preserves $f$. Since the set $G_f$ of linear operators preserving $f$ is a group, $G$ is also a group by isomorphism $[]_\beta$. 
    \end{proof}

    \begin{remark}
        Consider the case which $f$ is a symmetric nondegenerate bilinear form over a vector space $V$ over a subfield $\F\subset\CC$. Then a linear operator $T$ on $V$ preserves $f$ if and only if $T$ preserves 
        \begin{equation*}
            q(v) = f(v,v),
        \end{equation*}
        the quadratic form associated with $f$. This can be verified as follows. If $T$ preserves $f$, then it is clear that
        \begin{equation*}
            q\left( Tv \right) = f\left( Tv,Tv \right) = f\left( v,v \right) = q\left( v \right) .
        \end{equation*}
        Conversely, if $T$ preserves $q$, then by the polarization identity
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right) , 
        \end{equation*}
        $T$ preserves $f$.
    \end{remark}
    
    
    

    








\end{document}
