\documentclass[math_245.tex]{subfiles}

\begin{document}

    \chap{Bilinear Form} 

    \section{Bilinear Forms}

    \ruleline{Notation}
    \noindent Unless otherwise specified, let $V$ denote a finite-dimensional vector space.

    \ruleline{Notation}

    \begin{recall}{Linear Functional}{on a Vector Space}
        Let $V$ be a vector space. We say a function $L:V\to\F$ is a \emph{linear functional} on $V$ if $L$ is linear.
    \end{recall}

    \begin{definition}{Bilinear Form}{on a Vector Space}
        Let $V$ be a vector space. We say a function $f:V\times V\to\F$ is a \emph{bilinear form} on $V$ if
        \begin{equation*}
            \begin{cases}
                f(cv+u,w) = cf(v,w)+f(u,w) \\
                f(v,cu+w) = cf(v,u)+f(v,w) \\
            \end{cases}
        \end{equation*}
        In other words, $f(v,u)$ is bilinear if $f$ is a linear functional of $v$ when $u$ is fixed and vice versa.
    \end{definition}

    \begin{example}
        Clearly the zero function $0:V\times V\to \F$ is a bilinear form on $V$.
    \end{example}

    \begin{remark}
        Let $f,g:V\times V\to\F$ be bilinear and let $c\in\F$. Then
        \begin{equation*}
            cf+g:V\times V\to\F
        \end{equation*}
        is also bilinear. In fact, if $f_1,f_2,\ldots,f_n: V\times V\to\F$ are bilinear, then
        \begin{equation*}
            \sum^{n}_{i=1} c_if_i: V\times V\to\F 
        \end{equation*}
        is also bilinear. Together with Example 4.1, it follows that the set of bilinear forms on $V$, denoted as $\lin\left( V,V,\F \right)$, is a subspace of the vector space $\F^{V\times V}$.
    \end{remark}

    \begin{example}
        Let $V$ be a vector space and let $L_1,L_2:V\to\F$ be linear functionals on $V$. Then
        \begin{equation*}
            f(v,u) = L_1(v)L_2(u)
        \end{equation*}
        is bilinear, since $f$ is a linear functional of $v$ when $u$ is fixed and vice versa.
    \end{example}

    \begin{example}
        Let $m,n\in\N$ and let $V=M_{m\times n}(\F)$. Let $A\in M_{m\times m}(\F)$. Define
        \begin{equation*}
            f_A(X,Y) = \tr\left( X^TAY \right),
        \end{equation*}
        then $f_A$ is a linear functional on $V$. For, if $X, Y, Z\in V$ and $c\in\F$, then
        \begin{equation*}
            f_A\left( cX+Y,Z \right) = \tr\left( \left( cX+Y \right) ^TAZ \right) = \tr\left( cX^TAZ \right) + \tr\left( cY^TAZ \right) = f_A\left( cX,Z \right) + f_A\left( cY, Z \right).
        \end{equation*}
        In particular, when $n=1$, the matrix $X^TAY$ is $1\times 1$, and the bilinear form is simply
        \begin{equation*}
            f_A(X,Y) = X^TAY = \sum^{}_{i,j} A_{ij}X_iY_j.
        \end{equation*}
        We shall presently show that every bilinear form on the space $M_{m\times 1}(\F)$ is of this type, for some $A\in M_{m\times m}(\F)$.
    \end{example}

    \begin{example}
        Let us find all bilinear forms on $\F^2$. Suppose $f:\F^2\times\F^2\to\F$ is bilinear. If we let
        \begin{equation*}
            v = \left( v_1,v_2 \right) , u = \left( u_1,u_2 \right) \in\F^2,
        \end{equation*}
        then
        \begin{align*}
            f(v,u) & = f\left( v_1e_1+v_2e_2, u \right) = v_1f\left( e_1,u \right) + v_2f\left( e_2,u \right) = v_1f\left( e_1,u_1e_1+u_2e_2 \right) + v_2f\left( e_2,u_1e_1+u_2e_2 \right) \\
                   & = v_1u_1f\left( e_1,e_1 \right) + v_1u_2f\left( e_1,e_2 \right) + v_2u_1f\left( e_2,e_1 \right) + v_2u_2f\left( e_2e_2 \right) ,
        \end{align*} 
        where each $e_i\in\F^2$ is the $i$th element of the standard ordered basis for $\F^2$. That is, $f$ is completely determined by $a_{ij}=f(e_i,e_j)$'s, such that
        \begin{equation*}
            f(v,u) = \sum^{}_{i,j} a_{ij}v_iu_j.
        \end{equation*}
        If $X$ and $Y$ are the coordinate matrices of $v$ and $u$, respectively, and if $A\in M_{2\times 2}(\F)$ with entries
        \begin{equation*}
            A_{ij} = a_{ij} = f\left( e_i,e_j \right) ,
        \end{equation*}
        then
        \begin{equation*}
            f(v,u) = X^TAY.
        \end{equation*}
        Thus we see that any biliear form on $\F^2$ is precisely of the form which we discussed in Example 4.4.
    \end{example}

    \begin{remark}
        We may generalize the results of Example 4.5 as follows. Let $\beta = \left\lbrace v1,v_2,\ldots,v_n \right\rbrace $ be an ordered basis for $V$ and let $f:V\times V\to\F$ be bilinear. If
        \begin{equation*}
            x = \sum^{n}_{i=1} x_iv_i, y = \sum^{n}_{i=1} y_iv_i\in V
        \end{equation*}
        for some $x_1,x_2,\ldots,x_n, \ldots, y_n\in\F$, then
        \begin{equation*}
            f(x,y) = f\left( \sum^{}_{i} x_iv_i,y \right) = \sum^{}_{i} x_if\left( v_i,y \right) = \sum^{}_{i} x_if\left( v_i, \sum^{}_{j} y_jv_j \right) = \sum^{}_{i,j} x_iy_jf\left( v_i,v_j \right) . 
        \end{equation*}
        That is, if we let $A_{ij} = f\left( v_i,v_j \right)$, then
        \begin{equation*}
            f\left( x,y \right) = \sum^{}_{i,j} A_{ij}x_iy_j = X^TAY
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrices of $x$ and $y$ in the ordered basis $\beta$ for $V$, respectively. Thus every bilinear form on $V$ is of the type
        \begin{equation*}
            f\left( x,y \right) = \left[ x \right] _\beta A\left[ y \right] _\beta
        \end{equation*}
        for some $A\in M_{n\times n}(\F)$ and an ordered basis $\beta$ for $V$. Conversely, if $A\in M_{n\times n}(\F)$ is given, then clearly the above equation defines a bilinear form $f:V\times V\to\F$ such that
        \begin{equation*}
            A_{ij} = f\left( v_i,v_j \right) .
        \end{equation*}
        This motivates the following definition.
    \end{remark}

    \begin{definition}{Matrix}{of a Bilinear Function with Respect to an Ordered Basis}
        Let $f:V\times V\to\F$ be bilinear and let $\beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace$ be an ordered basis for $V$. Then we define the \emph{matrix} of $f$ with respect to $\beta$ by
        \begin{equation*}
            \left( \left[ f \right] _\beta \right) _{ij} = f\left( v_i,v_j \right) .
        \end{equation*}
    \end{definition}
    
    \begin{theorem}{$[]_\beta:\lin\left( V,V,\F \right)\to M_{n\times n}(\F)$ Is an Isomorphism}
        Let $\beta$ be an ordered basis for $V$. Then $[]_\beta:\lin\left( V,V,\F \right)\to M_{n\times n}(\F)$ is an isormorphism.
    \end{theorem}

    \begin{proof}
        The bijectivity of $[]_\beta$ is supplied by Remark 4.6. To verify the linearity, let $f,g\in\lin\left( V,V,\F \right)$ and $c\in\F$. Then
        \begin{equation*}
            \left( \left[ cf+g \right] _\beta \right) _{ij} = \left( cf+g \right) \left( v_i,v_j \right) = cf\left( v_i,v_j \right) + g\left( v_i,v_j \right) = \left( c\left[ f \right]_\beta  \right)_{ij} + \left( \left[ g \right]_\beta  \right) _{ij}.
        \end{equation*}
        But this exactly means
        \begin{equation*}
            \left[ cf+g \right] _\beta = c\left[ f \right] _\beta + \left[ g \right] _\beta,
        \end{equation*}
        as desired.
    \end{proof}

    \begin{recall}{Dual}{of an Ordered Basis}
        Let $\beta = \left\lbrace v_1,v_2,v_n \right\rbrace $ be an ordered basis for $V$. Then the \emph{dual} of $\beta$, denoted as $\beta^* = \left\lbrace L_1,L_2,\ldots,L_n \right\rbrace $ is such that
        \begin{equation*}
            L_i\left( v \right) = \left( \left[ v \right] _\beta \right) _i
        \end{equation*}
        for all $v\in V$ and $i\in\left\lbrace 1,2,\ldots,n \right\rbrace$.
    \end{recall}

    \begin{cor}{}
        Let $\beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace$ be a basis for $V$ and let $\beta^* = \left\lbrace L_1,L_2,\ldots,L_n \right\rbrace $ be the dual of $\beta$. Then
        \begin{equation*}
            \left\lbrace f_{ij} = L_iL_j: i,j\in\left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace 
        \end{equation*}
        is a basis for $\lin(V,V,\F)$. In particular,
        \begin{equation*}
            \dim\left( \lin\left( V,V,\F \right)  \right) = n^2.
        \end{equation*}
    \end{cor}	

    \begin{proof}
        For convenience, write
        \begin{equation*}
            \alpha = \left\lbrace f_{ij}=L_iL_j: i,j\in \left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace .
        \end{equation*}
        Notice that each $f_{ij}$ is defined by
        \begin{equation*}
            f_{ij}\left( x,y \right) = L_i(x)L_j(y)
        \end{equation*}
        is a bilinear form on $V$ by Example 4.3. That is, if
        \begin{equation*}
            x = \sum^{n}_{i=1} x_iv_i, y = \sum^{n}_{i=1} y_iv_i\in V,
        \end{equation*}
        then
        \begin{equation*}
            f_{ij} (x,y) = x_iy_i.
        \end{equation*}
        Now, let $f:V\times V\to\F$ be bilinear and let $A = \left[ f \right] _\beta$. Then
        \begin{equation*}
            f\left( x,y \right) = \sum^{}_{i,j} A_{ij}x_iy_j
        \end{equation*}
        which exactly means
        \begin{equation*}
            f = \sum^{}_{i,j} A_{ij}f_{ij}.
        \end{equation*}
        Thus $\alpha$ is a basis for $\lin\left( V,V,\F \right)$, as required.
    \end{proof}

    \begin{remark}
        In terms of matrix point of view, one may rephrase Corollary 4.1.1 as follows: the matrix of each $f_{ij}$ with respect to $\beta$ is such that
        \begin{equation*}
            \left( \left[ f_{ij} \right] _\beta \right) _{rs} =
            \begin{cases}
                1 & \text{ if } r = i \land s = j \\
                0 & \text{ otherwise} 
            \end{cases}.
        \end{equation*}
        In other words,
        \begin{equation*}
            \left\lbrace \left[ f_{ij} \right] _\beta : i,j\in \left\lbrace 1,2,\ldots,n \right\rbrace  \right\rbrace 
        \end{equation*}
        is a set of $n\times n$ matrices whose entries are all zero except for the entry $\left( \left[ f_{ij} \right] _\beta \right) _{ij} = 1$, which clearly is a basis for $M_{n\times n}(\F)$. Thus it follows from Theorem 4.1 that $\alpha$ is a basis for $\lin\left( V,V,\F \right)$.
    \end{remark}

    \begin{remark}
        The concept of the matrix of a bilinear form in an ordered basis is similar to that of the matrix representation of a linear operator. Just as for linear operators, we shall be interested in what happens to the matrix representing a bilinear form, as we change from one ordered basis to another. So suppose
        \begin{equation*}
            \beta = \left\lbrace v_1,v_2,\ldots,v_n \right\rbrace, \gamma = \left\lbrace u_1,u_2,\ldots,u_n \right\rbrace \subseteq V 
        \end{equation*}
        are ordered basis for $V$ and let $f:V\times V\to\F$ be bilinear. How are the matrices $[f]_\beta$ and $[f]_\gamma$ related? First, write
        \begin{equation*}
            u_i = \sum^{n}_{j=1} a_{ij}v_j 
        \end{equation*}
        for each $j\in\left\lbrace 1,2,\ldots,n \right\rbrace $. For all $v\in V$, there exists $c_1,c_2, \ldots,c_n\in\F$ such that
        \begin{equation*}
            v = \sum^{n}_{i=1} c_iu_i.
        \end{equation*}
        That is,
        \begin{equation*}
            v = \sum^{n}_{i=1} c_iu_i = \sum^{n}_{i=1} c_i\sum^{n}_{j=1} a_{ij}u_j = \sum^{}_{i,j} c_ia_{ij}v_j,
        \end{equation*}
        which means
        \begin{align*}
            \left( \left[ v \right] _\beta \right) _j & = \sum^{n}_{i=1} c_ia_{ij} \\ 
            \left( \left[ v \right] _\gamma \right) _j & = c_j.
        \end{align*} 
        So if $P_{ij} = a_{ij}$, then
        \begin{equation*}
            \left[ v \right] _\beta = P\left[ v \right] _\gamma.
        \end{equation*}
        This matrix is unique, and for all $v,u\in V$,
        \begin{equation*}
            f\left( v,u \right) = \left[ v \right] ^T_\beta \left[ f \right] _\beta \left[ v \right] _\beta = \left( P\left[ u \right] _\gamma \right) ^T \left[ f \right] _\beta P\left[ v \right] _\gamma = \left[ u \right] ^T_\gamma \left( P^T\left[ f \right] _\beta P \right) \left[ v \right] _\gamma.
        \end{equation*}
        Thus by the definition and uniqueness of the matrix $\left[ f \right] _\gamma$, it follows that
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P.
        \end{equation*}
    \end{remark}

    \begin{example}
        Define $f:\R^2\times\R^2\to\R$ by
        \begin{equation*}
            f\left( x,y \right) =x_1y_1+x_1y_2+x_2y_1+x_2y_2
        \end{equation*}
        provided that $x=\left( x_1,x_2 \right), y=\left( y_1,y_2 \right) \in\R^2$. Then $x=x_1e_1+x_2e_2$ and $y=y_1e_1+y_2e_2$ where $\beta = \left\lbrace e_1,e_2 \right\rbrace$ is the standard ordered basis for $\R^2$. So
        \begin{equation*}
            f\left( e_1,e_1 \right) =f\left( e_1,e_2 \right) =f\left( e_2,e_1 \right) =f\left( e_2,e_2 \right) = 1
        \end{equation*}
        which means
        \begin{equation*}
            \left[ f \right] _\beta =
            \begin{bmatrix}
                1 & 1 \\
                1 & 1 \\
            \end{bmatrix}.
        \end{equation*}
        Now let $v_1=\left( 1,-1 \right), v_2=\left( 1,1 \right) \in\R^2$ and let $\gamma = \left\lbrace v_1,v_2 \right\rbrace$ be an ordered basis for $\R^2$. If $P\in M_{2\times 2}(\R)$ is the change of basis matrix, then
        \begin{align*}
            v_1 = \left( 1,-1 \right) = P_{11}e_1 + P_{21}e_2 & \implies P_{11}=1, P_{21} = -1 \\
            v_2 = \left( 1,1 \right) = P_{11}e_1 + P_{21}e_2 & \implies P_{12} = P_{22} = 1 .
        \end{align*} 
        Thus,
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P =
            \begin{bmatrix}
                1 & -1 \\ 1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\ 1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\ -1 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0 \\ 0 & 4
            \end{bmatrix}.
        \end{equation*}
        What this means is that, if $x = x_1v_1 + x_2v_2$ and $y = y_1v_1+y_2v_2$, then
        \begin{equation*}
            f\left( x,y \right) = 4x_2y_2.
        \end{equation*}
    \end{example}

    \begin{remark}
        One consequence of the change of basis equation
        \begin{equation*}
            \left[ f \right] _\gamma = P^T\left[ f \right] _\beta P
        \end{equation*}
        is the following. If $A,B\in M_{n\times n}(\F)$ represents the same bilinear form on $V$, then $A$ and $B$ have the same rank. For, if
        \begin{equation*}
            B = P^TAP
        \end{equation*}
        for some invertible $P\in M_{n\times n}(\F)$, it is clear that $A$ and $B$ have the same rank. This makes it possible to define the rank of a bilinear form $f$ on $V$ to be the rank of $\left[ f \right] _\beta$ for some ordered basis $\beta$ on $V$. However, it is more desirable to give more intrinsic definition of the rank of a bilinear form. This can be done as follows. Suppose $f\left( v,u \right)$ is a bilinear form on $V$. If we fix $v\in V$, then $f(v,u)$ becomes a linear functional on $V$; let us denote this by $L_f(v)$. This provides us a linear transformation $L_f:V\to V^*$ by the mapping
        \begin{equation*}
            v\mapsto L_f(v).
        \end{equation*}
        On the other hand, if we fix $u\in V$, then we also get a linear functional $R_f(u):V\to \F$, and $R_f$ is a linear transformation. After we prove that
        \begin{equation*}
            \rank\left( L_f \right) = \rank\left( R_f \right),
        \end{equation*}
        we shall define the rank of a bilinear form on a finite-dimensional vector space to be the rank of the associated linear transformations.
    \end{remark}

    \begin{prop}{$\rank\left( L_f \right) = \rank\left( R_f \right)$}
        Let $f$ be a bilinear form on $V$ and define linear transformations
        \begin{equation*}
            L_f,R_f:V\to V^*
        \end{equation*}
        as Remark 4.10. Then $\rank\left( L_f \right) = \rank\left( R_f \right)$.
    \end{prop}

    \begin{proof}
        To prove $\rank\left( L_f \right) = \rank\left( R_f \right)$, it is sufficient to prove that $\nullity\left( L_f \right) = \nullity\left( R_f \right)$ by rank-nullity theorem. Let $\beta$ be an ordered basis for $V$ and let $A=\left[ f \right] _\beta$. Then
        \begin{equation*}
            f\left( x,y \right) = X^T\left[ f \right] _\beta Y,
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrix of $x$ and $y$, respectively. So if $L_f(x)=0$, then
        \begin{equation*}
            \left( X^T\left[ f \right] _\beta \right) Y = 0
        \end{equation*}
        for any $Y\in M_{n\times 1}(\F)$. Clearly this means
        \begin{equation*}
            X^T\left[ f \right] _\beta = 0,
        \end{equation*}
        or, equivalently,
        \begin{equation*}
            \left[ f \right] ^T\beta X = 0.
        \end{equation*}
        This means
        \begin{equation*}
            \nullity\left( L_f \right) = \dim \left\lbrace X\in M_{n\times 1}(\F): \left[ f \right] ^T_\beta X = 0  \right\rbrace  .
        \end{equation*}
        Similar argument shows that
        \begin{equation*}
            \nullity\left( R_f \right) = \dim\left\lbrace Y\in M_{n\times 1}(\F): \left[ f \right] _\beta Y = 0  \right\rbrace .
        \end{equation*}
        Simce $\left[ f \right] ^T_\beta$ and $\left[ f \right] _\beta$ have the same column rank, they have the same dimension of solution space of homogeneous equations. That is,
        \begin{equation*}
            \nullity\left( L_f \right) = \nullity\left( R_f \right),
        \end{equation*}
        as desired.
    \end{proof}

    \begin{definition}{Rank}{of a Bilinear Form}
        Let $f$ be a bilinear form on $V$ and let $L_f, R_f: V\to V^*$ be linear transformations provided by Remark 4.10. Then we define the rank of $f$ by
        \begin{equation*}
            \rank\left( f \right) = \rank\left( L_f \right) = \rank\left( R_f \right),
        \end{equation*}
        where the second equality holds by Proposition 4.2.
    \end{definition}

    \begin{cor}{$\rank\left( f \right) =\rank\left[ f \right] _\beta$}
        Let $f$ be a bilinear form on $V$ and let $\beta$ be an ordered basis for $V$. Then
        \begin{equation*}
            \rank\left( f \right) = \rank\left[ f \right] _\beta.
        \end{equation*}
    \end{cor}	

    \begin{proof}
        From the proof of Proposition 4.2,
        \begin{equation*}
            \nullity\left( f \right) = \dim\left\lbrace X\in M_{n\times 1}(\F) : \left[ f \right] _\beta X = 0 \right\rbrace .
        \end{equation*}
        But it is clear that
        \begin{equation*}
            \dim\left\lbrace X\in M_{n\times 1}(\F) : \left[ f \right] _\beta X = 0 \right\rbrace = \nullity\left[ f \right] _\beta.
        \end{equation*}
        Thus $\rank\left( f \right) = \rank\left[ f \right] _\beta$ by rank-nullity theorem, as desired.
    \end{proof}

    \clearpage
    \begin{cor}{}
        Let $f$ be a bilinear form on $V$. Then the following are equivalent.
        \begin{enumerate}
            \item $\rank(f) = \dim(V)$.
            \item $\forall v\in V\setminus \left\lbrace 0 \right\rbrace\exists u\in V \left[ f\left( v,u \right) \neq 0 \right]$.
            \item $\forall u\in V\setminus \left\lbrace 0 \right\rbrace\exists v\in V \left[ f\left( v,u \right) \neq 0 \right]$.
        \end{enumerate}
    \end{cor}	

    \begin{proof}
        Observe that (a), (b), and (c) are all equivalent to the statement
        \begin{equation*}
            \nullity\left( L_f \right) = \nullity\left( R_f \right) = 0. \eqedsym
        \end{equation*}
    \end{proof}

    \begin{definition}{Nondegenerate}{Bilinear Form}
        Let $f$ be a bilinear form on an arbitrary vector space $V$. We say $f$ is \emph{nondegenerate} if it satisfies (b) and (c) of Corollary 4.2.2.
    \end{definition}

    \begin{remark}
        As Corollary 4.2.2 implies, if $f$ is a bilinear form on a finite-dimensional vector space $V$, then $f$ is nondegenerate if it satisfies one of (a), (b), and (c). In particular, $f$ is nondegenerate if and only if $\left[ f \right] _\beta$ is invertible for any ordered basis $\beta$ for $V$.
    \end{remark}

    \begin{definition}{Dot Product}{}
        Let $V = \F^n$ and let $f$ be a bilinear form on $V$ defined by
        \begin{equation*}
            f\left( x,y \right) = \sum^{n}_{i=1} x_iy_i,
        \end{equation*}
        where $x_i$ and $y_i$ are the $i$th entries of $x$ and $y$, respectively. Then $f$ is nondegenerate on $V$. Moreover, if $\beta$ is the standard ordered basis for $\F^n$, then
        \begin{equation*}
            \left[ f \right] _\beta = I.
        \end{equation*}
        That is,
        \begin{equation*}
            f\left( x,y \right) = \left[ x \right] _\beta^T \left[ y \right] _\beta.
        \end{equation*}
        We call such $f$ the \emph{dot product}.
    \end{definition}

    \section{Symmetric Bilienar Form}
    
    \begin{remark}
        The main purpose of this section is to answer the following question: if $f$ is a bilinear form on $V$, when does an ordred basis $\beta$ for $V$ such that $[f]_\beta$ is diagonal exist? We shall prove that such $\beta$ exists if and only if $f\left( v,u \right) = f\left( u,v \right)$ for all $v,u\in V$.  The result is proved only when the field underlying $V$ has characteristic zero.
    \end{remark}

    \begin{definition}{Symmetric}{Bilinear Form}
        Let $f$ be a bilinear form. We say $f$ is \emph{symmetric} if
        \begin{equation*}
            f\left( v,u \right) = f\left( u,v \right) 
        \end{equation*}
        for all $v,u\in V$.
    \end{definition}

    \begin{recall}{Characteristic}{of a Field}
        Let $\F$ be a field. We say $n\in\N$ is the \emph{characteristic} of $\F$, denoted as $\char\left( \F \right)$, if $n$ is the minimum positive integer such that adding 1 $n$ times is zero,
        \begin{equation*}
            1 + 1 + \cdots + 1 = 0.
        \end{equation*}
        If no such $n$ exists, we say $\F$ has \emph{characteristic zero}.
    \end{recall}

    \begin{remark}
        If $V$ is finite-dimensional, then a bilinear form $f$ is symmetric if and only if $\left[ f \right] _\beta$ is symmetric for some ordered basis $\beta$ for $V$. To see this, first suppose that $f$ is symmetric. Then,
        \begin{equation*}
            f\left( x,y \right) = X^TAY
        \end{equation*}
        where $X$ and $Y$ are the coordinate matrices of $x$ and $y$, respectively. Since $f$ is symmetric, $f(x,y) = f(y,x)$ for any $x,y\in V$, so
        \begin{equation*}
            X^TAY = Y^TAX.
        \end{equation*}
        But $X^TAY, Y^TAX\in M_{1\times 1}(\F)$, which means
        \begin{equation*}
            X^TAY = Y^TAX = \left( Y^TAX \right) ^T = X^TA^TY
        \end{equation*}
        for any $X,Y\in M_{n\times 1}(\F) $. Thus it is clear from the above equation that $A=A^T$. The converse statement is clear, since $X^TAY, Y^TAX\in M_{1\times 1}(\F)$, so
        \begin{equation*}
            X^TAY = \left( X^TAY \right) ^T = Y^TA^TX = Y^TAX.
        \end{equation*}
        One particular result is that, if $\left[ f \right] _\beta$ is diagonal for some ordred basis $\beta$ for $V$, then $f$ is symmetric, since any diagonal matrix is symmetric. Whenver a bilinear form is symmetric, we are allowed to make the following definition.
    \end{remark}

    \begin{definition}{Quadratic Form}{Associated with a Symmetric Bilinear Form}
        Let $f$ be a symmetric bilinear form on $V$. Then we define the \emph{quadratic form} associated with $f$ to be
        \begin{equation*}
            q(v) = f(v,v) : V\to \F
        \end{equation*}
        for all $v\in V$.
    \end{definition}

    \begin{remark}
        If $\F\subseteq\CC$ is a subfield, the symmetric bilinear form $f$ is completely determined by its associated quadratic form, that
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right) .
        \end{equation*}
        The estabilishment of the above equation is a routine computation, so we shall omit it.
    \end{remark}

    \begin{definition}{Polarization Identity}{}
        We call the equation
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right), 
        \end{equation*}
        where $f$ is a symmetric bilinear form on $V$ over a subfield of $\CC$ and $q$ is the quadratic form of $f$, the \emph{polarization identity}.
    \end{definition}

    \begin{example}
        Suppose the bilinear form $f$ over $\F^n$ is the dot product. Then the associated quadratic form is
        \begin{equation*}
            q\left( x_1,x_2,\ldots,x_n \right) = \sum^{n}_{i=1} x^2_i.
        \end{equation*}
        In other words, the geometric interpretation is that $q(x)$ is the square of the length of $v$. Moreover, notice that for any symmetric bilinear form $f_A(x,y) = XA^TY$, the associated quadratic form is
        \begin{equation*}
            q_A(x) = X^TAX = \sum^{}_{i,j} A_{ij}x_ix_j.
        \end{equation*}
    \end{example}

    \begin{remark}
        One important class of symmetric bilinear forms consists of inner products on a vector space over $\R$ or $\CC$.
    \end{remark}

    \begin{definition}{Inner Product}{}
        Let $V$ be a vector space over a subfield of $\CC$. An \emph{inner product} $f$ on $V$ is a symmetric bilinear form which is positive definite. That is,
        \begin{equation*}
            q(v) = f\left( v,v \right) > 0
        \end{equation*}
        for any nonzero $v\in V$.
    \end{definition}

    \begin{definition}{Orthogonal}{Vectors}
        Let $v,u\in V$. We say $v$ and $u$ are \emph{orthogonal} with respect to an inner product $f$ if
        \begin{equation*}
            f(v,u) = 0.
        \end{equation*}
    \end{definition}

    \continued{Remark}
    \noindent The motivation for the definition of orthogonal vectors is clear: notice that if $f$ is the dot product - which is a special form of inner products -, then
    \begin{equation*}
        f(v,u) = 0
    \end{equation*}
    whenenver $v$ and $u$ are orthogonal. The above definition is a generalization of this result. The quadratic form of an inner product $q(v) = f(v,v)$ takes only nonnegative values by positive definiteness, and is usually thought as the square of the length of $v$. More discussions of inner product will be done in the following chapter.

    \begin{theorem}{If $f$ Is a Symmetric Bilinear Form, Then $[f]_\beta$ Is Diagonal for Some Ordered Basis $\beta$}
        Let $V$ be a finite-dimensional vector space over $\F$ of characteristic zero and let $f$ be a symmetric bilinear form on $V$. Then there exists an ordered basis $\beta$ for $V$ such that $[T]_\beta$ is diagonal.
    \end{theorem}

    \begin{proof}
        We proceed inductively. Observe that if $f=0$, then the proof is trivial, so suppose that $f$ is nonzero. When $\dim\left( V \right) =1$, then $[f]_\beta$ is diagonal for any ordered basis $\beta$ for $V$. Now suppose that the result holds for any bilinear form $f$ on a $k$-dimensional vector space. Let $V$ be a vector space with $\dim(V)=k+1$. Then there exists $v_{k+1}\in V$ such that $f\left( v_{k+1},v_{k+1} \right) = q\left( v_{k+1} \right) \neq 0$, where $q$ is the quadratic form of $f$, since any symmetric bilinear form can be written as
        \begin{equation*}
            f\left( v,u \right) = \frac{1}{4}q\left( v+u \right) - \frac{1}{4}q\left( v-u \right) 
        \end{equation*}
        by the polarization identity, and it is clear from the above equation that $q(v_{k+1})\neq 0$ for some $v_{k+1}\in V$. Let $W = \spn\left( v_{k+1} \right)$, then $\dim\left( W \right) =1$. Moreover, let
        \begin{equation*}
            W_\perp = \left\lbrace w\in V: f\left( v_{k+1},w \right) = 0 \right\rbrace .
        \end{equation*}
        Now the claim is that $V = W\oplus W_\perp$. To verify this, first observe that $W_\perp$ is a subspace of $V$. For, $f(v_{k+1}, 0) = 0$ and if $w_1,w_2\in W_\perp$ and $c\in\F$, then
        \begin{equation*}
            f\left( v_{k+1}, cw_1+w_2 \right) = cf\left( v_{k+1}, w_1 \right) + f\left( v_{k+1}, w_2 \right) = 0
        \end{equation*}
        so $cw_1+w_2\in W_\perp$, which means $W_\perp$ is closed under addition and scalar multiplication. Therefore, $W_\perp$ is a subspace of $V$. It is clear that $W$ and $W_\perp$ are independent, since if $w\in W$, then $w=cv$ for some $c\in\F$. So if $w=cv\in W_\perp$, then
        \begin{equation*}
            f\left( v,cv \right) = cf\left( v,v \right) = 0
        \end{equation*}
        which means $c=0$ and $w=cv=0$. To see that every $v\in V$ can be written as
        \begin{equation*}
            v = w + w_\perp
        \end{equation*}
        for some $w\in W$ and $w_\perp\in W_\perp$, observe that if we define
        \begin{equation*}
            w_\perp = v - \frac{f\left( v,v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v,
        \end{equation*}
        then
        \begin{equation*}
            f\left( v_{k+1}, w_\perp \right) = f\left( v_{k+1}, v \right) - \frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }f\left( v_{k+1},v_{k+1} \right) 
        \end{equation*}
        and since $f$ is symmetric, $f\left( v, w_\perp \right) = 0$. That is, $w_\perp\in W_\perp$. In other words, every $v\in V$ can be written as
        \begin{equation*}
            v = \frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v_{k+1} + w_\perp
        \end{equation*}
        for some $\frac{f\left( v, v_{k+1} \right) }{f\left( v_{k+1},v_{k+1} \right) }v_{k+1}\in W$ and $w_\perp\in W_\perp$. Thus $V=W\oplus W_\perp$, as claimed. By induction hypothesis, $W_\perp$ has an ordered basis $\beta_\perp = \left\lbrace v_1,v_2,\ldots,v_k \right\rbrace $ such that $\left[ f_\perp \right] _{\beta_\perp}$ is diagonal. But this exactly means
        \begin{equation*}
            \left( \left[ f_\perp \right] _{\beta_\perp} \right) _{ij} = f\left( v_i,v_j \right) = 0
        \end{equation*}
        whenenver $i\neq j$. Thus, if we define $\beta = \left\lbrace v_1,v_2,\ldots,v_k,v_{k+1} \right\rbrace$, then $\beta$ is the ordered basis for $V$ by direct sum decomposition $V = W\oplus W_\perp$, and we have diagonal $\left[ f \right] _\beta$, since
        \begin{equation*}
            \left( \left[ f_\perp \right] _{\beta_\perp} \right) _{ij} = f\left( v_i,v_j \right) = 0
        \end{equation*}
        for all $i,j\in \left\lbrace 1,2,\ldots,k \right\rbrace$ whenever $i\neq j$, and 
        \begin{equation*}
            \left( \left[ f_\perp \right] _{\beta_\perp} \right) _{(k+1)i} = \left( \left[ f_\perp \right] _{\beta_\perp} \right) _{i(k+1)} = f\left( v_{k+1}, v_i \right) = f\left( v_i, v_{k+1} \right) = 0
        \end{equation*}
        for all $i\in \left\lbrace 1,2,\ldots,k \right\rbrace$ by construction.
    \end{proof}





    






\end{document}
