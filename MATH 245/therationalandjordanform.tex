\documentclass[linearalgebraII]{subfiles}

\begin{document}

    \chap{The Rational and Jordan Form} 

    \ruleline{Notations}
    \noindent For simplicity, we shall adapt the following notation for this chapter. Unless otherwise specified, let $V$ denote a finite-dimensional vector space over a field $\F$ and let $T:V\to V$ denote a linear operator on $V$.

    \ruleline{End of Notations}

    \section{Cyclic Subspaces and Annihilators}

    \begin{remark}
        Let $v\in V$ and consider finding the smallest $T$-invariant subspace $W\subseteq V$ that contains $v$. Clearly, $Tv\in W$. Not only that, for any $f\in\F[x]$, $f(T)v\in W$, since
        \begin{equation*}
            \left\lbrace f(T)v:f\in\F[x] \right\rbrace 
        \end{equation*}
        for any $v\in V$ is a subspace of $V$, we conclude that $W = \left\lbrace f(T)v:f\in\F[x] \right\rbrace$ is the smallest $T$-invariant subspace of $V$. This motivates the following definition.
    \end{remark}

    \begin{definition}{$T$-Cyclic Subspace}{Generated by a Vector}
        Let $v\in V$. We say the subspace
        \begin{equation*}
            Z(v;T) = \left\lbrace f(T)v:f\in\F[x] \right\rbrace \subseteq V
        \end{equation*}
        the \emph{$T$-cyclic subspace} generated by $v$.
    \end{definition}

    \begin{remark}
        Sometimes there existst $v\in V$ such that $Z(v;T)=V$. Although this need not be the case for every linear operator $T:V\to V$, it is certainly important to give a name for it.
    \end{remark}

    \begin{definition}{Cyclic Vector}{of a Linear Operator}
        Let $v\in V$. We say $v$ is a \emph{cyclic vector} of $T$ if $Z(v;T)=V$.
    \end{definition}

    \begin{remark}
        An alternative way to define $Z(v;T)$ is that,
        \begin{equation*}
            Z(v;T) = \spn \left\lbrace T^kv:k\in\N\cup\left\lbrace 0 \right\rbrace  \right\rbrace.
        \end{equation*}
        Thus $v$ is a cyclic vector of $T$ whenever $V = \spn\left\lbrace T^kv:k\in\N\cup\left\lbrace 0 \right\rbrace  \right\rbrace$.
    \end{remark}
    
    \begin{example}
        Consider the following examples:
        \begin{enumerate}
            \item $Z(0;T) = \left\lbrace 0 \right\rbrace $.
            \item $\dim \left( Z(v;T) \right) = 1$ if and only if $v$ is an eigenvector of $T$.
            \item $\dim \left( Z(v;I) \right) = 1$ for any $v\in V$. That is, the identity operator of $V$ does not have a cyclic vector if $\dim(V)>1$.
            \item Let $\beta = \left\lbrace e_1,e_2 \right\rbrace$. Suppose a linear operator $T:\F^2\to\F^2$ satisfies
                \begin{equation*}
                    [T]_\beta =
                    \begin{bmatrix}
                        0 & 0 \\ 1 & 0
                    \end{bmatrix}.
                \end{equation*}
                Then clearly $v=(1,0)\in\F^2$ is a cyclic vector of $T$, since
                \begin{equation*}
                    \spn \left\lbrace T^0v, T^1v, \ldots \right\rbrace = \spn \left\lbrace (1,0), (0,1), \ldots \right\rbrace = \F^2.
                \end{equation*}
        \end{enumerate}
    \end{example}

    \begin{remark}
        For any $v\in V$, we shall be interested in linear relations
        \begin{equation*}
            \sum^{k}_{i=0} c_iT^iv = 0
        \end{equation*}
        between $T^0v, T^1v, \ldots, T^kv$. That is, we are interested in the polynomials
        \begin{equation*}
            f = \sum^{k}_{i=0} c_ix^i\in\F[x]
        \end{equation*}
        such that $f(T)v$. Recall the followind definitions.
    \end{remark}

    \begin{recall}{$T$-Annihilator}{of a Vector}
        Let $v\in V$. We call the polynomial ideal
        \begin{equation*}
            M(v;T) = \left\lbrace f\in\F[x]:f(T)v=0 \right\rbrace \subseteq\F[x]
        \end{equation*}
        the \emph{$T$-annihilator} of $v$. Moreover, the unique monic generator of $M(v;T)$, denoted as $m(v;T)$, is also called the \emph{$T$-annihilator} of $v$.
    \end{recall}

    \begin{remark}
        Let $p\in\F[x]$ be the minimal polynomial for $T$. Then $p(T) = 0$ so $p(T)v = 0$. It follows that $m(v;T)\mid p$. One should also note that $\deg \left( m(v;T) \right) > 0$ whenever $v\neq 0$.
    \end{remark}

    \begin{prop}{}
        Let $v\in V$ be nonzero and let $p_v = m(v;T)$. Then the following hold.
        \begin{enumerate}
            \item $\deg\left( p_v \right) = \dim \left( Z(v;T) \right) $.
            \item $\left\lbrace v, Tv, \ldots, T^{k-1}v \right\rbrace$ is a basis for $Z(v;T)$ provided that $\dim \left( Z(v;T) \right) = k$.
            \item If $U:Z(v;T)\to Z(v;T)$ is the linear operator on $Z(v;T)$ induced by $T$, then the minimal polynomial for $U$ is $p_v$.
        \end{enumerate}
    \end{prop}

    \begin{proof}
        We verify (b) first, which verifies (a) as well. Let $z\in Z(v;T)$. Then $z = f(T)v$ for some $f\in\F[x]$. Notice that
        \begin{equation*}
            f = dp_v + r
        \end{equation*}
        for some $d,r\in\F[x]$ satisfying $r=0$ or $\deg(r)<\deg\left( p_v \right)$ by the division algorithm. That is,
        \begin{equation*}
            r = \sum^{k-1}_{i=0} c_ix^i
        \end{equation*}
        for some $c_0, c_1, \ldots, c_{k-1}\in\F$, provided that $\deg\left( p_v \right) > k$. It follows that $f(T)v\in \spn\left\lbrace v, Tv, \ldots, T^{k-1}v \right\rbrace$, since
        \begin{equation*}
            f(T)v = \left( dp_v \right) (T)v + r(T)v = r(T)v.
        \end{equation*}
        Notice that $\left\lbrace v, Tv, \ldots, T^{k-1}v \right\rbrace $ is linearly independent by the minimality of the unique monic generator $p_v = m(v;T)$ of $M(v;T)$. This verifies (b) and, hence, (a). To verify (c), let $z\in Z(v;T)$. Then $z = f(T)v$ for some $f\in\F[x]$. Moreover, $p_v(U)z = p_v(T)z$, since $U$ is the restriction operator on $Z(v;T)$ induced by $T$. Thus
        \begin{equation*}
            p_v(U)z = p_v(T)z = p_v(T)f(T)v = f(T)p_v(T)v = 0,
        \end{equation*}
        as desired.
    \end{proof}

    \begin{definition}{Cyclic Basis}{}
        Notice that (b) of Theorem 3.1 guarantees that, given a cyclic vector $v\in V$ of $T$,
        \begin{equation*}
            \left\lbrace v, Tv, \ldots, T^{n-1}v \right\rbrace 
        \end{equation*}
        is a basis for $V$, provided that $\dim(V)=n$. We call this a \emph{cyclic basis} for $V$.
    \end{definition}

    \begin{remark}
        A particular consequence of Theorem 3.1 is as follows. If $v\in V$ is a cyclic vector of $T$, then
        \begin{equation*}
            \deg(p_v) = \dim \left( Z(v;T) \right) = \dim(V).
        \end{equation*}
        Since $p_v$ divides the characteristic polynomial $p$ of $T$, and
        \begin{equation*}
            \deg(p)\leq\dim(V),
        \end{equation*}
        we have $p_v=p$.
    \end{remark}

    \begin{remark}
        Our plan is to study general linear operator by linear operators which have a cyclic vector. So let $W$ be a $k$-dimensional vector space and consider a linear operator $U:W\to W$ which has a cyclic vector $w\in W$. Then
        \begin{equation*}
            \beta = \left\lbrace T^iw: i\in \left\lbrace 0,1,\ldots,n-1 \right\rbrace  \right\rbrace 
        \end{equation*}
        is a basis for $W$ by Theorem 3.1. For convenience, let $w_i = T^iw$. Then the action of $U$ on the ordered basis $\beta$ is
        \begin{equation*}
            Uw_i = w_{i+1}
        \end{equation*}
        for all $i<k-1$, and
        \begin{equation*}
            Uw_{k-1} = - \sum^{k-1}_{i=0} c_iw_i
        \end{equation*}
        provided that $p_v = x^k + \sum^{k-1}_{i=0} c_ix^i\in\F[x]$. This is because
        \begin{equation*}
            p_v(U)w = U^kw + \sum^{k-1}_{i=0} c_iU^iw = 0.
        \end{equation*}
        Thus the matrix representation of $U$ in $\beta$ is
        \begin{equation*}
            [U]_\beta = 
            \begin{bmatrix}
                0 & 0 & \cdots & 0 & -c_0 \\
                1 & 0 & \cdots & 0 & -c_1 \\
                0 & 1 & \cdots & 0 & -c_2 \\
                \vdots & \vdots & \ddots  & \vdots & \vdots \\
                0 & 0 & \cdots & 1 & -c_{k-1} \\
            \end{bmatrix}.
        \end{equation*}
        This motivates the following definition.
    \end{remark}

    \begin{definition}{Companion Matrix}{of a Monic Polynomial}
        Let $p = \sum^{k}_{i=0} c_ix^i\in\F[x]$ be monic. We define the \emph{companion matrix} of $p$ as
        \begin{equation*}
            \begin{bmatrix}
                0 & 0 & \cdots & 0 & -c_0 \\
                1 & 0 & \cdots & 0 & -c_1 \\
                0 & 1 & \cdots & 0 & -c_2 \\
                \vdots & \vdots & \ddots  & \vdots & \vdots \\
                0 & 0 & \cdots & 1 & -c_{k-1} \\
            \end{bmatrix}.
        \end{equation*}
    \end{definition}

    \clearpage
    \begin{theorem}{$U$ Has a Cyclic Vector If and Only If There Exists $\beta$ Such That $[U]_\beta$ Is the Companion Matrix of the Minimal Polynomial}
        Let $W$ be a finite-dimensional vector space and let $U:W\to W$ be a linear operator. Then $U$ has a cyclic vector if and only if there exists an ordered basis $\beta$ for $W$ such that $[U]_\beta$ is the companion matrix of the minimal polynomial.
    \end{theorem}

    \begin{proof}
        Notice that the forward direction is supplied by Remark 3.8. To prove the reverse direction, suppose that there exists an ordered basis $\beta = \left\lbrace w_1, w_2,\ldots, w_n \right\rbrace $ for $W$ such that
        \begin{equation*}
            [U]_\beta =
            \begin{bmatrix}
                0 & 0 & \cdots & 0 & -c_0 \\
                1 & 0 & \cdots & 0 & -c_1 \\
                0 & 1 & \cdots & 0 & -c_2 \\
                \vdots & \vdots & \ddots  & \vdots & \vdots \\
                0 & 0 & \cdots & 1 & -c_{k-1} \\
            \end{bmatrix}
        \end{equation*}
        where $p = \sum^{k}_{i=0} c_ix^i$ is the minimal polynomial for $U$. Then it is clear that $w_1\in\beta$ is a cyclic vector of $U$.
    \end{proof}

    \begin{cor}{}
        If $A\in M_{n\times n}(\F)$ is the companion matrix of a monic $p\in\F[x]$, then $p$ is both the minimal and the characteristic polynomial of $A$.
    \end{cor}	

    \begin{proof}
        By the isomorphism between $M_{n\times n}(\F)$ and $\lin(\F^n)$, there exists a unique linear operator $T:\F^n\to\F^n$ such that
        \begin{equation*}
            [T]_\beta = A
        \end{equation*}
        where $\beta = \left\lbrace e_1, e_2, \ldots. e_n \right\rbrace$ is the standard ordered basis for $\F^n$. Then clearly $e_1$ is a cyclic vector of $T$, since $T^ke_1 = e_{k+1}$ so $\left\lbrace e_1, Te_1, \ldots, T^{n-1}e_1 \right\rbrace$ is a basis for $\F^n$. Moreover, $p$ is the $T$-annihilator of $e_1$, since
        \begin{equation*}
            p(T)e_1 = 0
        \end{equation*}
        by definition, and, since $\left\lbrace e_1, Te_1, \ldots, T^{n-1}e_1 \right\rbrace$ is a basis for $\F^n$, any $f\in\F[x]$ such that
        \begin{equation*}
            f(T)e_1 = 0
        \end{equation*}
        satisfies $\deg(f)\geq n$. But $\deg(p) = n$ and $p$ is monic, so $p$ must be the $T$-annihilator of $e_1$. Furthermore, $Z(e_1;T)=V$, so $T$ itself is the linear operator induced on $Z(e_1;T)$ by $T$. So the minimal polynomial for $T$ is $p$ by Theorem 3.1. Thus by the Cayley-Hamilton theorem, $p$ is also the characteristic polynomial of $T$, as required.
    \end{proof}

    \begin{remark}
        Let $v\in V$. Then the linear operator $U:Z(v;T)\to Z(v;T)$ induced by $T$ on $Z(v;T)$ has a cyclic vector, namely $v$. That is, there exists an ordered basis $\beta$ for $Z(v;T)$ such that $[U]_\beta$ is the companion matrix of $p_v$, the $T$-annihilator of $v$.
    \end{remark}

    \ruleline{Exercises}

    \begin{exercise}
        Suppose $\dim(V)=n$ and let $N:V\to V$ be nilpotent, where $N^{n-1}\neq 0$. Further let $v\in V$ be such that $N^{n-1}v\neq 0$. Prove that $v$ is a cyclic vector of $N$.
    \end{exercise}

    \begin{proof}
        For the sake of contradiction, suppose there exists nonzero $\left( c_0, c_1, \ldots, c_{n-1} \right) \in\F^n$ be such that
        \begin{equation*}
            \sum^{n-1}_{i=0} c_iN^iv = 0.
        \end{equation*}
        This is a contradiction, since
        \begin{equation*}
            N^{n-1} \sum^{n-1}_{i=0} c_iN^iv = c_0N^{n-1}v \neq 0.
        \end{equation*}
        Thus $\left\lbrace v, Nv, \ldots, N^{n-1} \right\rbrace$ is linearly independent, which means $v$ is a cyclic vector of $N$, as desired.
    \end{proof}

    \begin{exercise}
        Suppose that $\dim(v)=n$ and that $T$ is diagonalizable.
        \begin{enumerate}
            \item Prove that if $T$ has a cyclic vector, then $T$ has $n$ distinct eigenvalues.
            \item Prove that if $T$ has $n$ distinct eigenvalues and if $\left\lbrace v_1, v_2 ,\ldots, v_n \right\rbrace $ is an eigenbasis for $V$, then
                \begin{equation*}
                    v = \sum^{n}_{i=1} v_i\in V
                \end{equation*}
                is a cyclic vector of $v$.
        \end{enumerate}
    \end{exercise}

    \begin{proof}
        To prove (a), let $v\in V$ be a cyclic vector of $T$. Then the minimal polynomial $p\in\F[x]$ of $T$ is of the form
        \begin{equation*}
            p = \prod^{k}_{i=1} (x-c_i)
        \end{equation*}
        for some $k\leq n$ by Theorem 2.14. Moreover, $p$ is the characteristic polynomial by Remark 3.7, so $k=n$ and $T$ has $n$ distinct eigenvalues. To verify (b), let $c_1, c_2, \ldots, c_n\in\F$ be the eigenvalues corresponding to $v_1, v_2, \ldots, v_n$. Then
        \begin{equation*}
            T^kv = \sum^{n}_{i=1} c_i^kv_i.
        \end{equation*}
        Now the claim is that $\left\lbrace v, Tv, \ldots, T^{n-1}v \right\rbrace$ is linearly independent. To verify this, suppose
        \begin{equation*}
            \sum^{n-1}_{p=0} a_pT^pv = 0
        \end{equation*}
        for some nonzero $\left( a_0, a_1, \ldots, a_{n-1} \right)\in\F^n$ for the sake of contradiction. Then
        \begin{equation*}
            \sum^{n-1}_{p=0} a_p \sum^{n}_{i=1} c_i^pv_i = \sum^{n-1}_{p=0} \sum^{n}_{i=1} a_pc_i^pv_i = \sum^{n}_{i=1} \sum^{n-1}_{p=0} a_pc_i^pv_i = 0.
        \end{equation*}
        Since $v_1, v_2, \ldots, v_n$ are linearly independent, it must be the case that
        \begin{equation*}
            \sum^{n-1}_{p=0} a_pc_i^p = 0
        \end{equation*}
        for all $i\in \left\lbrace 1, 2, \ldots, n \right\rbrace$. That is, we have a nonzero solution to the system
        \begin{equation*}
            \begin{bmatrix}
                1 & c_1 & c_1^2 & \cdots & c_1^{n-1} \\
                1 & c_2 & c_2^2 & \cdots & c_2^{n-1} \\
                \vdots & \vdots & \vdots & \cdots & \vdots \\
                1 & c_n & c_n^2 & \cdots & c_n^{n-1} \\
            \end{bmatrix}
            \begin{bmatrix}
                a_0 \\ a_1 \\ \vdots \\ a_{n-1}
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 \\ 0 \\ \vdots \\ 0
            \end{bmatrix}.
        \end{equation*}
        However, the rows are linearly independent, since $c_1, c_2, \ldots, c_n$ are distinct. Thus the solution set to the system is $\left\lbrace 0 \right\rbrace$, which is a contradiction.
    \end{proof}

    \begin{exercise}
        Suppose that $T$ has a cyclic vector. Prove that if $U:V\to V$ commutes with $T$, then $U$ is a polynomial in $T$.
    \end{exercise}

    \begin{proof}
        Let $v\in V$ be a cyclic vector of $T$. Then by Theorem 3.1, $\left\lbrace v, Tv, \ldots, T^{n-1}v \right\rbrace $ is a basis for $V$, where $n=\dim(V)$. So there exists $f\in\F[x]$ with $\deg(f)\leq n-1$ such that
        \begin{equation*}
            Uv = f(T)v,
        \end{equation*}
        since $Uv\in V$. Moreover, for any $T^k\in \left\lbrace T^1, T^2, \ldots, T^{n-1} \right\rbrace $,
        \begin{equation*}
            UT^kv = T^kUv = T^kf(T)v = f(T)T^kv.
        \end{equation*}
        Since $\left\lbrace v, Tv, \ldots, T^{n-1} \right\rbrace$ is a basis for $V$, it follows that
        \begin{equation*}
            U = f(T),
        \end{equation*}
        as desired.
    \end{proof}
    
    \ruleline{Exercises}

    \section{Cyclic Decomposition and the Rational Form}
    
    \begin{remark}
        The primary purpose of this section is to prove that if $T$ is any linear operator on $V$, then there exist $v_1, v_2, \ldots, v_r\in V$ such that
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z(v_i;T).  
        \end{equation*}
        In other words, we desire to prove that $V$ is a direct sum of $T$-cyclic subspaces, which will show that $T$ is the direct sum of a finite number of linear operators, each of which has a cyclic vector. The cyclic decomposition theorem is deeply related to the following question: which $T$-invariant subspace $W\subseteq V$ has the property that there exists a $T$-invariant $W'\subseteq V$ such that
        \begin{equation*}
            V = W\oplus W'.
        \end{equation*}
    \end{remark}

    \begin{definition}{Complementary}{Subspace}
        Let $W\subseteq V$ be a subspace. We say $W'\subseteq V$ is a \emph{complementary subspace} of $W$ if $V = W\oplus W'$.
    \end{definition}

    \begin{remark}
        Suppose that $V=W\oplus W'$ for some $T$-invariant $W,W'\subseteq V$ and see what we can discover about $W$. Notice that each $v\in V$ is of the form
        \begin{equation*}
            v = w+w'
        \end{equation*}
        for some $w\in W$ and $w'\in W'$. If $f\in\F[x]$, then
        \begin{equation*}
            f(T)v = f(T)w + f(T)w'.
        \end{equation*}
        Since $W$ and $W'$ are $T$-invariant subspaces, it follows that $f(T)v\in W$ if and only if $f(T)w'=0$. Equivalently, $f(T)v\in W$ if and only if $f(T)v=f(T)w$. This motivates the following definition.
    \end{remark}

    \begin{definition}{$T$-Admissible}{Subspace}
        Let $W\subseteq V$ be a $T$-invariant subspace. We say $W$ is \emph{$T$-admissible} if there exists $w\in W$ such that $f(T)v=f(T)w$ whenever $v\in V$ is such that $f(T)v\in W$.
    \end{definition}

    \begin{remark}
        By Remark 3.14, if $W$ is a $T$-invariant and has a complementary subspace, then $W$ is $T$-admissible. One of the consequences of the cyclic decomposition theorem is the converse: the admissibility characterizes those $T$-invariant subspaces with $T$-invariant complements.
    \end{remark}

    \begin{remark}
        Let us indicate how the admissibility is involved in the attempt to obtain a decomposition
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z(v_i;T).
        \end{equation*}
        One basic method is to select $v_1, v_2, \ldots, v_r\in V$ inductively. Suppose that we have selected $v_1, v_2, \ldots, v_j\in V$ and
        \begin{equation*}
            W = \bigoplus^{j}_{i=1} Z(v_i;T)\subsetneq V
        \end{equation*}
        is proper. We desire to find $v_{j+1}\in V$ such that
        \begin{equation*}
            W\cap Z(v_{j+1};T) = \left\lbrace 0 \right\rbrace ,
        \end{equation*}
        because then $\dim\left(W\cap Z(v_{j+1};T) \right) > \dim(W)$, allowing us to come at least one dimension nearer to exhausting $V$. But why such $v_{j+1}\in V$ always exist? Rather than answering this question directly, we observe the following: if $v_1, v_2,\ldots, v_j\in V$ are chosen such that $W=\bigoplus^{j}_{i=1} Z(v_i;T)$ is $T$-admissible, then it is much easier to find a suitable $v_{j+1}\in V$. To explain this, let us take one step back and suppose $W\subsetneq V$ is a proper $T$-invariant subspace, and consider finding a nonzero $v\in V$ such that
        \begin{equation*}
            W\cap Z(v;T) = \left\lbrace 0 \right\rbrace .
        \end{equation*}
        Let $u\in V\setminus W$ and let $f=s(u;W)$, the unique monic generator of the $T$-conductor of $u$ into $W$, $S(u;W)$. Clearly $f(T)u\in W$. Now, if $W$ is $T$-admissible, then there exists $w\in W$ such that
        \begin{equation*}
            f(T)u = f(T)w.
        \end{equation*}
        Let $v=u-w$ and let $g=\F[x]$. Since $u-v=w\in W$, $g(T)u\in W$ if and only if $g(T)v\in W$. That is,
        \begin{equation*}
            S(u;W) = S(v;W)
        \end{equation*}
        and $s(u;W) = f = s(v;W)$, the $T$-conductor of $v$ into $W$. But
        \begin{equation*}
            f(T)v = f(T)(u-w)=f(T)u-f(T)w = 0,
        \end{equation*}
        which means $g(T)v=0$ whenever $g\in S(v;W)$. But since any element of $Z(v;T)$ is of the form $g(T)v$ for some $g\in\F[x]$, it follows that
        \begin{equation*}
            W\cap Z(v;T) = \left\lbrace 0 \right\rbrace 
        \end{equation*}
        which is what we desired.
    \end{remark}

    \begin{theorem}{Cyclic Decomposition Theorem}
        Let $W_0\subsetneq V$ be a proper $T$-invariant subspace. Then there exist nonzero $v_1, v_2, \ldots, v_r\in V$ with respective $T$-annihilators $p_1,p_2, \ldots,p_r\in\F[x]$ that satisfies the following:
        \begin{enumerate}
            \item $V = W_0\oplus \left( \bigoplus^{r}_{i=1} Z(v_i;T) \right) $.
            \item For all $i\in \left\lbrace 2, 3, \ldots, r \right\rbrace$, $p_i\nmid p_{i-1}$.
        \end{enumerate}
        Moreover, the integer $r\in\N$ and the $T$-annihilators $p_1,p_2,\ldots,p_r$ are uniquely determined by (a), (b), and the fact that $v_1, v_2, \ldots, v_r$ are nonzero.
    \end{theorem}

    \begin{lemma_inside}{}
        There exist nonzero $u_1,u_2,\ldots,u_r\in V$ such that
        \begin{enumerate}
            \item $V = W_0+\sum^{r}_{i=1} Z(u_i;T)$ and
            \item if we define $W_k = W_0+\sum^{k}_{i=1} Z(u_i;T)$ for each $k\in\left\lbrace 1,2,\ldots,r \right\rbrace$, then the $T$-conductor $p_k=s(u_i;W_{k-1})$ has maximum degree among all $T$-conductors into the subspace $W_{k-1}$. That is,
                \begin{equation*}
                    \deg\left( p_k \right) = \max_{v\in V}\left( \deg\left( s\left( v;W_:k-1 \right)  \right)  \right) .
                \end{equation*}
        \end{enumerate}
    \end{lemma_inside}

    \begin{proof}
        Clearly, $W_k$ is a proper $T$-invariant subspace for all $k\in \left\lbrace 0,1,\ldots,r-1 \right\rbrace$. So,
        \begin{equation*}
            0 < \max_{v\in V}\left( \deg\left( s\left( v;W_{k-1} \right)  \right)  \right) \leq \dim(V)
        \end{equation*}
        for all $k\in\left\lbrace 1,2,\ldots,r \right\rbrace$, and we can certainly choose $v_k\in V$ such that
        \begin{equation*}
            \deg\left( s\left( v_k;W_{k-1} \right)  \right) = \max_{v\in V} \left( \deg\left( s(v;W_{k-1}) \right)  \right) .
        \end{equation*}
        But $0<\deg\left( s(v_k;W_{k-1}) \right) $ means $v_k\notin W_{k-1}$, and so
        \begin{equation*}
            \dim\left( W_{k-1}+Z(v_k;T) \right) > \dim(W_{k-1}).
        \end{equation*}
        Thus by repeating the above process at most $r$ times, we have the desired result by construction.
    \end{proof}

    \begin{lemma_inside}{}
        Suppose $u_1,u_2,\ldots,u_k\in V$ are nonzero vectors satisfying (a) and (b) of Lemma 3.3.1. Fix $k\in \left\lbrace 1,2,\ldots,r \right\rbrace$. Let $u\in V$ be arbitrary and let $f=s(v_k;W_{k-1})$. If
        \begin{equation*}
            f(T)u = u_0 + \sum^{k-1}_{i=1} g_i(T)u_i
        \end{equation*}
        for some $g_1,g_2,\ldots,g_{k-1}\in\F[x]$ and $u_0\in W_0$, then
        \begin{enumerate}
            \item $f\mid g_i$ for all $i\in\left\lbrace 1,2,\ldots,k-1 \right\rbrace$ and
            \item $u_0=f(T)w_0$ for some $w_0\in W_0$.
        \end{enumerate}
    \end{lemma_inside}

    \begin{proof}
        When $k=1$, notice that $f(T)u=u_0$, so we only have to verify
        \begin{equation*}
            f(T)u = u_0 = f(T)w_0
        \end{equation*}
        for some $w_0\in W_0$, which is true since $W_0$ is $T$-admissible. For each $k\in \left\lbrace 2,3,\ldots,r \right\rbrace$ and $i\in\left\lbrace 1,2,\ldots,k-1 \right\rbrace$, write
        \begin{equation*}
            g_i = fh_i+r_i
        \end{equation*}
        for some $h_i,r_i\in\F[x]$ satisfying $r_i=0$ or $\deg(r_i)<\deg(f)$ by the division algorithm. For the sake of contradiction, suppose that there exists $i\in \left\lbrace 1,2,\ldots,k-1 \right\rbrace$ such that $r_i\neq 0$, and let $j\in\left\lbrace 1,2,\ldots,k-1 \right\rbrace$ be the greatest such index,
        \begin{equation*}
            j = \max\left\lbrace i\in\left\lbrace 1,2,\ldots,k-1 \right\rbrace :r_i\neq 0 \right\rbrace .
        \end{equation*}
        Let
        \begin{equation*}
            w = u - \sum^{k-1}_{i=1} h_i(T)u_i\in V,
        \end{equation*}
        then clearly $w-u = -\sum^{k-1}_{i=1} h_i(T)u_i\in W_{k-1}$. Recall from Remark 3.16 that, this means
        \begin{equation*}
            s(w;W_{k-1}) = s(u;W_{k-1}) = f.
        \end{equation*}
        Moreover, observe that
        \begin{equation*}
            f(T)w = f(T)u - \sum^{k-1}_{i=1} f(T)h_i(T)u_i = u_0 + \sum^{k-1}_{i=1} \left( fh_i+r_i \right) (T)u_i - \sum^{k-1}_{i=1} \left( fh_i \right) (T)u_i = u_0 + \sum^{k-1}_{i=1} r_i(T)u_i.
        \end{equation*}
        Since $r_{j+1}, r_{j+2}, \ldots, r_{k-1}=0$,
        \begin{equation*}
            f(T)w = u_0 + \sum^{j}_{i=1} r_i(T)u_i,
        \end{equation*}
        where $\deg\left(r_j\right)<\deg(f)$. Let $p=s\left( w;W_{j-1} \right) $. Since $W_{k-1}\superseteq W_{j-1}$, it follows that $f = s\left( w;W_{k-1} \right) $ divides $p$. That is,
        \begin{equation*}
            p = fg
        \end{equation*}
        for some $g\in\F[x]$. Then
        \begin{equation*}
            p(T)w = g(T)f(T)w = g(T)u_0 + \sum^{j-1}_{i=1} g(T)r_i(T)u_i + g(T)r_j(T)u_j.
        \end{equation*}
        But $g(T)u_0+\sum^{j-1}_{i=1} g(T)r_i(T)u_i\in W_{j-1}$ and $p(T)w\in W_{j-1}$ by definition, so $g(T)r_j(T)u_j\in W_{j-1}$ as well. This means $p_j\mid gr_j$ so $\deg\left( gr_j \right) \geq \deg\left( s\left( u_j;W_{j-1} \right) \right)$. Furthermore, $\deg\left( s\left( u_j;W_{j-1} \right)  \right) \deg\left( s\left( w;W_{j-1} \right)  \right)$ by the maximality of $s\left( u_j;W_{j-1} \right) $ that (b) of Lemma 3.3.1 guarantees. But $s\left( u_j;W_{j-1} \right) = p_j$ and $s\left( w;W_{j-1} \right) = p = fg$, so
        \begin{equation*}
            \deg\left( gr_j \right) \leq \deg\left( p_j \right) \leq \deg(p) = \deg(fg).
        \end{equation*}
        But this means $\deg\left( r_j \right) \geq \deg(f)$, which is a contradiction. Thus $f$ divides $g_i$ for each $i\in\left\lbrace 1,2,\ldots,r-q \right\rbrace $. It follows that
        \begin{equation*}
            u_0 = u_0 + \sum^{k-1}_{i=1} r_i(T)u_i = f(T)w,
        \end{equation*}
        so by taking $w_0=w\in W_0$, we have $u_0 = f(T)w_0$ for some $w_0\in W_0$, as desired.
    \end{proof}

    \ruleline{Proof of Theorem 3.3 Begins Here}

    \begin{proof}[Proof of Theorem 3.3]
        We first verify the existence part. Let $u_1,u_2,\ldots,u_k\in V$ be nonzero vectors satisfying (a) and (b) of Lemma 3.3.1. Fix $k\in\left\lbrace 1,2,\ldots,r \right\rbrace $ and let $p_k=s\left( u_k;W_{k-1} \right) $, the $T$-conductor of $u_k$ into $W_{k-1}$. Then
        \begin{equation*}
            p_k(T)u_k = p_k(T)w_0 + \sum^{k-1}_{i=1} p_i(T)h_i(T)u_i
        \end{equation*}
        for some $w_0\in W_0$ and $h_1,h_2,\ldots,h_{k-1}\in\F[x]$ by Lemma 3.3.2. Let
        \begin{equation*}
            v_k = u_k - w_0 - \sum^{k-1}_{i=1} h_i(T)u_i.
        \end{equation*}
        Then since
        \begin{equation*}
            p_k(T)v_k = p_k(T)\left( u_k-w_0-\sum^{k-1}_{i=1} h_i(T)u_i \right) = p_k(T)u_k - p_k(T)\left( u_i+\sum^{k-1}_{i=1} h_i(T)u_i \right) = 0,
        \end{equation*}
        we have
        \begin{equation*}
            W_{k-1}\cap Z(v_k;T) = \left\lbrace 0 \right\rbrace .
        \end{equation*}
        That is, since the choice of $k\in\left\lbrace 1,2,\ldots,r \right\rbrace$ is arbitrary, $W_0, Z(v_1;T), Z(v_2;T), \ldots, Z(V_r;T)$ are independent and the sum
        \begin{equation*}
            V = W_0 \oplus \left( \bigoplus^{k}_{i=1} Z\left( v_i;T \right)  \right) 
        \end{equation*}
        is direct, and that the polynomials $p_1,p_2,\ldots,p_r$ are the respective $T$-annihilators of $v_1,v_2,\ldots,v_r$. Therefore, the vectors $v_1,v_2,\ldots,r_r$ determine the subspaces $W_1,W_1,\ldots,W_r$ as do the vectors $u_1,u_2,\ldots,u_r$, and the $T$-conductor $p_k = s\left( v_k;W_{k-1} \right)$ - which really is the $T$-annihilator - is maximal by (b) of Lemma 3.3.1. The vectors $v_1,v_2,\ldots,v_r$ have an additional property that $W_0,Z\left( v_1;T \right) ,Z\left( v_2;T \right), \ldots, Z\left( v_r;T \right) $ are independent. That is,
        \begin{equation*}
            W_k = W_0 \oplus \left( \bigoplus^{k}_{i=1} Z\left( v_i;T \right)  \right) 
        \end{equation*}
        for all $k\in \left\lbrace 1,2,\ldots,r \right\rbrace$. Moreover, since $p_i(T)v_i = 0$ for all $i\in \left\lbrace 1,2,\ldots,r \right\rbrace$ by definition, we have a trivial linear relation
        \begin{equation*}
            p_k(T)v_k = 0 + \sum^{k-1}_{i=1} p_i(T)v_i
        \end{equation*}
        and by Lemma 3.3.2, $p_k\mid p_i$ for all $i<k$. To verify the uniqueness part, suppose there exist nonzero $z_1,z_2,\ldots,z_s\in V$ and the respective $T$-annihilators $q_1,q_2,\ldots,q_s\in\F[x]$ for some $z\in\N$ that satisfy (a) and (b). We first show that $p_1=q_1$. Define
        \begin{equation*}
            S(V;W) = \left\lbrace f\in\F[x]:\forall v\in V \left[ f(T)v\in W \right] \right\rbrace \subseteq \F[x] 
        \end{equation*}
        for any subspace $W\subseteq V$. Now the claim is that $S(V;W)$ is an ideal of $\F[x]$. To verify this, let $\alpha,\beta\in S(V;W)$ and $\gamma\in\F[x]$. Then
        \begin{equation*}
            \left( \alpha-\beta \right) (T)v = \alpha(T)v - \beta(T)v\in W
        \end{equation*}
        and
        \begin{equation*}
            \left( \alpha\gamma \right) (T)v = \alpha\left( \gamma(T)v \right) \in W.
        \end{equation*}
        Since any $v\in V$ is of the form
        \begin{equation*}
            v = w_0 + \sum^{s}_{i=1} f_i(T)z_i
        \end{equation*}
        for some $w_0\in W_0$ and $f_1,f_2,\ldots,f_s\in\F[x]$,
        \begin{equation*}
            q_1(T)v = q_1(T)w_0 + \sum^{s}_{i=1} q_1(T)f_i(T)z_i = q_1(T)w_0,
        \end{equation*}
        which follows from the fact that $q_1\mid q_i$ so $q_1(T)z_i = 0$ for all $i\in \left\lbrace 1,2,\ldots,s \right\rbrace$. Thus $q_1\in S\left( V;W \right)$. Notice that $q_1$ is the polynomial of least degree such that
        \begin{equation*}
            q_1(T)z_1\in W_0,
        \end{equation*}
        since $Z\left( z_1;T \right)$ and $W_0$ are independent. Therefore, $q_1$ is the unique monic generator of $S\left( V;W \right)$. But by symmetry, $p_1$ is also the unique monic generator of $S\left( V;W \right) $, so $p_1=q_1$. Now we proceed inductively to show $r=s$ and $p_i=q_i$ for all $i\in\left\lbrace 2,3,\ldots,r \right\rbrace $. But before doing so, we first have to prove the following lemma. \suppressqedsym
    \end{proof}

    \begin{lemma_inside}{}
        Let $f\in\F[x]$ We define
        \begin{equation*}
            f(T)W = \left\lbrace f(T)w\in w\in W \right\rbrace 
        \end{equation*}
        for any subspace $W\subseteq V$. Then the following hold:
        \begin{enumerate}
            \item Let $v\in V$ be arbitrary. Then $f(T)Z\left( v;T \right) = Z\left( f(T)v;T \right)$.
            \item If $V = \bigoplus^{k}_{i=1} V_i$ for some $T$-invariant $V_1,V_2,\ldots,V_k\subseteq V$, then $f(T)V = \bigoplus^{k}_{i=1} f(T)V_i$. 
            \item If $v,z\in V$ have the same $T$-annihilator, then $f(T)v$ and $f(T)z$ have the same $T$-annihilator. That is,
                \begin{equation*}
                    \dim\left( Z\left( f(T)v;T \right)  \right) = \dim\left( Z\left( f(T)z;T \right)  \right) .
                \end{equation*}
        \end{enumerate}
    \end{lemma_inside}

    \begin{proof}
        For (a), notice that
        \begin{equation*}
            y\in f(T)Z(v;T)\iff \exists g\in\F[x]\left[ y=f(T)g(T)v \right] \iff \exists g\in\F[x]\left[ y=g(T)f(T)v \right] \iff y\in Z\left( f(T)v;T \right) .
        \end{equation*}
        For (b), let $v\in V$. Then there exist $f_1,f_2,\ldots,f_k\in\F[x]$ and $v_1\in V_1, v_2\in V_2, \ldots, v_k\in V_k$ such that
        \begin{equation*}
            v = \sum^{k}_{i=1} f_i(T)v_i.
        \end{equation*}
        So
        \begin{equation*}
            f(T)v = f(T)\sum^{k}_{i=1} f_i(T)v_i = \sum^{k}_{i=1} f_i(T)f(T)v_i.
        \end{equation*}
        Since the choice of $v\in V$ is arbitrary, it follows that
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} V_i.
        \end{equation*}
        For (c), let $p\in\F[x]$ be the common $T$-annihilator of $v$ and $z$, and let $q_v\in\F[x]$ be the $T$-annihilator of $f(T)v$. Then
        \begin{equation*}
            q_v(T)f(T)v = 0
        \end{equation*}
        so $q_v(T)v = 0$ or $f(T)v=0$. Notice that we may disregard the case which $f(T)v = 0$ easily, since $p\mid f$ by definition and thus $f(T)z = 0$ as well. Therefore $f(T)v$ and $f(T)z$ have the same $T$-annihilator, namely $1\in\F[x]$. So suppose $q_v(T)v = 0$. But this means $q_v(T)z = 0$ as well, since $p\mid q_v$. So by symmetry, if we let $q_z\in\F[x]$ be the $T$-annihilator of $f(T)z$, then $q_z(T)v = 0$ as well. It follows that $q_v = q_z$, and the result  
            \begin{equation*}
                \dim\left( Z\left( f(T)v;T \right)  \right) = \dim\left( Z\left( f(T)z;T \right)  \right) .
            \end{equation*}
            easily follows as well.
    \end{proof}

    \ruleline{Proof of Theorem 3.3 Is Continued Here}

    \begin{proof}[Proof of Theorem 3.3 Cont'd]
        Let $k\in \left\lbrace 2,3,\ldots,r \right\rbrace$ and suppose $W_{k-2}$ is $T$-invariant and $p_{k-1} = q_{k-1}$. Notice that we have proven the base case which $k=2$. Moreover, notice that
        \begin{equation*}
            \dim\left( W_{k-2} \right) + \dim\left( Z\left( v_{k-1};T \right)  \right) < \dim(V).
        \end{equation*}
        Since $p_{k-1} = q_{k-1}$, $\dim\left( Z\left(v_{k-1};T\right) \right) = \dim\left( Z\left( z_{k-1};T \right)  \right)$ by (c) of Lemma 3.3.3, and so
        \begin{equation*}
            \dim\left( W_{k-2} \right) + \dim\left( Z\left( z_{k-1};T \right)  \right) < \dim(V)
        \end{equation*}
        which shows $s\geq k$ as well. Notice that
        \begin{equation*}
            \begin{cases} 
                p_k(T)V = p_k(T)W_k \oplus Z\left( p_k(T)v_{k-1};T \right)  \\ 
                p_k(T)V = p_k(T)W_k \oplus \left(\bigoplus^{s}_{i=k-1} Z\left( p_k(T)v_{k-1};T \right)\right) 
            \end{cases}
        \end{equation*}
        by (a) and (b) of Lemma 3.3.3. But
        \begin{equation*}
            \dim\left( Z\left( p_k(T)v_{k-1};T \right)  \right) = \dim\left( Z\left( p_k(T)z_{k-1};T \right) \right) 
        \end{equation*}
        by (c) of Lemma 3.3.3, so it is apparent that
        \begin{equation*}
            \dim\left( Z\left( p_k(T)z_i \right)  \right) = 0
        \end{equation*}
        for all $i\in \left\lbrace k, k+1, \ldots, s \right\rbrace$. Thus $p_k(T)z_k = 0$ and so $q_k\mid p_k$. But by symmetry, $p_k\mid q_k$ as well. Thus $p_k = q_k$ for all $k\in \left\lbrace 1,2,\ldots,r \right\rbrace$ and $r=s$, as desired.
    \end{proof}

    \begin{cor}{}
        Every $T$-admissible $W\subseteq V$ has a $T$-invariant complementary $W'\subseteq V$.
    \end{cor}	

    \begin{proof}
        If $W=V$, then $W'=\left\lbrace 0 \right\rbrace$. Otherwise, by Theorem 3.3,
        \begin{equation*}
            V = W\oplus \left( \bigoplus^{r}_{i=1} Z\left( v_i;T \right)  \right) 
        \end{equation*}
        for some $v_1,v_2,\ldots,v_r\in V$. Then clearly
        \begin{equation*}
            W' = \bigoplus^{r}_{i=1} Z(v_i;T)
        \end{equation*}
        is a $T$-invariant subspace such that $W\oplus W' = V$.
    \end{proof}

    \begin{cor}{}
        There exists $v\in V$ such that the $T$-annihilator of $v$ is the minimal polynomial of $T$.
    \end{cor}	

    \begin{proof}
        By Theorem 3.3,
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z\left( v_i;T \right) 
        \end{equation*}
        for some $v_1,v_2,\ldots,v_r\in V$, where the respective $T$-annihilators $p_1,p_2,\ldots,p_k\in\F[x]$ satisfy $p_i\mid p_{i-1}$ for all $i\in \left\lbrace 2,3,\ldots,r \right\rbrace$. Then it is clear that $p=p_1$ is the minimal polynomial of $T$, since for any $v\in V$, we have $z_1\in Z(v_1;T), z_2\in Z(v_2;T), \ldots, z_r\in Z\left( v_r;T \right)$ such that
        \begin{equation*}
            v = \sum^{r}_{i=1} z_i
        \end{equation*}
        so
        \begin{equation*}
            p(T)v = p(T)\sum^{r}_{i=1} z_i = \sum^{r}_{i=1} p(T)z_i = 0
        \end{equation*}
        which follows from the fact that $p_i\mid p$ for any $i\in \left\lbrace 1,2,\ldots,r \right\rbrace$. Moreover, from the definition, $p=p_1$ is the monic polynomial of least degree which sends $v_1$ to $\left\lbrace 0 \right\rbrace$. This is because the decomposition $V=\bigoplus^{r}_{i=1} Z\left( v_i;T \right)$ is essentially
        \begin{equation*}
            V = \left\lbrace 0 \right\rbrace \oplus \left( \bigoplus^{r}_{i=1} Z\left( v_i;T \right)  \right) .
        \end{equation*}
        Thus $v=v_1$ is a vector such that $s\left( v;\left\lbrace 0 \right\rbrace  \right) $ is the minimal polynomial of $T$, as desired.
    \end{proof}

    \begin{cor}{}
        $T$ has a cyclic vector if and only if the minimal and the characteristic polynomials of $T$ are identical.
    \end{cor}	

    \begin{proof}
        The forward direction is supplied by Remark 3.7. To verify the reverse direction, suppose that the minimal and characteristic polynomials of $T$ coincide, and let $p$ be the minimal polynomial of $T$. Then by Corollary 3.3.5, there exists $v\in V$ such that the $T$-annihilator of $v$ is $p$. Since $p$ is also the characteristic polynomial, it follows that $\left\lbrace v,Tv, \ldots, T^{n-1}v \right\rbrace$ is a basis for $V$, where $n=\dim(V)$. But this exactly means $v$ is a cyclic vector of $T$.
    \end{proof}

    \begin{theorem}{Generalized Cayley-Hamilton Theorem}
        Let $p,f\in\F[x]$ be the minimal and characteristic polynomials of $T$, respectively. Then the following hold.
        \begin{enumerate}
            \item $p\mid f$.
            \item $p$ and $f$ have the same prime factors, except for the multiplicities.
            \item If
                \begin{equation*}
                    p = \prod^{k}_{i=1} f_i^{r_i}
                \end{equation*}
                for some monic $f_1,f_2,\ldots,f_k\in\F[x]$ and $r_1,r_2,\ldots,r_k\in\N$ is the prime factorization of $p$, then
                \begin{equation*}
                    f = \prod^{k}_{i=1} f_i^{d_i}
                \end{equation*}
                where
                \begin{equation*}
                    d_i = \frac{\nullity\left( f_i(T)^{ri} \right) }{\deg\left( f_i \right) }.
                \end{equation*}
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        By the cyclic decomposition theorem, write
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z(v_i;T)
        \end{equation*}
        for some $v_1,v_2,\ldots,v_r\in V$, and lent $p_1,p_2\ldots,p_r\in\F[x]$ be the respective $T$-annihilators. Then $p=p_1$ as we have seen in Corollary 3.3.5. Moreover, if we define $T_i:Z\left( v_i;T \right) \to Z\left( v_i;T \right)$ to be the restriction operator induced by $T$ for each $i\in \left\lbrace 1,2,\ldots,r \right\rbrace $, then
        \begin{equation*}
            T = \bigoplus^{r}_{i=1} T_i
        \end{equation*}
        and the minimal and characteristic polynomials of each $T_i$ are $p_i$, the $T$-annihilator (and thus $T_i$-annihilator) of $v_i$. It follows that
        \begin{equation*}
            f = \prod^{r}_{i=1} p_i
        \end{equation*}
        is the characteristic polynomial for $T$. But from the above expression, it is clear that $p=p_1$ divides $f$ and any prime factor which divides $f$ divides some $p_i$'s, which in turn divide $p=p_1$. This verifies (a) and (b). To verify (c), notice that
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} N\left( f_i(T)^{r_i} \right) 
        \end{equation*}
        and each $f_i^{r_i}$ is the minimal polynomial of the restriction operator $T_i:N\left( f_i(T)^{r_i} \right) \to N\left( f_i(T)^{r_i} \right)$ induced by $T$ by the primary decomposition theorem (Theorem 2.20). Then by (b), it must be the case
        \begin{equation*}
            f = \prod^{k}_{i=1} f_i^{d_i}
        \end{equation*}
        for some $d_1,d_2,\ldots,d_k\in\N$, where
        \begin{equation*}
            d_i = \frac{\nullity\left( f_i(T)^{r_i} \right) }{\deg\left( f_i \right) },
        \end{equation*}
        since the characteristic polynomial $\varphi_i\in\F[x]$ of each $T_i$ satisfies $\varphi_i=f_i^{d_i}$. That is,
        \begin{equation*}
            \nullity\left( f_i(T)^{r_i} \right) = \dim\left( N\left( f_i(T)^{r_i} \right)  \right) = \deg\left( \varphi_i \right) = d_i\deg\left( f_i \right) .
        \end{equation*}
        Thus by rearranging the above equality in terms of $d_i$, we have the desired result.
    \end{proof}

    \begin{cor}{}
        Let $N:V\to V$ be nilpotent. Then the characteristic polynomial of $N$ is $x^n$, where $n=\dim(V)$.
    \end{cor}	

    \begin{proof}
        It is clear that $N^k=0$ for some $k\in\left\lbrace 1,2,\ldots,n \right\rbrace $. Then by (b) of the generalized Cayley-Hamilton theorem, $x^n$ is the characteristic polynomial of $N$.
    \end{proof}
    
    \begin{remark}
        Let us take a look at the matrix analogue of the cyclic decomposition theorem. Consider a cyclic decomposition
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z\left( v_i;T \right) 
        \end{equation*}
        for some $v_1,v_2,\ldots,v_r\in V$, and let $p_1,p_2,\ldots,p_r\in\F[x]$ be the respective $T$-annihilators. Now, for each $i\in \left\lbrace 1,2,\ldots,r \right\rbrace $, let
        \begin{equation*}
            \beta_i = \left\lbrace v_i, Tv_i, \ldots, T^{k_i-1}v_i \right\rbrace 
        \end{equation*}
        be a cyclic basis for $Z\left( v_i;T \right)$, where $k_i=\dim\left( Z\left( v_i;T \right)  \right) = \deg\left( p_i \right)$. Moreover, if we define $T_i:Z\left( v_i;T \right) \to Z\left( v_i;T \right)$ to be the operator induced by $T$, then $\left[ T_i \right] _{\beta_i}$ is the companion matrix of $p_i$. Thus, if we define
        \begin{equation*}
            \beta = \left\lbrace \beta_1,\beta_2, \ldots,\beta_r \right\rbrace ,
        \end{equation*}
        then
        \begin{equation*}
            [T]_\beta = 
            \begin{bmatrix}
                & & & \\
                \left[ T_1 \right]_{\beta_1} & & & \\
                & \left[ T_2 \right]_{\beta_2} & & \\
                & & \ddots & \\
                & & & \left[ T_r \right]_{\beta_r}\\
                & & & \\
            \end{bmatrix}
            = \bigoplus^{r}_{i=1} \left[ T_i \right] _{\beta_i}.
        \end{equation*}
        This motivates the following definition.
    \end{remark}

    \begin{definition}{Rational Form}{}
        Let $A\in M_{n\times n}(\F)$. We say $A$ is in \emph{rational form} if
        \begin{equation*}
            A = \bigoplus^{r}_{i=1} A_i,
        \end{equation*}
        where each $A_i$ is the companion matrix of a nonscalar monic $p_i\in\F[x]$ satisfying $p_i\mid p_{i-1}$ for all $i\in\left\lbrace 2,3,\ldots,r \right\rbrace $.
    \end{definition}

    \begin{theorem}{Every $B\in M_{n\times n}(\F)$ Is Similar over $\F$ to a Unique $A\in M_{n\times n}(\F)$ in Rational Form}
        Let $B\in M_{n\times n}(\F)$. Then there exists a unique $A\in M_{n\times n}(\F)$ in rational form such that $B$ and $A$ are similar.
    \end{theorem}

    \begin{proof}
        Let $\beta$ be the standard ordered basis for $\F^n$ and let $T:\F^n\to\F^n$ be the unique linear operator such that $[T]_\beta = B$. By using the construction in Remark 3.17, there exist $v_1,v_2,\ldots,v_r\in\F^n$ and the respective $T$-annihilators $p_1,p_2,\ldots,p_r\in\F[x]$ satisfying $p_i\mid p_{i-1}$ for all $i\in \left\lbrace 2,3,\ldots,r \right\rbrace$ such that
        \begin{equation*}
            \F^n = \bigoplus^{r}_{i=1} Z\left( v_i;T \right) ,
        \end{equation*}
        where $\alpha = \left\lbrace T^kv_i:0\leq k< \deg\left( p_i \right) = \dim\left( Z\left( v_i;T \right)  \right)  \right\rbrace$ is a basis for $\F^n$ such that $[T]_\alpha$ is in rational form. Now suppose there exists another ordered basis $\gamma$ for $V$ such that $[T]_\gamma$ is in rational form. Notice that
        \begin{equation*}
            [T]_\gamma = \bigoplus^{s}_{i=1} C_i
        \end{equation*}
        for some $s\in\N$ and $C_1,C_2, \ldots, C_s$, which are the respective companion matrices of some monic nonscalar $q_1,q_2,\ldots,q_s\in\F[x]$. Then by Corollary 3.2.1, each $q_i$ is the characteristic and the minimal polynomial of $C_i$, so by Corollary 3.3.5, there exists $z_i\in V$ such that the $T$-annihilator of $z_i$ is $g_i$. So we have nother cyclic decomposition
        \begin{equation*}
            \F^n = \bigoplus^{s}_{i=1} Z\left( z_i;T \right) ,
        \end{equation*}
        which means $r=s$ and $p_i=q_i$ for all $i\in \left\lbrace 1,2,\ldots,r \right\rbrace$ by the uniqueness part of the cyclic decomposition theorem. Thus $[T]_\alpha = [T]_\gamma$, as desired.
    \end{proof}

    \begin{definition}{Invariant Factor}{of a Matrix}
        Let $B\in M_{n\times n}(\F)$ and let $T:\F^n\to\F^n$ satisfy $[T]_\beta = B$, where $\beta$ is the standard ordered basis for $\F^n$. Then for any cyclic decomposition
        \begin{equation*}
            \F^n = \bigoplus^{r}_{i=1} Z\left( v_i;T \right) 
        \end{equation*}
        for some $v_1,v_2,\ldots,v_r\in\F^n$, the respective $T$-annihilators $p_1,p_2,\ldots,p_r\in\F[x]$ is the same regardless of the choice of $v_1,v_2,\ldots,v_r$. We call these polynomials $p_1,p_2,\ldots,p_r$ the \emph{invariant factors} of $B$.
    \end{definition}

    \begin{remark}
        We shall discuss an algorithm for calculating invariant factors later. The fact that invariant factors can be computed by means of finite number of rational operations on the entries of a matrix is what gives rational form its name.
    \end{remark}

    \begin{example}
        Suppose $\dim(V)=2$. Observe that the possibilities for the cyclic decomposition of $V$ is very limited. Let $p\in\F[x]$ be the minimal polynomial of $T$. If $\deg(p)=2=\dim(V)$, then $p$ is also the characteristic polynomial of $T$, and thus $T$ has a cyclic vector. That is, there exists some ordered basis $\beta$ for $V$ such that $[T]_\beta$ is the companion matrix of $p$. On the other hand, if $\deg(p)=1$, then $T=cI$ for some $c\in\F$, and thus for any linearly independent $v_1,v_2\in V$, we have
        \begin{equation*}
            V = Z(v_1;T)\oplus Z(v_2;T)
        \end{equation*}
        and the respective $T$-annihilators $p_1,p_2\in\F[x]$ are such that
        \begin{equation*}
            p_1=p_2=p=x-c.
        \end{equation*}
        That is, in terms of matrics in $M_{2\times 2}(\F)$, every $A\in M_{2\times 2}(\F)$ is similar to exactly one matrix of the types
        \begin{equation*}
            \begin{bmatrix}
                c & 0 \\ 0 & c
            \end{bmatrix}
        \end{equation*}
        and
        \begin{equation*}
            \begin{bmatrix}
                0 & -c_0 \\ 1 & -c_1
            \end{bmatrix}
        \end{equation*}
        over $\F$, for some $c, c_0, c_1\in\F$.
    \end{example}

    \begin{example}
        Suppose $T:\R^3\to\R^3$ satisfies
        \begin{equation*}
            [T]_\beta =
            \begin{bmatrix}
                5 & -6 & -6 \\
                -1 & 4 & 2 \\
                3 & -6 & -4  \\
            \end{bmatrix}
        \end{equation*}
        where $\beta$ is the standard ordered basis. By some calculations, we can show that
        \begin{equation*}
            f = (x-1)(x-2)^2
        \end{equation*}
        is the characteristic polynomial of $T$ and
        \begin{equation*}
            p = (x-1)(x-2)
        \end{equation*}
        is the minimal polynomial of $T$. So if we consider a cyclic decomposition
        \begin{equation*}
            \R^3 = \bigoplus^{r}_{i=1} Z(v_i;T)
        \end{equation*}
        it is clear that the $T$-annihilator of $v_1$ is $p_1=p$. Moreover, since
        \begin{equation*}
            \dim\left( Z\left( v_1;T \right)  \right) =\deg\left( p_1 \right) =\deg(p) = 2,
        \end{equation*}
        it follows that
        \begin{equation*}
            \R^3 = Z\left( v1;T \right) \oplus Z\left( v_2;T \right) 
        \end{equation*}
        where $\dim\left( Z\left( v_2;T \right)  \right)=1$. But this exactly means that $v_2$ is an eigenvector of $T$, which the corresponding eigenvalue is 2, since we must have $f=p_1p_2$ where $p_2\in\F[x]$ is the $T$-annihilator of $v_2$. It follows immediately that
        \begin{equation*}
            [T]_\alpha = 
            \begin{bmatrix}
                0 & -2 & 0 \\
                1 & 3 & 0 \\
                0 & 0 & 2 \\
            \end{bmatrix}
        \end{equation*}
        where $\alpha = \left\lbrace v_1, Tv_1, v_2 \right\rbrace$. But how do we actually compute $v_1$ and $v_2$? TO answer this question, notice that any $v_1\in V$ such that $\dim\left( Z\left( v_1;T \right)  \right) =2$ and any $v_2\in V\setminus Z\left( v_1;T \right)$ such that $Tv_2=2v_2$ are suitable. For instance, if we take $v_1 = \left( 1,0,0 \right)$, then 
        \begin{equation*}
            Tv_1 = 
            \begin{bmatrix}
                5\\-1\\3
            \end{bmatrix}
        \end{equation*}
        which clearly is linearly independent of $v_1$. To find $v_2$, notice that
        \begin{equation*}
            T
            \begin{bmatrix}
                u_1 \\ u_2 \\ u_3
            \end{bmatrix}
            = 2
            \begin{bmatrix}
                u_1 \\ u_2 \\ u_3
            \end{bmatrix}
        \end{equation*}
        if and only if $u_1 = 2u_2+2u_3$ by some calculations, and an example of such $v_2$ is $v_2=\left( 2,1,0 \right)$. By direct calculations, it can be verified that
        \begin{equation*}
            [T]_\alpha =
            \begin{bmatrix}
                0 & -2 & 0 \\
                1 & 3 & 0 \\
                0 & 0 & 2 \\
            \end{bmatrix}
        \end{equation*}
        when $\alpha = \left\lbrace \left( 1,0,0 \right) , \left( 5,-1,3 \right) , (2,1,0) \right\rbrace$.
    \end{example}

    \section{Jordan Form}
    
    \begin{remark}
        Consider analyzing a cyclic decomposition of a nilpotent linear operator. Let $N:V\to V$ be nilpotent. By the cyclic decomposition theorem, there exist $v_1, v_2, \ldots,v_r\in V$ and the corresponding $N$-annihilators $p_1,p_2,\ldots,p_r\in\F[x]$ such that
        \begin{equation*}
            V = \bigoplus^{r}_{i=1} Z\left( v_i;N \right) 
        \end{equation*}
        and $p_i\mid p_{i-1}$ for all $i\in \left\lbrace 2,3,\ldots,r \right\rbrace$. Since $N$ is nilpotent, the minimal polynomial for $N$ is $x^k$ for some $k\in \left\lbrace 1,2,\ldots,n \right\rbrace$ by Corollary 3.4.1. It follows that $p_i = x^{k_i}$ for some $k_i\in\N$ such that
        \begin{equation*}
            k_1\geq k_2\geq\cdots\geq k_r
        \end{equation*}
        and $k_1=k$. The companion matrix $C_i\in M_{k_i\times k_i}(\F)$ of $p_i=x^{k_i}$ is
        \begin{equation*}
            C_i = 
            \begin{bmatrix}
                0 & 0 & \cdots & 0 & 0 \\
                1 & 0 & \cdots & 0 & 0 \\
                0 & 1 & \cdots & 0 & 0 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                0 & 0 & \cdots & 1 & 0 \\
            \end{bmatrix},
        \end{equation*}
        the matrix with 1's at the entries directly below the main diagonal and 0's at every other entries. Therefore, the cyclic decomposition theorem provides a basis $\beta$ for $V$ such that
        \begin{equation*}
            [N]_\beta = \bigoplus^{r}_{i=1} C_i,
        \end{equation*}
        a direct sum of elementary nilpotent matrices, the sizes of which nonincreases as $i$ increases. Moreover, $r\in\N$ and $p_1,p_2,\ldots,p_r\in\F[x]$ are unique, so with a nilpotent operator $N$ on a $n$-dimensional vector space, we see that the number $r\in\N$ of the cyclic subspaces involved in a decomposition and some $k_1,k_2,\ldots,k_r\in\N$ such that $\sum^{r}_{i=1} k_i=n$ and $k_1\geq k_2\geq \cdots\geq k_r$ uniquely determine the rational form of the linear operator up to similarity. Moreover, here is one more thing that we would like to point out. That is,
        \begin{equation*}
            r = \nullity(N).
        \end{equation*}
        In fact, the $r$ vectors $N^{k_1-1}v_1, N^{k_2-1}v_2,\ldots,N^{k_r-1}v_r$ form a basis for $\ker(N)$. To see this, let
        \begin{equation*}
            v = \sum^{r}_{i=1} f_i(N)v_i\in \ker(N)
        \end{equation*}
        for some $f_1,f_2,\ldots,f_r\in\F[x]$ and denote $\alpha = \left\lbrace N^{k_1-1}v_1, N^{k_2-1}v_2, \ldots, N^{k_r-1}v_r \right\rbrace$ for convenience. Since $\dim\left( Z\left( v_i;T \right)  \right) $, we may assume $\deg\left( f_i \right) < k_i$ without loss of generality. Since $Nv = 0$,
        \begin{equation*}
            Nf_i(N)v_i = \left( xf_i \right) (N)v_i = 0
        \end{equation*}
        for each $i\in\left\lbrace 1,2,\ldots,r \right\rbrace$, which means $p_i\mid f_i$ for all $i\in\left\lbrace 1,2,\ldots,r \right\rbrace$. But each $p_i = x^{k_i}$ and $\deg\left( f_i \right) < k_i$, so each
        \begin{equation*}
            f_i = c_ix^{k_i-1}
        \end{equation*}
        for some $c_i\in\F$. Then,
        \begin{equation*}
            v = \sum^{r}_{i=1} c_iN^{k_i-1}v_i
        \end{equation*}
        so clearly $\ker(N)\subseteq\spn\left( \alpha \right) $. On the other hand,
        \begin{equation*}
            N\sum^{r}_{i=1} c_iN^{k_i-1}v_i = \sum^{r}_{i=1} c_iN^{k_i}v_i = 0,
        \end{equation*}
        so $\spn\left( \alpha \right)\subseteq\ker(N)$. Since $\alpha$ is linearly independent, it follows that $\alpha$ is a basis for $\ker(N)$.
    \end{remark}

    \begin{remark}
        Now, we are going to combine the results of Remark 3.22 and the primary decomposition theorem (Theorem 2.20). Suppose that the characteristic polynomial $f\in\F[x]$ factors over $\F$ as
        \begin{equation*}
            f = \prod^{k}_{i=1} \left( x-c_i \right) ^{d_i}
        \end{equation*}
        for some distinct $c_1,c_2,\ldots,c_k\in\F$ and $d_1,d_2,\ldots,d_k\in\N$. Then the minimal polynomial $p\in\F[x]$ of $T$ would be
        \begin{equation*}
            p = \prod^{k}_{i=1} \left( x-c_i \right) ^{r_i}
        \end{equation*}
        where $1\leq r_i\leq d_i$ for each $i\in\left\lbrace 1,2,\ldots,k \right\rbrace$. Let $W_i = \ker\left( T_i-c_iI \right)^{r_i}$. Then by the primary decomposition theorem,
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} W_i
        \end{equation*}
        and the linear operator $T_i:W_i\to W_i$ induced by $T$ has minimal polynomial $\left( x-c_i \right) ^{r_i}$ for each $i\in\left\lbrace 1,2,\ldots,k \right\rbrace $. Now, define
        \begin{equation*}
            N_i = T_i-c_iI: W_i\to W_i
        \end{equation*}
        for each $i\in\left\lbrace 1,2,\ldots,k \right\rbrace$. Then $N_i$ is nilpotent and has minimal polynomial $x^{r_i}$. On $W_i$, $T$ acts like $N_i+c_iI$. That is, if we choose a basis $\beta$ for $W_i$ corresponding to the cyclic decomposition for $N_i$, then $\left[ T_i \right] _\beta$ would be the direct sum of matrices of the form
        \begin{equation*}
            \begin{bmatrix}
                c & 0 & 0 & \cdots & 0 & 0 \\
                1 & c & 0 & \cdots & 0 & 0 \\
                0 & 1 & c & \cdots & 0 & 0 \\
                \vdots & \vdots & \ddots & \ddots & \vdots & \vdots \\
                0 & 0 & 0 & \ddots & c & 0 \\
                0 & 0 & 0 & \cdots & 1 & c \\
            \end{bmatrix}
        \end{equation*}
        each with $c=c_i$. Furthermore, the sizes of these matrices would nonincrease as one reads from left to right. We use the following terminology to specify such matrices. 
    \end{remark}

    \begin{definition}{Elementary Jordan Matrix}{with Eigenvalue}
        We call a matrix of the form
        \begin{equation*}
            \begin{bmatrix}
                c & 0 & 0 & \cdots & 0 & 0 \\
                1 & c & 0 & \cdots & 0 & 0 \\
                0 & 1 & c & \cdots & 0 & 0 \\
                \vdots & \vdots & \ddots & \ddots & \vdots & \vdots \\
                0 & 0 & 0 & \ddots & c & 0 \\
                0 & 0 & 0 & \cdots & 1 & c \\
            \end{bmatrix}
        \end{equation*}
        an \emph{elementary Jordan matrix} with eigenvalue $c$.
    \end{definition}

    \note{Remark 3.22 is continued here} \noindent Now, if we define
    \begin{equation*}
        \beta = \left\lbrace \beta_1, \beta_2, \ldots, \beta_k \right\rbrace ,
    \end{equation*}
    where each $\beta_i$ is an ordered basis for $W_i$ such that $\left\lbrace T_i \right\rbrace _{\beta_i}$ is a direct sum of elementary Jordan matrices, then $\beta$ is a basis for $V$. Then the matrix representation $[T]_\beta$ of $T$ would be the direct sum
    \begin{equation*}
        [T]_\beta =
        \begin{bmatrix}
            A_1 & & & \\
            & A_2 & & \\
            & & \ddots & \\
            & & & A_k\\
        \end{bmatrix}
    \end{equation*}
    where each $A\in M_{d_i\times d_i}(\F)$ is of the form
    \begin{equation*}
        \bigoplus^{n_i}_{j=1} J_{ij}
    \end{equation*}
    where each $J_{ij}$ is an elementary Jordan matrix with eigenvalue $c_i$. Moreover, the sizes of elementary Jordan matrices $J_{i 1}, J_{i 2}, \ldots. J_{ij}$ are nonincreasing as one reads from left to right. This motivates the following definition.

    \begin{definition}{Jordan Form}{}
        Let $c_1,c_2,\ldots,c_k\in\F$ be distinct and suppose $A\in M_{n\times n}(\F)$ is a direct sum of $A_1,A_2,\ldots,A_k$, each $A_i$ of which is a direct sum of elementary Jordan matrices with eigenvalue $c_i$, the sizes of which are nonincreasing as one reads from left to right. Such matrix $A$ is said to be in \emph{Jordan form}. 
    \end{definition}

    \begin{remark}
        So in summary, we have shown that any linear operator $T$ on a finite-dimensional vector space $V$ for which the characteristic polynomial $f\in\F[x]$ factors over $\F$ has an ordered basis $\beta$ for $V$ such that $[T]_\beta$ is in Jordan form. What we now want to point out is that, this Jordan form $[T]_\beta$ of $T$ is something uniquely associated with $T$, up to the order which the eigenvalues of $T$ are written down. In other words, if $A,B\in M_{n\times n}(\F)$ are similar and in Jordan form, then they only differ in the order which the eigenvalues $c_1,c_2,\ldots,c_k\in\F$ are written down.
    \end{remark}

    \begin{prop}{Uniqueness of Jordan form up to Reordering Eigenvalues}
        Let $T:V\to V$ be a linear operator such that the characteristic polynomial of $T$ factors over $\F$. Then there exists an ordered basis $\alpha$ for $V$ such that $[T]_\alpha$ is in Jordan form. Moreover, if $\beta$ is a basis for $V$ such that $[T]_\beta$ is in Jordan form, then $[T]_\alpha$ and $[T]_\beta$ are same up to reordering the eigenvalues $c_1,c_2,\ldots,c_k\in\F$ of $T$.
    \end{prop}

    \begin{proof}
        The existence part is supplied by Remark 3.22. To verify the uniqueness up to reordering, first write
        \begin{equation*}
            A = \bigoplus^{k}_{i=1} A_i\in M_{d_i\times d_i}(\F) 
        \end{equation*}
        where each
        \begin{equation*}
            A_i = \bigoplus^{r_i}_{j=1} J_{ij}
        \end{equation*}
        for some elementary Jordan matrices $J_{i 1}, J_{i 2}, \ldots, J_{ir_i}$, the sizes of which are nonincreasing as we read from left to right. Then $A$ is lower triangular and
        \begin{equation*}
            f = \prod^{r_i}_{j=1} \left( x-c_i \right) ^{d_i}
        \end{equation*}
        is the characteristic polynomials for $A$, since each $c_i$ is repeated $d_i$ times on the main diagonal. This verifies that $c_1,c_2,\ldots,c_k$ and $d_1,d_2,\ldots,d_k$ are unique up to reordering. The fact that $A$ is the direct sum of $A_1,A_2,\ldots,A_k$ provides a direct sum decomposition
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} W_i
        \end{equation*}
        by the primary decomposition theorem (Theorem 2.20), where $W_i = \ker\left( A_i-c_iI \right) ^{r_i}$. Now the claim is
        \begin{equation*}
            W_i = \ker\left( T-c_iI \right) ^n.
        \end{equation*}
        To verify this, first observe that
        \begin{equation*}
            \ker\left( T-c_iI \right) ^n \subseteq W_i.
        \end{equation*}
        This is because there exists uniue $v_1\in W_1, v_2\in W_2,\ldots, v_k\in W_k$ such that
        \begin{equation*}
            v = \sum^{k}_{i=1} v_i
        \end{equation*}
        for any $v\in V$ by the direct sum decomposition of $V$. Therefore, if $v\in\ker\left( T-c_iI \right) ^n$, then
        \begin{equation*}
            \left( T-c_iI \right) ^nv = \sum^{k}_{j=1} \left( T-c_i \right)^nv_j = \sum^{k}_{j=1} \left( T_jc_i \right)^n v_j = 0, 
        \end{equation*}
        which means
        \begin{equation*}
            \left( T_j-c_iI \right)^nv = 0 
        \end{equation*}
        for all $j\in\left\lbrace 1,2,\ldots,n \right\rbrace$. But $T_j-c_iI$ is invertible whenever $j\neq i$, so it follows that $v_j=0$ whenever $j\neq i$ and thus $v=v_i\in W_i$. Moreover, observe that 
        \begin{equation*}
            \ker\left( T_i-c_iI \right) ^{r_i} \subseteq \ker\left( T_i-c_iI \right)^n 
        \end{equation*}
        where $T_i:W_i\to W_i$ is the operator induced by $T$, since $r_i\leq n$ and
        \begin{equation*}
            v\in\ker\left( T_i-c_iI \right) ^{r_i} \implies \left( T_i-c_iI \right) ^{r_i}v = 0 \implies \left( T_i-c_iI \right) ^nv = \left( T_i-c_iI \right) ^{n-r_i}\left( T_i-c_iI \right) ^{r_i}v = 0.
        \end{equation*}
        Thus we have
        \begin{equation*}
            W_i = \ker\left( A_i-c_iI \right) ^{r_i} = \ker\left( T_i-c_i \right) ^{r_i} \subseteq \ker\left( T_i-c_iI \right) ^n \subseteq \ker\left( T-c_iI \right) ^n \subseteq W_i 
        \end{equation*}
        verifying our claim. This means $W_1,W_2,\ldots,W_k$ are also unique up to reordering. Since each $W_i$ is $T$-invariant, the Jordan form of $T_i$ is unique. Thus the Jordan form of $T$, which is a direct sum of Jordan form of $T_1,T_2,\ldots,T_k$ is unique up to reordering.
    \end{proof}

\end{document}
