\documentclass[linearalgebraII]{subfiles}

\begin{document}

    \chap{Elementary Canonical Forms}

    \begin{remark}
        Throughout this chapter, let $V$ denote a finite-dimensional vector space over a field $\F$, unless otherwise specified.
    \end{remark}

    \begin{remark}
        The goal of this chapter is to find an ordered basis $\beta$ for $V$ provided a linear operator $T:V\to V$ such that $\left[ T \right] _\beta$ is in a simple form (i.e. diagonal, triangular, ...).
    \end{remark}

    \section{Eigenvalues}

    \begin{prop}{$f(T)v = f(c)v$ for any Eigenpair $(c,v)$}
        Let $T$ be a linear operator on $V$, $v\in V$ be an eigenvector of $T$, and $c\in \F$ be the eigenvalue corresponding to $v$. Then $f(T)v = f(c)v$ for any $f\in \F[x]$.
    \end{prop}

    \begin{proof}
        Write $f = \sum^{n}_{i=0} a_ix^i \in \F[x]$. Then
        \begin{equation*}
            f(T)v = \left( \sum^{n}_{i=0} a_iT^i \right) v = \sum^{n}_{i=0} a_iT^i(v) = \sum^{n}_{i=0} a_i(cv)^i = \left( \sum^{n}_{i=0} a_ic^i \right) v = f(c)v. \eqedsym
        \end{equation*}
    \end{proof}

    \begin{prop}{}
        Let $T:V\to V$ be a linear operator. Let $c_1, c_2, \ldots, c_k\in \F$ be distinct eigenvalues of $T$ and let $W_1, W_2, \ldots, W_k$ be the corresponding eigenspaces, respectively. Then
        \begin{equation*}
            \dim \left( W_1 + W_2 + \cdots + W_k \right) = \sum^{k}_{i=1} \dim(W_i). 
        \end{equation*}
        In particular, if $\beta_1, \beta_2, \ldots, \beta_k$ are bases for $W_1, W_2, \ldots, W_k$, respectively, then
        \begin{equation*}
            \beta = \bigcup^{k}_{i=1} \beta_i
        \end{equation*}
        is a basis for $W = W_1 + W_2 + \cdots + W_k$.
    \end{prop}

    \begin{proof}
        Notice that $W_i\cup W_j = \left\lbrace 0 \right\rbrace$ for any distinct $i,j\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, since each eigenspace is characterized by the corresponding eigenvalue. It follows that any bases $\beta_i$ for $W_i$ and $\beta_j$ for $W_j$ are linearly independent. Thus we conclude $\beta=\bigcup^{k}_{i=1} \beta_i$ is a basis for $W$. It follows that $\dim \left( W \right) = \sum^{k}_{i=1} \dim(W_i)$. 
    \end{proof}

    \begin{prop}{Alternative Definitions of Diagonalizability}
        Let $T:V\to V$ be a linear operator. The following are equivalent.
        \begin{enumerate}
            \item $T$ is diagonalizable.
            \item There exist $c_1, c_2, \ldots, c_k\in \F$ such that
                \begin{equation*}
                    f = \prod^{k}_{i=1} (x-c_i)^{m_i}
                \end{equation*}
                is the characteristic polynomial of $T$, where $m_i$ is the dimension of the eigenspace $W_i$ corresponding to $c_i$.
            \item $\sum^{k}_{i=1} \dim(W_i) = \dim(V)$.
        \end{enumerate}
    \end{prop}

    \begin{proof}
        (a)$\implies$(b) is a direct result of the fact that the diagonal entries of a diagonal matrix is the eigenvalues of the matrix. (b)$\implies$(c) is clear, since
        \begin{equation*}
            \dim(V) = \deg(f) = \sum^{k}_{i=1} m_i = \sum^{k}_{i=1} \dim(W_i).  
        \end{equation*}
        Notice that (c)$\implies$(a) is a direct result of Proposition 2.2. 
    \end{proof}

    \section{Annihilating Polynomials}

    \begin{definition}{Annihilating Polynomial}{of a Linear Operator}
        Let $V$ be a vector space and $T:V\to V$ be a linear operator. We say $f\in\F[x]$ is an \emph{annihilating polynomial} of $T$ if $f(T)=0$.
    \end{definition}

    \begin{prop}{Set of Annihilating Polynomial Is an Ideal}
        Let $T:V\to V$. Then the set of annihilating polynomials of $T$,
        \begin{equation*}
            M_T = \left\lbrace f\in\F[x]:f(T)=0 \right\rbrace ,
        \end{equation*}
        is an ideal of $\F[x]$.
    \end{prop}

    \begin{proof}
        Let $f,g\in M_T$ and $h\in \F[x]$. Clearly $(f-g)(T) = f(T)-g(T) = 0 - 0 = 0$. Moreover, $(fh)(T) = f(T)h(T) = 0h(T) = 0$ so $fh\in M$. Thus by the ideal test, $M_T\subseteq\F[x]$ is an ideal of $\F[x]$.
    \end{proof}

    \begin{remark}
        If $V$ is an arbitrary vector space and $T:V\to V$ is linear, then $T$ need not have an annihilating polynomial except for $0\in\F[x]$. But when $V$ is finite-dimensional, $T$ always has a nonzero annihilating polynomial. This can be shown as follows. Let $n=\dim(V)\in\N$ then $\dim\left( \lin(V,V) \right) = n^2$, so $T^0, T^1, \ldots, T^{n^2}$ are linearly dependent. This means there exists nonzero $\left( c_0,c_1,\ldots,c_{n^2} \right) \in\F^{n^2+1}$ such that
        \begin{equation*}
            \sum^{n^2}_{i=0} c_iT^i = 0.
        \end{equation*}
        But this exactly means
        \begin{equation*}
            f = \sum^{n^2}_{i=0} c_ix^i \in\F[x]
        \end{equation*}
        is an annihilating polynoimal of $T$. Then Proposition 1.10 and Corollary 1.10.1 guarantee that there exists a unique $p\in\F[x]$ which generates the ideal of annihilating polynomials of $T$, $M_T$. This motivates the following definition.
    \end{remark}

    \begin{definition}{Minimal Polynomial}{of a Linear Operator}
        Let $T:V\to V$ be linear. We say $p\in\F[x]$ is the \emph{minimal polynomial} of $T$ if $p$ is the unique monic generator of $M_T$.
    \end{definition}

    \begin{remark}
        Observe that the minimal polynomial $p\in\F[x]$ of a linear operator $T:V\to V$ is uniquely determined by the following.
        \begin{enumerate}
            \item $p$ is monic.
            \item $p(T)=0$.
            \item No polynomial over $\F$ which annihilates $T$ has smaller degree than $p$ has.
        \end{enumerate}
        The name \textit{minimal} stems from (c).
    \end{remark}

    \begin{definition}{Minimal Polynomial}{of a Matrix}
        Let $A\in M_{n\times n}(\F)$. We say $p\in\F[x]$ is the \emph{minimal polynomial} of $A$ if $p$ is the unique monic generator of the ideal of annihilating polynomials of $A$ over $\F$.
    \end{definition}

    \begin{remark}
        Let $T:V\to V$ be a linear operator and let $\beta$ be an ordered basis for $V$. Then it is clear that the minimal polynomial of $T$ and $\left[ T \right] _\beta$ are identical, since
        \begin{equation*}
            \left[ f(T) \right] _\beta = f\left[ T \right] _\beta
        \end{equation*}
        for any $f\in\F[x]$, so $f(T)=0$ if and only if $f\left[ T \right] _\beta=0$. Furthermore, this result also shows that, if $A, B\in M_{n\times n}(\F)$ are similar, then $A$ and $B$ have the same minimal polynomial. 
    \end{remark}

    \begin{remark}
        Let $T:V\to V$ be diagonalizable and let $c_1,c_2,\ldots,c_k\in\F$ be the distinct eigenvalues of $T$. Then it is easy to see that
        \begin{equation*}
            p = \prod^{k}_{i=1} \left( x-c_i \right) 
        \end{equation*}
        is the minimal polynomial of $T$. To verify this, let $v\in V$ be arbitrary. Then by Proposition 2.2, there exist eigenvectors $v_1,v_2,\ldots,v_k\in V$ corresponding to $c_1,c_2,\ldots,c_k$, respectively, such that
        \begin{equation*}
            v = \sum^{k}_{i=1} v_i.
        \end{equation*}
        Then, for any $f\in\F[x]$,
        \begin{equation*}
            f(T)v = f(T)\sum^{k}_{i=1} v_i = \sum^{k}_{i=1} f(T)v_i = \sum^{k}_{i=1} f\left( c_i \right) v_i,
        \end{equation*}
        so if $f(T)v = 0$, then $f\left( c_i \right) =0$ for all $i\in\left\lbrace 1,2,\ldots,k \right\rbrace$. This means $p\mid f$. But it is clear that $p$ is the polynomial of minimum degree which satisfies the above equation.
    \end{remark}

    \begin{prop}{Minimal Polynomial of any Diagonalizable Operator Is a Product of Linear Factors}
        Let $V$ be a vector space over $\F$ and let $T:V\to V$ be a diagonalizable linear operator with eigenvalues $c_1, c_2, \ldots, c_n\in \F$. Then
        \begin{equation*}
            p = \prod^{k}_{i=1} \left( x-c_i \right) 
        \end{equation*}
        is the minimal polynomial of $T$.
    \end{prop}

    \begin{remark}
        We will discuss that the converse of Proposition 2.5 is also true later. That is, $T$ is diagonalizabe if and only if its minimal polynomial is a product of linear factors of degree 1.
    \end{remark}

    \begin{prop}{The Characteristic and Minimal Polynomials Have the Same Roots}
        Let $T:V\to V$ be linear. Then the characteristic polynomial and minimal polynomial have the same roots.
    \end{prop}

    \begin{proof}
        Let $p\in\F[x]$ be the minimal polynomial of $T$. Notice that it is equivalent to prove that $p(c) = 0$ if and only if $c\in \F$ is an eigenvalue of $T$. For the forward direction, suppose $p(c) = 0$. It follows that $p = (x-c)q$ for some $q\in\F[x]$, where $q(T)\neq 0$ by the minimality of $p$. Let $v\in V$ be such that $q(T)v\neq 0$. Then
        \begin{equation*}
            p(T)v = (T-cI)q(T)v = 0
        \end{equation*}
        so $T-cI$ is not invertible and $c$ is an eigenvalue of $T$. The reverse direction is a direct consequence of Proposition 2.1.
    \end{proof}

    \begin{theorem}{Cayley-Hamilton Theorem}
        Let $T:V\to V$ be a linear operator. If $p$ is the characteristic polynomial of $T$, then $p(T) = 0$. Equivalently, the minimal polynomial divides the characteristic polynomial. 
    \end{theorem}

    \begin{proof}
        Let $K$ be a commutative, unital algebra over $\F$. Let $\left( v_1, v_2, \ldots, v_n \right)$ be an ordered basis for $V$ and $A = \left[ T \right]_\beta$. Then
        \begin{equation*}
            T(v_i) = \sum^{n}_{j=1} A_{ji}v_j
        \end{equation*} 
        or, equivalently,
        \begin{equation*}
            \sum^{n}_{j=1} \left( T - A_{ji}I \right) (v_j) = 0.
        \end{equation*}
        Define $B\in M_{n\times n}(K)$ by $B_{ij} = T - A_{ji}I$. Then $\det(B) = f(T)$. Now the claim is that $\det(B)v_i = 0$ for each $i\in \left\lbrace 1, 2, \ldots, n \right\rbrace$. To verify this, let $\tilde{B} = \adj(B)$. By definition, $\sum^{n}_{j=1} B_{ij}v_j = 0$, so
        \begin{equation*}
            \sum^{n}_{j=1} \tilde{B}_{ki} B_{ij} v_j = 0
        \end{equation*}
        for each $k, i\in \left\lbrace 1, 2, \ldots, n \right\rbrace$. By summing over $i$,
        \begin{equation*}
            \sum^{n}_{i=1} \sum^{n}_{j=1} \tilde{B}_{ki}B_{ij}v_j = \sum^{n}_{j=1} \left( \sum^{n}_{i=1} \tilde{B}_{ki} B_{ij} \right) v_j = 0. 
        \end{equation*}
        Since $\tilde{B}B = \adj(B)B = \det(B)I$ by definition,
        \begin{equation*}
            \tilde{B}_{ki}B_{ij} = \delta_{kj}\det(B).
        \end{equation*}
        That is,
        \begin{equation*}
            \sum^{n}_{j=1} \det(B)v_j = \det(B)v_k = 0
        \end{equation*}
        for each $k\in \left\lbrace 1, 2, \ldots, n \right\rbrace$. Thus by the linearity of $\det(B) = f(T)$,
        \begin{equation*}
            f(T)v = 0
        \end{equation*}
        for all $v\in V$, as desired.
    \end{proof}
    
    \section{Triangulation and Diagonalization}

    \begin{definition}{$T$-Invariant}{Subspace}
        Let $V$ be a vector space and $T:V\to V$ be a linear operator on $V$. We say a subspace $W\subseteq U$ is \emph{$T$-invariant} if $T(W)\subseteq W$. Equivalently,
        \begin{equation*}
            \forall w\in W \left[ Tw\in W \right].
        \end{equation*}
    \end{definition}

    \begin{remark}
        Whenever $W\subseteq V$ is $T$-invariant, we may restrict $T$ to $W$. That is, there is a $T_W:W\to W$ by $w\to Tw$. In terms of matrices, suppose $\alpha = \left\lbrace v_1, v_2, \ldots, v_k \right\rbrace$ is an ordered basis for $W$. By basis extension, there exist $v_{k+1}, v_{k+2}, \ldots, v_n\in V$ such that $\beta = \left\lbrace v_1, v_2, \ldots, v_n \right\rbrace$ is an ordered basis for $V$. Then
        \begin{equation*}
            \left[ T \right]_\beta = \begin{bmatrix} \left[ T_W \right] _\alpha & B \\ O & C \end{bmatrix}
        \end{equation*}
        for some $B\in M_{k\times (n-k)}(\F)$, $C\in M_{(n-k)\times (n-k)}$ and zero matrix $O$.
    \end{remark}

    \begin{definition}{Restriction}{of a Linear Operator on an Invariant Subspace}
        Let $T:V\to V$ be a linear operator and let $W\subseteq V$ be a $T$-invariant subspace. The linear operator $T_W:W\to W$ defined by
        \begin{equation*}
            w\mapsto Tw
        \end{equation*}
        is called the \emph{restriction} of $T$ on $W$.
    \end{definition}

    \begin{remark}
        For any linear operator $T:V\to V$, $\left\lbrace 0 \right\rbrace$ and $V$ are $T$-invariant. Moreover, $\ker(T), \image(T)\subseteq V$ are $T$-invariant as well.
    \end{remark}

    \begin{remark}
        Here is a generalization of Remark 2.8. Let $T, U:V\to V$ linear operators such that $T$ and $U$ commute. Then $\ker(U)$ and $\image(U)$ are $T$-invariant. That is, if $v\in\image(U)$, say $v=Uw$ for some $w\in V$, then
        \begin{equation*}
            Tv = TUw = UTw \in\image(U).
        \end{equation*}
        Similarly, if $v\in\ker(U)$, then
        \begin{equation*}
            UTv = TUv = T_0 = 0,
        \end{equation*}
        which means $Tv\in\ker(U)$. A particular type of operators which commute with $T$ is a polynomial in $T$. For instance, $U = T-cI$ for some eigenvalue $c\in\F$ for $T$ (suppose $T$ has an eigenvalue) is a polynomial in $T$, and $\ker(U)$ is the eigenspace corresponding to $c$. That is, any eigenspace of a linear operator $T$ is $T$-invariant.
    \end{remark}

    \begin{prop}{}
        Let $T:V\to V$ be linear and let $W\subseteq V$ be $T$-invariant. Let $T_W:W\to W$ be the restriction of $T$ on $W$. Let $p,f\in\F[x]$ be the minimal and characteristic polynomials for $T$ and let $p_W,f_W\in\F[x]$ be the minimal and characteristic polynomials for $T_W$. Then $p_W\mid p$ and $f_W\mid f$.
    \end{prop}

    \begin{proof}
        Consider the block matrix
        \begin{equation*}
            \left[ T \right]_\beta = \begin{bmatrix} \left[ T_W \right] _\alpha & B \\ O & C \end{bmatrix}
        \end{equation*}
        from Remark 2.8. It is clear that
        \begin{equation*}
            f = \det\left( xI-\left[ T \right] _\beta \right) = \det\left( xI-\left[ T_\alpha \right] _\alpha \right) \det\left( xI-C \right) = f_W\det\left( xI-C \right) 
        \end{equation*}
        so $f_W\mid f$. Moreover, for any $k\in\N$,
        \begin{equation*}
            \left[ T \right]_\beta^k = \begin{bmatrix} \left[ T_W \right] _\alpha^k & B' \\ O & C^k \end{bmatrix}
        \end{equation*}
        for some $B'\in M_{r\times n-r}(\F)$. Therefore, any annihilating polynomial of $\left[ T \right] _\beta$ annihilates $\left[ T_W \right] _\alpha$ as well. That is, $p_W\mid p$.
    \end{proof}

    \begin{definition}{$T$-Conductor, $T$-Annihilator}{of a Vector}
        Let $V$ be an $n$-dimensional vector space and let $T:V\to V$ be a linear operator. Let $W\subseteq V$ be a $T$-invariant subspace of $V$ and $v\in V$. Define
        \begin{equation*}
            S_T (v; W) = \left\lbrace f\in F[x]: f(T)v \in W \right\rbrace
        \end{equation*}
        which we call the \emph{$T$-conductor} of $v$ into $W$. If $W = \left\lbrace 0 \right\rbrace$, we say $S_T(v; W)$ is a \emph{$T$-annihilator} of $v$. Moreover, the unique monic generator of $S_T(v; W)$, denoted as $s_T(v; W)$, is unique and also called the \emph{$T$-conductor} of $v$ into $W$.
    \end{definition}

    \begin{prop}{$T$-Conductor Is an Ideal}
        Let $T:V\to V$ be linear and $W\subseteq V$ be a $T$-invariant subspace. Then $S_T(v; W)$ is an ideal of $\F[x]$.
    \end{prop}

    \begin{proof}
        Let $f,g\in S_T(v; W)$ and $h\in \F[x]$. Then
        \begin{equation*}
            (f-g)(T)v = \left( f(T)-g(T) \right) v = f(T)v - g(T)v\in W,
        \end{equation*}
        so $f-g\in S_T(v; W)$. Moreover,
        \begin{equation*}
            (fh)(T)v = (hf)(T)v = h(T)\left( f(T)v \right) \in W,
        \end{equation*}
        so $fh\in S_T(v; W)$, as desired.
    \end{proof}

    \begin{definition}{Triangulable}{Linear Operator}
        Let $T:V\to V$ be a linear operator on a finite-dimensional vector space $V$. We say $T$ is \emph{triangulable} if there is an ordered basis $\beta$ for $V$ such that $[T]_\beta$ is truangular.
    \end{definition}
    
    \begin{prop}{}
        Let $V$ be a finite-dimensional vector space over $\F$ and let $T:V\to V$ be a linear operator. Suppose the minimal polynomial $p\in\F[x]$ of $T$ is a product of linear factors, that
        \begin{equation*}
            p = \prod^{k}_{i=1} (x-c_i)^{r_i}
        \end{equation*}
        for some $c_1, c_2, \ldots, c_k\in \F$ and $r_1, r_2, \ldots, r_k\in \N$. Let $W\subsetneq V$ be a $T$-invariant proper subspace of $V$. Then there exists $v\in V$ such that
        \begin{enumerate}
            \item $v\notin W$ and
            \item $(T-cI)v\in W$ for some eigenvalue $c\in\F$ of $T$.
        \end{enumerate}
    \end{prop}

    \begin{proof}
        Let $u\in V$ be such that $u\notin W$ and let $g$ be the $T$-conductor of $u$ into $W$. Then $g\mid p$, so $g = \prod^{k}_{i=1} (x-c_i)^{s_i}$ for some $s_1, s_2, \ldots, s_k$ where $0\leq s_i\leq r_i$ for each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$ and at least one $s_i$ is nonzero. Then
        \begin{equation*}
            g = (x-c_i)h
        \end{equation*}
        for some $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$ and $h\in \F[x]$. By minimality of $g$, $v = h(T)(u)\notin W$. However,
        \begin{equation*}
            (T-c_iI)(v) = \left( T-c_iI \right)h(T)(u) = g(T)(v) \in W 
        \end{equation*}
        by construction.
    \end{proof}

    \begin{remark}
        Notice that $(T-c_iI)(v) = (x-c_i)(T)(v)$. So if $(T-c_iI)(v)\in W$, the $T$-conductor of $v$ into $W$ is a linear polynomial $(x-c_i)$.
    \end{remark}

    \begin{theorem}{$T$ Is Triangulable If and Only If Its Minimal Polynomial Splits}
        Let $V$ be a finite-dimensional vector space over $\F$ and let $T:V\to V$ be a linear operator. Then $T$ is triangulable if and only if the minimal polynomial of $T$ is a product of linear polynomials over $\F$.
    \end{theorem}

    \begin{proof}
        For the forward direction, suppose $T$ is triangulable. Then the characteristic polynomial of $T$ is a product of linear factors, so minimal polynomial is also a product of linear factors. For the reverse direction, we proceed inductively. Suppose that $p$, the minimal polynomial of $T$, is a product of linear factors. Observe that $W_0 = \left\lbrace 0 \right\rbrace$ is a proper $T$-invariant subspace, so there exists $v_1\in V\setminus W_0$ such that $(T-cI)v_1\in W_0$ for some eigenvalue $c$ by Proposition 2.10. We alsosee that $\beta_1 = \left\lbrace v_1 \right\rbrace$ spans a $T$-invariant subspace, since
        \begin{equation*}
            (T-cI)v_1\in W_0 \iff Tv_1 = cv_1 \iff Tv_1\in \spn \left\lbrace v_1 \right\rbrace.
        \end{equation*}
        Now suppose taht $\beta_k = \left\lbrace v_1, v_2, \ldots, v_k \right\rbrace$ is a basis for $T$-invariant proper subspace $W_k$. Then by Proposition 2.10, tere exists $v_{k+1}\in V\setminus W_k$ such that $(T-cI)v_{k+1}\in W_k$. It follows that
        \begin{equation*}
            (T-cI)v_{k+1}\in W_k \iff Tv_{k+1} = cv_{k+1} + \sum^{k}_{i=1} a_iv_i  \iff Tv_{k+1}\in \spn \left\lbrace v_1, v_2, \ldots, v_{k+1} \right\rbrace.
        \end{equation*}
        But this exactly means $W_{k+1}$ is $T$-invariant. So continuing this process, we have a basis $\beta = \left\lbrace v_1, v_2, \ldots, v_n \right\rbrace$. Moreover, the $T$-invariance of $W_1, W_2, \ldots, W_{n-1}$ guarantees that $[T]_\beta$ is upper-triangular, as required.
    \end{proof}

    \clearpage
    \begin{theorem}{$T$ Is Diagonalizable If and Only If Its Minimal Polynomial Is a Product of Distinct Linear Factors}
        Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be a linear operator. Then $T$ is diagonalizable if and only if the minimal polynomial of $T$ is of the form
        \begin{equation*}
            \prod^{k}_{i=1} (x-c_i).
        \end{equation*}
    \end{theorem}

    \begin{proof}
        The forward direction is supplied by Proposition 2.5. For the reverse direction, suppose
        \begin{equation*}
            p = \prod^{k}_{i=1} \left( x-c_i \right) 
        \end{equation*}
        is the minimal polynomial. Let $W\subseteq V$ be the subspace of $V$ generated by every eigenvectors of $T$. For the sake of contradiction, suppose $W\subsetneq V$. Then ther exists $v\in V\setminus W$ such that $(T-cI)v\in W$ by Proposition 2.10, since a subspace generated by eigenvectors is $T$-invariant. Let $u = \left( T - cI \right)v\in W$. Then,
        \begin{equation*}
            u = \sum^{k}_{i=1} v_i
        \end{equation*}
        for some $v_1, v_2, \ldots, v_k\in W$ satisfying $Tv_i = c_iv_i$ for all $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$. Then for any $f\in \F[x]$,
        \begin{equation*}
            f(T)u = f(T) \sum^{k}_{i=1} v_i = \sum^{k}_{i=1} f(T)v_i = \sum^{k}_{i=1} f(c_i)v_i
        \end{equation*}
        is an element of $W$. Now $p = (x-c_j)q$ for some eigenvalue $c_j\in\F$ and $q\in \F[x]$. Also,
        \begin{equation*}
            q - q(c_j) = (x-c_j)g
        \end{equation*}
        for some $g\in\F[x]$, since $q(c_j) - q(c_j) = 0$. So we have
        \begin{equation*}
            \left( q -q(c_j) \right)(T)v = \left( T-c_jI \right) g(T)v = g(T)u.
        \end{equation*}
        But $g(T)u\in W$, and since
        \begin{equation*}
            0 = p(T)v = (T-c_jI)q(T)v,
        \end{equation*}
        $q(T)v$ is an eigenvector corresponding to $c_j$. That is, $q(T)v\in W$. So clearly $q(c_j)v\in W$ as well, but $v\notin W$ so $q(c_j) = 0$. This is a contradiction, since $p = (x-c_j)q$ is a product of linear factors of degree 1. Thus $W=V$, which exactly means that $T$ is diagonalizable.
    \end{proof}

    \begin{remark}
        We shall give a different proof of Theorem 2.12 later. 
    \end{remark}

    \section{Direct Sum Decompositions}

    \begin{remark}
        At the beginning of the chapter, we introduced that our goal is to find an ordered basis in which the matrix representation of a linear operator assumes a simple form. Now, we shall describe our goal as follows: To decompose the underlying space $V$ into a direct sum of $T$-invariant subspaces such that the restriction operators on those subspaces are simple.
    \end{remark}
    
    \begin{definition}{Independent}{Subspaces}
        Let $V$ be a vector space and let $W_1, W_2, \ldots, W_k\subseteq V$ be subspaces. We say $W_1, W_2, \ldots, W_k$ are \emph{independent} if
        \begin{equation*}
            \sum^{k}_{i=1} w_i = 0\implies w_1=w_2=\cdots=w_k=0,
        \end{equation*}
        provided that each $w_i\in W_i$.
    \end{definition}

    \begin{remark}
        When $k=2$, the meaning of independence is merely $\left\lbrace 0 \right\rbrace$ intersection. When $k>2$, it means more than that:
        \begin{equation*}
            \forall j\in \left\lbrace 1, 2, \ldots, k \right\rbrace \left[ W_j\cap \left( W_1+W_2+\cdots+W_{j-1}+W_{j+1}+\cdots+W_k \right) = 0 \right].
        \end{equation*}
        The significance of independence can be shown as follows. If $W_1+W_2+\cdots+W_k=W\subseteq V$, then there exist $w_1\in W_1, w_2\in W_2,\ldots,w_k\in W_k$ such that
        \begin{equation*}
            w = \sum^{k}_{i=1} w_i
        \end{equation*}
        for all $w\in W$.
    \end{remark}

    \begin{prop}{Alternative Definitions of Independence}
        Let $V$ be a finite-dimensional vector space and let $W_1, W_2, \ldots, W_k\subseteq V$ be subspaces, where $W = W_1+W_2+\cdots+W_k$. Then the following are equivalent.
        \begin{enumerate}
            \item $W_1, W_2, \ldots, W_k$ are independent.
            \item $\forall j\in \left\lbrace 2, 3, \ldots, k \right\rbrace \left[ W_j\cap \left( W_1+W_2+\cdots+W_{j-1} \right) = \left\lbrace 0 \right\rbrace  \right]$.
            \item If $\beta_1, \beta_2, \ldots, \beta_k$ are ordered bases for $W_1, W_2, \ldots, W_k$, then $\beta = \left( \beta_1, \beta_2, \ldots, \beta_k \right)$ is an ordered basis for $W$.
        \end{enumerate}
    \end{prop}

    \begin{proof}
        For (a)$\implies$(b), suppose $W_1, W_2, \ldots, W_k$ are independent. Then for any $j\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, 
        \begin{equation*}
            W_j\cap \left( W_1+W_2+\cdots+W_{j-1}+W_{j+1}+\cdots+W_k \right) = \left\lbrace 0 \right\rbrace 
        \end{equation*}
        by Remark 2.14. It follows that (b) is true as well. For (b)$\implies$(c), let $w\in W$. Then there exist $w_1\in W_1, W_2\in W_2, \ldots, w_k\in W_k$ such that
        \begin{equation*}
            w = \sum^{k}_{i=1} w_i.
        \end{equation*}
        This is a unique representation of $w$ as a sum of elements of $W_1, W_2, \ldots, W_k$, since (b) implies that $w_i\notin W_j$ whenever $i\neq j$. This exactly means $\beta = \left( \beta_1, \beta_2, \ldots, \beta_k \right)$ is an ordered basis for $W$. For (c)$\implies$(a), suppose $\beta = \left( \beta_1, \beta_2, \cdots, \beta_k \right)$ is an ordered basis for $W$. Then $\beta_1, \beta_2, \ldots, \beta_k$ are linearly independent, so whenever we have a linear
        relation
        \begin{equation*}
            \sum^{k}_{i=1} w_i = 0,
        \end{equation*}
        where each $w_i\in W_i$ for each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, $w_1 = w_2 = \cdots = w_k = 0$. But this exactly means that $W_1, W_2, \ldots, W_k$ are independent.
    \end{proof}

    \begin{definition}{Direct Sum}{of Subspaces}
        Let $V$ be a vector space and let $W_1, W_2, \ldots, W_k\subseteq V$ be subspaces. If $W_1, W_2, \ldots, W_k$ are independent, then we say the sum
        \begin{equation*}
            W = W_1 + W_2 + \cdots + W_k
        \end{equation*}
        is \emph{direct}. Moreover, we write
        \begin{equation*}
            W = W_1\oplus W_2\oplus \cdots\oplus W_k = \bigoplus^{k}_{i=1} W_i
        \end{equation*}
        to indicate that $W$ is the direct sum of $W_1, W_2, \ldots, W_k$.
    \end{definition}

    \begin{definition}{Projection}{of a Vector Space}
        Let $V$ be a vector space. A linear operator $E:V\to V$ is called a \emph{projection} of $V$ if $E^2 = E$.
    \end{definition}

    \begin{prop}{Properties of Projections}
        Let $V$ be a vector space and let $E: V\to V$ be a projection. Then the following hold.
        \begin{enumerate}
            \item For any $v\in V$, $v\in \image(E)$ if and only if $Ev = v$.
            \item $V = \image(E)\oplus \ker(E)$. In particular,
                \begin{equation*}
                    Ev + (v-Ev) = v\in V
                \end{equation*}
                is the unique representation for any $v\in V$ as a sum of vectors in $\image(E)$ and $\ker(E)$.
        \end{enumerate}
    \end{prop}

    \begin{cor}{Projection on $R$ along $N$}
        Let $V$ be a vector space and let $R, N\subseteq V$ be subspaces satisfying $R\oplus N = V$. Then there exists a unique projection $E:V\to V$ such that $\image(E) = R$ and $\ker(E) = N$.
    \end{cor}	

    \begin{definition}{Projection}{on $R$ along $N$}
        Consider Corollary 2.14.1. We say $E:V\to V$ the \emph{projection} on $\R$ along $\N$.
    \end{definition}

    \begin{remark}
        Any projection is trivially diagonalizable. Let $E: V\to V$ be a projection and let $\beta_R,\beta_N$ be ordered bases for $\image(E)$ and $\ker(E)$. Then
        \begin{equation*}
            \left[ E \right]_{\left( \beta_R, \beta_N \right)} = \begin{bmatrix} I_k & 0 \\ 0 & 0 \end{bmatrix},
        \end{equation*}
        where $k = \dim(\image(E)) = \left| \beta_R \right|$.
    \end{remark}

    \begin{remark}
        Projections can be used to conveniently describe direct sum decompositions of vector spaces. Let $V$ be a vector space and suppose
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} W_i
        \end{equation*}
        where $W_1, W_2, \ldots, W_k\subseteq W$ are subspaces. Then we shall define linear operators $E_1, E_2, \ldots, E_k:V\to V$ such that
        \begin{equation*}
            E_i(v) = w_i
        \end{equation*}
        provided that $v = \sum^{k}_{i=1} w_i$ for some $w_1\in W_1, w_2\in W_2, \ldots, w_k\in W_k$. It can be easily verified that each $E_i$ is a projection on $W_i$ along $\bigoplus^{k}_{j=1, j\neq i} W_i$. Moreover,
        \begin{equation*}
            \forall v\in V \left[ v = \sum^{k}_{i=1} E_iv \right],
        \end{equation*}
        or, equivalently,
        \begin{equation*}
            I = \sum^{k}_{i=1} E_i. 
        \end{equation*}
        Lastly,
        \begin{equation*}
            E_iE_j = 0
        \end{equation*}
        whenever $i\neq j$. We summarize our observations and prove its converse in the following theorem.
    \end{remark}

    \clearpage
    \begin{theorem}{Direct Sum Decomposition}
        Let $V$ be a vector space and let $W_1, W_2, \ldots, W_k\subseteq V$ be subspaces such that $V = \bigoplus^{k}_{i=1} W_i$. Then there exist linear operations $E_1, E_2, \ldots, E_k:V\to V$ such that the following hold.
        \begin{enumerate}
            \item Each $E_i$ is a projection.
            \item $E_iE_j = 0$ whenever $i\neq j$. 
            \item $I = \sum^{k}_{i=1} E_i$.
            \item $\image(E_i) = W_i$.
        \end{enumerate}
        Conversely, if $E_1, E_2, \ldots, E_k:V\to V$ are linear operators satisfying (a), (b), and (c), then
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} \image(E_i).
        \end{equation*}
    \end{theorem}

    \begin{proof}
        We only have to prove the converse statement. By (c),
        \begin{equation*}
            \forall v\in V\exists E_1v\in \image(E_1)\exists E_2v\in \image(E_2)\cdots\exists E_kv\in \image(E_k) \left[ v = Iv = \sum^{k}_{i=1} E_iv \right].
        \end{equation*}
        This means
        \begin{equation*}
            V = \image(E_1) + \image(E_2) + \cdots + \image(E_k).
        \end{equation*}
        This sum is unique, since the representation $v = \sum^{k}_{i=1} E_iv$ is the unique representation of $v$ as a sum of vectors in $\image(E_1), \image(E_2), \ldots, \image(E_k)$. To verify this, suppose
        \begin{equation*}
            v = \sum^{k}_{i=1} w_i
        \end{equation*}
        for some $w_1\in \image(E_1), w_2\in \image(E_2), \ldots, w_k\in \image(E_k)$. Then
        \begin{equation*}
            E_iv = E_i \sum^{k}_{j=1} w_j = \sum^{k}_{j=1} E_iw_j = E_iw_i = w_i.
        \end{equation*}
        Thus $\image(E_1), \image(E_2), \ldots, \image(E_k)$ are independent and
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} \image(E_i). \eqedsym
        \end{equation*}
    \end{proof}

    \section{Invariant Direct Sum}

    \begin{remark}
        We are primarily intereseted in direct sum decompositions $V = \bigoplus^{k}_{i=1} W_i$ such that each subspace $W_i\subseteq V$ is invariant under some linear operator $T:V\to V$. Given such decompositions, $T$ induces a linear operator $T_{W_i}: W_i\to W_i$ on each $W_i$ by restriction. Recall that given $v\in V$, the representation
        \begin{equation*}
            v = \sum^{k}_{i=1} w_i
        \end{equation*}
        as a sum of vectors from $W_1, W_2, \ldots, W_k$ is unique, provided that $\bigoplus^{k}_{i=1} W_i$. Then,
        \begin{equation*}
            Tv = \sum^{k}_{i=1} Tw_i = \sum^{k}_{i=1} T_{W_i}w_i.
        \end{equation*}
        We describe this situation as follows.
    \end{remark}

    \begin{definition}{Direct Sum}{of Linear Operators}
        Consider the case in Remark 2.17. We say $T$ is the \emph{direct sum} of linear operators $T_{W_1}, T_{W_2}, \ldots, T_{W_k}$.
    \end{definition}
    
    \begin{remark}
        One should note that the direct sum of operators is different from the addition of linear operators, since each $T_{W_i}$ has $W_i$ as its domain and range. We also have a matrix analogue of this. 
    \end{remark}

    \begin{definition}{Direct Sum}{of Matrices}
        Let $A\in M_{n\times n}(\F)$ and suppose
        \begin{equation*}
            A =
            \begin{bmatrix}
                A_1& & & \\
                & A_2& & \\
                & & \ddots& \\
                & & & A_k\\
            \end{bmatrix}
        \end{equation*}
        for some square matrices $A_1,A_2,\ldots,A_k$ over $\F$. Then we call $A$ the \emph{direct sum} of $A_1,A_2,\ldots,A_k$.
    \end{definition}

    \begin{remark}
        Most often, we shall describe each subspace $W_i$ by means of the associated projection $E_i$. In other words, we need to phrase the invariance of $W_i$ in terms of $E_i$.
    \end{remark}

    \begin{prop}{A Subspace Is $T$-Invariant If and Only If $T$ Commutes with the Associated Projection}
        Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be a linear operator. Let $W_1, W_2, \ldots, W_k$ be subspaces such that
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} W_i
        \end{equation*}
        and let $E_i:V\to V$ be the associated projection for each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$. Then each subspace $W_i$ is invariant under $T$ if and only if $T$ commutes with each $E_i$.
    \end{prop}

    \begin{proof}
        For the reverse direction, suppose $T$ commutes with each $E_i$. Then for any $w_i\in W_i$,
        \begin{equation*}
            Tw_i = TE_iw_i = E_iTw_i
        \end{equation*}
        so $Tw_i\in \image(E_i) = W_i$, which exactly means that $W_i$ is invariant under $T$. For the forward direction, suppose that each $W_i$ is $T$-invariant. Then for any $v\in V$,
        \begin{equation*}
            \sum^{k}_{i=1} E_iv
        \end{equation*}
        and so
        \begin{equation*}
            Tv = T \sum^{k}_{i=1} E_iv = \sum^{k}_{i=1} TE_iv.
        \end{equation*}
        Since each $W_i$ is $T$-invariant, there exists $w_i\in W_i$ such that $TE_iv = E_iw_i$. Moreover,
        \begin{equation*}
            E_jTE_iv =
            \begin{cases}
                0 & \text{ if }i\neq j \\
                E_j^2w_j & \text{ otherwise }
            \end{cases}
        \end{equation*}
        so
        \begin{equation*}
            E_jTv = \sum^{k}_{i=1} E_jTE_iv = E_j^2w_j = E_jw_j = TE_jv,
        \end{equation*}
        which exactly means that $T$ commutes with each $E_i$, as desired.
    \end{proof}

    \begin{remark}
        We now proceed to describe diagonalizable linear operator $T$ in terms of a direct sum decomposition of invariant subspaces by using projections that commute with $T$.
    \end{remark}

    \clearpage
    \begin{theorem}{}
        Let $V$ be a vector space and let $T:V\to V$ be diagonalizable. Let $c_1,c_2,\ldots,c_k\in\F$ be the distinct eigenvalues of $T$. Then there exists linear operators $E_1,E_2,\ldots,E_k:V\to V$ such that the following hold.
        \begin{enumerate}
            \item $T = \sum^{k}_{i=1} c_iE_i$.
            \item $I = \sum^{k}_{i=1} E_i$.
            \item $E_iE_j = 0$ whenever $i\neq j$.
            \item $E_i^2 = E_i$.
            \item $\image(E_i)$ is the eigenspace corresponding to $c_i$.
        \end{enumerate}
        Conversely, if there exist $k$ distinct scalars $c_1, c_2, \ldots, c_i\in \F$ and $k$ nonzero linear operators $E_1, E_2, \ldots, E_k: V\to V$ satisfying (a), (b), and (c), then $T$ is diagonalizable, $c_1, c_2, \ldots, c_k$ are eigenvalues of $T$, and conditions (d) and (e) are satisfied as well.
    \end{theorem}

    \begin{proof}
        Suppose that $T$ is diagonalizable and $c_1, c_2, \ldots, c_k\in \F$ are $k$ distinct eigenvalues of $T$. For each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, let $W_i$ denote the eigenspace corresponding to $c_i$ and let $E_i:V\to V$ be the associated projection. Then the conditions (b), (c), (d), and (e) are satisfied by construction. To verify (a), notice that
        \begin{equation*}
            \forall v\in V \left[ v = \sum^{k}_{i=1} E_iv \right] 
        \end{equation*}
        so
        \begin{equation*}
            \forall v\in V \left[ Tv = T \sum^{k}_{i=1} E_iv = \sum^{k}_{i=1} TE_iv = \sum^{k}_{i=1} c_iE_iv \right].
        \end{equation*}
        But this exactly means $T = \sum^{k}_{i=1} c_iE_i$. To verify the converse statement, suppose that there exist $k$ distinct scalars $c_1, c_2, \ldots, c_k$ and $k$ distinct linear operators $E_1, E_2, \ldots, E_k:V\to V$ satisfying (a), (b), (c). Then
        \begin{equation*}
            E_i = E_iI = E_i \sum^{k}_{j=1} E_j = E_i^2
        \end{equation*} 
        so (d) is satisfied. To show that each $c_i$ is an eigenvalue, notice that there exists $E_iv\in \image(E_i)$ such that
        \begin{equation*}
            TE_iv = \sum^{k}_{j=1} c_jE_jE_iv = c_iE_i^2v = c_iE_iv.
        \end{equation*}
        Notice that $E_i$ is nonzero by assumption, so there must exist a nonzero element in $W_i$, the eigenspace corresponding to $c_i$. Furthermore, the above equation shos that $\image(E_i)\subseteq W_i$. We also see that $T$ is diagonalizable, since
        \begin{equation*}
            \forall v\in V \left[ v = Iv = \sum^{k}_{i=1} E_iv \right] 
        \end{equation*}
        so $\spn \left( \bigcup^{k}_{i=1} \image(E_i) \right) = V$. That is, eigenvectors of $T$ span $V$. To verify that $c_1, c_2, \ldots, c_k$ are the only eigenvalues of $T$, suppose
        \begin{equation*}
            Tv = cv
        \end{equation*}
        for some $c\in \F$ and $v\in V$. Then $(T-cI)v = 0$ so
        \begin{equation*}
            (T-cI)v = \left( \sum^{k}_{i=1} c_iE_i - c \sum^{k}_{i=1} E_i \right) v = \sum^{k}_{i=1} (c_i-c) E_iv = 0.
        \end{equation*}
        So $(c_j-c)E_jv = 0$ for each $j\in \left\lbrace 1, 2, \ldots, k \right\rbrace$. If $v\neq 0$, then $E_iv\neq 0$ for some $i$. Notice that this $i$ is unique, since
        \begin{equation*}
            Tv = \sum^{k}_{i=1} c_iE_iv = cv = \sum^{k}_{i=1} cE_iv.
        \end{equation*}
        So in order for $c_1, c_2, \ldots, c_k$ to be distinct, there must be only one $i$ such that $E_iv\neq 0$. This means $c = c_i$ for some $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$ whenever $c$ is an eigenvalue of $T$. To verify that $W_i\subseteq \image(E_i)$, let $v\in W_i$. Then
        \begin{equation*}
            Tv = \sum^{k}_{j=1} c_jE_jv = c_iv = c_i \sum^{k}_{j=1} E_jv = \sum^{k}_{i=1} c_iE_jv.
        \end{equation*}
        So
        \begin{equation*}
            \sum^{k}_{j=1} \left( c_j - c_i \right) E_jv = 0. 
        \end{equation*}
        It follows that $E_jv = 0$ whenever $i\neq j$ so
        \begin{equation*}
            v = Iv = \sum^{k}_{j=1} E_jv = E_iv \in \image(E_i),
        \end{equation*}
        as desired.
    \end{proof}

    \begin{remark}
        Theorem 2.17 shows that for a diagonalizable linear operator $T$, the scalars $c_1, c_2, \ldots, c_k\in \F$ and linear operators $E_1, E_2, \ldots, E_k:V\to V$ are uniquely determined by (a), (b), (c), the fact that each $c_i$ is distinct, and the fact that each $E_i\neq 0$. One of the pleasant features of the decomposition
        \begin{equation*}
            T = \sum^{k}_{i=1} E_i
        \end{equation*}
        is that, for any polynomial $f\in \F[x]$,
        \begin{equation*}
            f(T) = \sum^{k}_{i=1} f(c_i)E_i.
        \end{equation*}
        This can be verified by checking the result for each powers of $x$. This is analogous to the fact that, given a diagonal $A\in M_{n\times n}(\F)$, then
        \begin{equation*}
            \left[ f(A) \right] _{ii} = f(A_{ii}).
        \end{equation*}
    \end{remark}

    \begin{remark}
        Consider applying Lagrange interpolation to distinct scalars $c_1, c_2, \ldots, c_k\in \F$. Define
        \begin{equation*}
            p_j = \prod^{k}_{i=1,i\neq j} \frac{(x-c_i)}{(c_j-c_i)}
        \end{equation*}
        then we have $p_j(c_i) = \delta_{ij}$, which means
        \begin{equation*}
            p_j(T) = \sum^{k}_{i=1} p_j(c_i)E_i = \sum^{k}_{i=1} \delta_{ij}E_i = E_j.
        \end{equation*}
        Since $E_j = p_j(T)$ is a polynomial in $T$, it follows that $E_i$ commutes not only with $T$ but with polynomials in $T$ as well. This enables us to give an alternative proof to Theorem 2.12.
    \end{remark}

    \setcounter{stcounter}{11}
    \begin{theorem}{$T$ Is Diagonalizable If and Only If Its Minimal Polynomial Is a Product of Distinct Linear Factors}
        Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be a linear operator. Then $T$ is diagonalizable if and only if the minimal polynomial of $T$ is of the form
        \begin{equation*}
            \prod^{k}_{i=1} (x-c_i).
        \end{equation*}
    \end{theorem}
    \setcounter{stcounter}{17}

    \begin{proof}
        For the forward direction, suppose $T$ is diagonalizable. Then for any $f\in \F[x]$,
        \begin{equation*}
            f(T) = \sum^{k}_{i=1} f(c_i)E_i
        \end{equation*}
        where $c_1, c_2, \ldots, c_k\in \F$ are distinct eigenvalues of $T$ and each $E_i$ is the associated projection to the age space corresponding to $c_i$. It follows that $f(c_i) = 0$ for each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$ provided that $f(T) = 0$, and in particular, the minimal polynomial is
        \begin{equation*}
            p = \prod^{k}_{i=1} (x-c_i).
        \end{equation*}
        For the reverse direction, suppose
        \begin{equation*}
            p = \prod^{k}_{i=1} (x-c_i)
        \end{equation*}
        for some distinct $c_1, c_2, \ldots, c_k\in \F$ is the minimal polynomial of $T$. We form the Lagrange polynomial
        \begin{equation*}
            p_j = \prod^{k}_{i=1,i\neq j} \frac{x-c_i}{c_j-c_i}
        \end{equation*}
        for each $j\in \left\lbrace 1, 2, \ldots, k \right\rbrace$. Recall that the Lagrange polynomials have the properties that $p_j(c_i) = \delta_{ij}$ and
        \begin{equation*}
            g = \sum^{k}_{j=1} g(c_i)p_i
        \end{equation*}
        provided that $\deg(g)\leq k-1$. Then
        \begin{equation*}
            1 = \sum^{k}_{i=1} p_i \ \ \ \ x = \sum^{k}_{j=1} c_ip_i.
        \end{equation*}
        Notice that we may not define $x$ as $\sum^{k}_{j=1} c_ip_i$ if $k = 1$. However, if $k=1$, then $p = (x-c_1)$ and the proof is trivial. So we may safely assume $k\geq 2$. Now let $E_i = p_i(T)$ then
        \begin{equation*}
            I = \sum^{k}_{i=1} E_i \ \ \ \ T = \sum^{k}_{i=1} c_iE_i.
        \end{equation*}
        Observe that $p\mid p_ip_j$ whenever $i\neq j$, since $c_ic_j$ has every factor that $p$ has. Thus
        \begin{equation*}
            \left( p_ip_j \right)T = E_iE_j = 0.
        \end{equation*}
        Furthermore, observe that $E_i\neq 0$ for each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, since $\deg(p_i) < \deg(p)$. Notice that we just proved the necessary conditions for the converse statement of Theorem 2.17 to hold. That is, $T$ is diagonalizable.
    \end{proof}

    \section{Primary Decomposition Theorem}
    
    \begin{remark}
        When we try to study a linear operator $T:V\to V$ on a finite-dimensional vector space $V$ in terms of its characteristic values, we are confronted with two particular problems.
        \begin{enumerate}
            \item The minimal polynomial of $T$ may not decompose into a product of linear factors. This is certainly a deficiency in the field $\F$, since we cannot guarantee algebraic closure. 
            \item Even if we find the characteristic polynomial to be decomposed into a product of linear factors, there is no guarantee that the direct sum of the corresponding eigenspaces is equal to $V$.
        \end{enumerate}
        However, one can verify that a weaker version of (b) always holds. Namely, given a linear operator $T$ with the characteristic polynomial
        \begin{equation*}
            p = \prod^{k}_{i=1} (x-c_i)^{r_i}
        \end{equation*}
        for some $c_1,c_2,\ldots,c_k\in\F$ and $r_1,r_2,\ldots,r_k\in\N$, it is always the case that
        \begin{equation*}
            V = \bigoplus^{k}_{i=1} \ker\left( \left( T-c_iI \right)^{r_i} \right).
        \end{equation*}
        In fact, we are going to prove more general version of this idea.
    \end{remark}

    \clearpage
    \begin{theorem}{Primary Decomposition Theorem}
        Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be a linear operator. Suppose
        \begin{equation*}
            p = \prod^{k}_{i=1} p_i^{r_i}
        \end{equation*}
        is the minimal polynomial of $T$, where $p_1, p_2, \ldots, p_k$ are distinct irreducible monic polynomials and $r_i\in \N$ for each $i$. Let
        \begin{equation*}
            W_i = \ker\left( p_i(T)^{r_i} \right) 
        \end{equation*}
        for each $i$. Then the following hold.
        \begin{enumerate}
            \item $V = \bigoplus^{k}_{i=1} W_i$.
            \item Each $W_i$ is $T$-invariant.
            \item If $T_i$ is the induced operator on $W_i$ by $T$, then the minimal polynomial for $T_i$ is $p_i^{r_i}$.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        For each $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$, define
        \begin{equation*}
            f_i = \frac{p}{p_i^{r_i}} = \prod^{k}_{j=1,j\neq i} p_j^{r_j}
        \end{equation*}
        then $f_1, f_2, \ldots, f_k$ are coprime by construction. So there exist $g_1, g_2, \ldots, g_k\in \F[x]$ such that
        \begin{equation*}
            \sum^{k}_{i=1} f_ig_i = 1
        \end{equation*}
        by (b) of Proposition 1.11. Define $E_i = f_i(T)g_i(T)$ for each $i$. Notice that $p\mid h_ih_j$ whenever $i\neq j$, so $E_iE_j = 0$ whenever $i\neq j$ and $\sum^{k}_{i=1} E_i = I$. It follows that $E_i = E_iI = E_i \sum^{k}_{j=1} E_j = E_i^2$, so each $E_i$ is a projection. Now the claim is that $\image(E_i) = W_i = \ker\left( p_i(T)^{r_i} \right)$. To verify this, observe that if $v\in \image(E_i)$, then $v = E_iv = f_i(T)g_i(T)v$ and
        \begin{equation*}
            p_i(T)^{r_i}v = p_i(T)^{r_i}f_i(T)g_i(T)v = p(T)g_i(T)v = 0.
        \end{equation*}
        Moreover, notice that $p_i(T)^{r_i}\mid E_j$ whenever $i\neq j$. It follows that if $v\in \ker\left( p_i(T)^{r_i} \right)$, then $E_jv = 0$ for each $j\neq i$. So
        \begin{equation*}
            v = Iv = \sum^{k}_{j=1} E_jv = E_iv.
        \end{equation*}
        Then $\bigoplus^{k}_{i=1} W_i = V$ by Theorem 2.15. Furthermore, notice that each $W_i$ is $T$-invariant by definition, since if $p_i(T)^{r_i}v = 0$, then
        \begin{equation*}
            p_i(T)^{r_i}Tv = Tp_i(T)^{r_i}v = 0,
        \end{equation*}
        since $T$ commutes with any polynomial in $T$. So define $T_i$ be the induced operator on $W_i$ by $T$. Clearly $p_i(T)^{r_i} = 0$, since $p_i^{r_i}$ annihilates $T$ on $W_i$ by definition. Moreover, suppose $q_I$ is an annihilating polynomial of $T_i$. Then $q_if_i$ annihilates $T$, so $p\mid q_if_i$. Buth $p = p_i^{r_i}f_i$, so
        \begin{equation*}
            p_i^{r_i}f_i\mid q_if_i
        \end{equation*}
        which means $p_i^{r_i}\mid q_i$ as well. Thus $p_i^{r_i} = q_i$ is the minimal polynomial for $T_i$, as desired.
    \end{proof}

    \begin{cor}{}
        If $E_1, E_2, \ldots, E_k$ are projections associated with the primary decomposition of $T$, then each $E_i$ is a projection in $T$, and any linear operator $U$ commutes with $T$ commutes with $E_i$. In particular, the associated subspace $W_i = \image(E_i)$ is $U$-invariant.
    \end{cor}	

    \begin{definition}{Diagonalizable Part}{of a Linear Operator}
        Consider analyzing a special case of the Theorem 2.18, when each factor $p_i$ of the minimal polynomial is linear (i.e. each $p_i = x-c_i$). Define
        \begin{equation*}
            D = \sum^{k}_{i=1} c_iE_i:V\to V
        \end{equation*}
        then $D$ is a diagonalizable operator by Theorem 2.15, which we call the \emph{diagonalizable part} of $T$.
    \end{definition}

    \begin{remark}
        Let $T$ be a linear operator with the characteristic polynomial
        \begin{equation*}
            \prod^{k}_{i=1} (x-c_i)^{r_i}
        \end{equation*}
        and let $D$ be the diagonalizable part of $T$. Further define
        \begin{equation*}
            N = T - D = \sum^{k}_{i=1} (T-c_iI)E_i.
        \end{equation*}
        Clearly
        \begin{equation*}
            N^r = \sum^{k}_{i=1} (T-c_I)^rE_i,
        \end{equation*}
        since each $E_i$ is a projection. Notice that
        \begin{equation*}
            (T-c_iI)^rE_i = 0
        \end{equation*}
        whenever $r\geq r_i$. It follows that
        \begin{equation*}
            N^r = \sum^{k}_{i=1} \left( T-c_iI \right) ^{r_i} E_i = 0 
        \end{equation*}
        provided that $r\geq r_i$ for all $i\in \left\lbrace 1, 2, \ldots, k \right\rbrace$. This motivates the following definition.
    \end{remark}

    \begin{definition}{Nilpotent}{Linear Operator}
        Let $V$ be a vector space and let $N:V\to V$ be a linear operator. We say $N$ is \emph{nilpotent} if there exists $r\in \N$ such that $N^r = 0$. 
    \end{definition}

    \begin{theorem}{Nilpotent Decomposition}
        Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be a linear operator. If the minimal polynomial of $T$ decomposes into a product of linear polynomials, then there exist diagonalizable $D:V\to V$ and $N:V\to V$ such that the following holds.
        \begin{enumerate}
            \item $T = D+N$.
            \item $DN = ND$.
        \end{enumerate}
        Moreover, $D$ and $N$ are polynomials in $T$ uniquely determined by (a) and (b).
    \end{theorem}

    \begin{proof}
        We have already shown the existence of $D, N:V\to V$ satisfying the listed conditions in Remark 2.24. Namely, take diagonalizable part of $T$ to be $D$ and define $N = T-D$. To verify the uniqueness, suppose that there exist diagonalizable $D':V\to V$ and nilpotent $N':V\to V$ satisfying the listed properties. Then
        \begin{equation*}
            N + D = N' + D'
        \end{equation*}
        so
        \begin{equation*}
            D - D' = N' - N.
        \end{equation*}
        We see that both $D$ and $D'$ are diagonalizable and $DD' = D'D$, so $D-D'$ is diagonalizable. Moreover, since both $N'$ and $N$ are nilpotent, it follows that $N'-N$ is nilpotent. This can be verified easily by using binomial theorem, since $N'$ and $N$ commute. It follows that $D-D'$ is nilpotent, so the minimal polynomial for $D-D'$ is $x^r$ for some $r\in \N$. But $D-D'$ is diagonalizable, so $r=1$ and $x$ is its minimal polynomial. Thus $D-D'=0$, and we have $D=D'$ and
        $N=N'$, as desired. 
    \end{proof}

    \begin{cor}{}
        Let $V$ be a finite-dimensional vector space over an algebraically closed field $\F$. Then every operator $T:V\to V$ can be uniquely written as a sum of diagonalizable $D:V\to V$ and nilpotent $N:V\to V$, where both $D$ and $N$ are polynomials in $T$.
    \end{cor}	

\end{document}
